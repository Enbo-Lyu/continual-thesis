{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current default GPU index: 2\n",
      "Current default GPU name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(2)\n",
    "if torch.cuda.is_available():\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"Current default GPU index: {current_gpu}\")\n",
    "    print(f\"Current default GPU name: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhW7Y0auSg3"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uHBgOTv8-Kln"
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "# buffer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Generic,\n",
    "    Optional,\n",
    "    List,\n",
    "    TYPE_CHECKING,\n",
    "    Set,\n",
    "    TypeVar,\n",
    ")\n",
    "\n",
    "from avalanche.benchmarks.utils import (\n",
    "    classification_subset,\n",
    "    AvalancheDataset,\n",
    ")\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "# from ..benchmarks.utils.utils import concat_datasets\n",
    "from avalanche.benchmarks.utils import concat_datasets\n",
    "from avalanche.training.storage_policy import ReservoirSamplingBuffer, BalancedExemplarsBuffer, ClassBalancedBuffer\n",
    "\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy, ExemplarsBuffer, ExperienceBalancedBuffer\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from typing import Optional, TYPE_CHECKING\n",
    "\n",
    "from avalanche.benchmarks.utils import concat_classification_datasets\n",
    "from avalanche.training.plugins.strategy_plugin import SupervisedPlugin\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from avalanche.training.templates import SupervisedTemplate, BaseSGDTemplate\n",
    "\n",
    "# dataset\n",
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader, ReplayDataLoader\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger, TensorboardLogger\n",
    "\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "\n",
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1, ICaRL\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "from types import SimpleNamespace\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Znp5LYsI-myD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fzOy2HmlWQnX"
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset, ConcatDataset, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim.lr_scheduler # ?\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, CenterCrop, RandomHorizontalFlip, Resize\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0sf0FvHf43e",
    "outputId": "88cee04e-55dd-4a4b-8372-1f90a50ac620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform = transform_train = Compose([\n",
    "    # Resize(224),\n",
    "    # Resize(384),\n",
    "    # RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    # Normalize(*stats,inplace=True)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "name_list = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "d0rW8AbrJ3VQ",
    "outputId": "0b5eb9c8-b53a-44f8-c006-b4c06a7c3e75"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telephone maple_tree   sea beetle\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Function to show images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize if Normalize was used in transform\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % trainset.classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUIbhWRql_oQ",
    "outputId": "17a0dfbd-270a-4696-b95c-dd5804b69f85"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_pil(image_tensor):\n",
    "    return transforms.ToPILImage()(image_tensor).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptMPtw0fYuhS",
    "outputId": "92b65eae-362e-4fea-a093-5881fecc8796"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def save_cifar100_random_replay(dataset, num_images_per_class, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.manual_seed(41)\n",
    "\n",
    "    saved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "    \n",
    "    transform_to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    for idx in indices:\n",
    "        image, label = dataset[idx]\n",
    "        image_tensor = transform_to_tensor(image)\n",
    "\n",
    "        # Skip saving if this class already has the desired number of images saved\n",
    "        if saved_counts[label] >= num_images_per_class:\n",
    "            continue\n",
    "\n",
    "        class_name = dataset.classes[label]\n",
    "        image_path = os.path.join(save_dir, f'{class_name}_{saved_counts[label]}.png')\n",
    "        save_image(image_tensor, image_path)\n",
    "        saved_counts[label] += 1\n",
    "\n",
    "        # Check if we have finished saving max_images for all classes\n",
    "        class_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            file.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "        # Check if we have finished saving the specified number of images for all classes\n",
    "        if all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "            break\n",
    "\n",
    "    print(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rlEluD6j2Zr",
    "outputId": "91c26ac2-c1ce-477e-ec44-f679ca9e8afe"
   },
   "outputs": [],
   "source": [
    "# save_cifar100_random_replay(trainset, 50, 'saved_data/cifar0411_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TGTjnkg03Dnn"
   },
   "outputs": [],
   "source": [
    "integer_to_name = {i: name for i, name in enumerate(name_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4feAAu63PEe",
    "outputId": "30d6c1d5-f867-444b-98ef-f77725a5de58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 apple\n"
     ]
    }
   ],
   "source": [
    "for id in integer_to_name:\n",
    "    print(id, integer_to_name[id])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VUd1jnlL4vV"
   },
   "source": [
    "# Data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZyQi87koaOjd"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch_pretrained_vit\n",
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# ?\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DkwtwPTByTNd"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, Resize\n",
    "import os\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "transform_train = Compose([\n",
    "    Resize((224, 224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats,inplace=True)\n",
    "])\n",
    "\n",
    "transform_test = Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    Normalize(*stats,inplace=True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a5YNVSvRJH6n"
   },
   "outputs": [],
   "source": [
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('logs/test.txt', 'w'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {
    "id": "Qx3UrGFcJH6o"
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics, class_accuracy_metrics\n",
    "\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "#     loss_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    class_accuracy_metrics(minibatch=False, epoch=False, epoch_running=False, experience=False, stream=True),\n",
    "    # forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {
    "id": "f3UKvdtpc9tL"
   },
   "outputs": [],
   "source": [
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRjpqXv9dR9V",
    "outputId": "c82c7a66-91d8-4f23-d9bc-ef1bf3a33cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "RNGManager.set_random_seeds(1234)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "checkpoint_plugin = CheckpointPlugin(\n",
    "    FileSystemCheckpointStorage(\n",
    "        directory='./checkpoints/task_cifar',\n",
    "    ),\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# Load checkpoint (if exists in the given storage)\n",
    "# If it does not exist, strategy will be None and initial_exp will be 0\n",
    "strategy, initial_exp = checkpoint_plugin.load_checkpoint_if_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {
    "id": "R-twpACd-97s"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def combine_files_with_numbers(folder, file_initial, numbers, output_folder):\n",
    "    \"\"\"use to get the data with label in the training experience\"\"\"\n",
    "    combined_content = \"\"  # Initialize an empty string to store combined content\n",
    "    # Compile a set of filenames to look for, based on the list of numbers\n",
    "    filenames_to_look_for = {file_initial + f\"{number}.txt\" for number in numbers}\n",
    "\n",
    "    # Iterate over each file in the specified folder\n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches exactly any in our set of filenames to look for\n",
    "        if file in filenames_to_look_for:\n",
    "            # Open and read the file, then add its content to the combined_content string\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                combined_content += f.read()  # Add a newline character after each file's content for better separation\n",
    "\n",
    "    joined_string = '_'.join(str(integer) for integer in numbers)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    output_file_path = output_folder +file_initial+ 'combined' + '_' + joined_string + '.txt'\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(combined_content)\n",
    "\n",
    "def shuffle_text_file_lines(file_path):\n",
    "    \"\"\"\n",
    "    Shuffles the lines in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the text file to shuffle.\n",
    "    \"\"\"\n",
    "    # Read the lines from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Shuffle the lines\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    # Write the shuffled lines back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux_kentvVEVO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVzdt5GsBMxC"
   },
   "source": [
    "# experiment 20 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {
    "id": "NjP1BTXZBMxC"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {
    "id": "6ztRwlSmBMxD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/55/enbo/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# resnet50 = torch.hub.load('ckpt/dino_resnet50_pretrain.pth', 'dino_resnet50')\n",
    "resnet_model = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "resnet_model.fc = nn.Identity()\n",
    "\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "# # Step 2: Create a new network class with an additional linear layer\n",
    "# class CustomNetwork(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_classes):\n",
    "#         super(CustomNetwork, self).__init__()\n",
    "#         self.pretrained_model = pretrained_model\n",
    "#         self.fc = nn.Linear(2048, num_classes)  # New linear layer with trainable weights\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Extract features using the pre-trained model\n",
    "#         features = self.pretrained_model(x)\n",
    "#         # Flatten the output for the linear layer if not already flattened\n",
    "#         features = torch.flatten(features, 1)\n",
    "#         # Pass features through the new linear layer\n",
    "#         output = self.fc(features)\n",
    "#         return output\n",
    "\n",
    "# # Step 3: Initialize the new network with the desired number of output classes\n",
    "# num_classes = 100  # Example: 10 classes for a new classification task\n",
    "# resnet_model = CustomNetwork(resnet_model, num_classes)\n",
    "\n",
    "# # The only parameters that are trainable are those of the new linear layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from typing import Optional\n",
    "\n",
    "class DINOFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {
    "id": "lW0jcAVOBMxD"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIAHFiAiBMxD",
    "outputId": "17846555-d146-499f-da2a-caceb232aee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "benchmark = SplitCIFAR100(n_experiences=20,\n",
    "                          train_transform=transform_train,\n",
    "                          eval_transform = transform_test,\n",
    "                          seed = 41\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import TypeVar, Generic\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from avalanche.training.templates.base import BaseTemplate\n",
    "\n",
    "CallbackResult = TypeVar(\"CallbackResult\")\n",
    "Template = TypeVar(\"Template\", bound=\"BaseTemplate\")\n",
    "\n",
    "\n",
    "class BasePlugin(Generic[Template], ABC):\n",
    "    \"\"\"ABC for BaseTemplate plugins.\n",
    "\n",
    "    A plugin is simply an object implementing some strategy callbacks.\n",
    "    Plugins are called automatically during the strategy execution.\n",
    "\n",
    "    Callbacks provide access before/after each phase of the execution.\n",
    "    In general, for each method of the training and evaluation loops,\n",
    "    `StrategyCallbacks`\n",
    "    provide two functions `before_{method}` and `after_{method}`, called\n",
    "    before and after the method, respectively.\n",
    "    Therefore plugins can \"inject\" additional code by implementing callbacks.\n",
    "    Each callback has a `strategy` argument that gives access to the state.\n",
    "\n",
    "    In Avalanche, callbacks are used to implement continual strategies, metrics\n",
    "    and loggers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def before_training(self, strategy: Template, *args, **kwargs):\n",
    "        \"\"\"Called before `train` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_training_exp(self, strategy: Template, *args, **kwargs):\n",
    "        \"\"\"Called before `train_exp` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_training_exp(self, strategy: Template, *args, **kwargs):\n",
    "        \"\"\"Called after `train_exp` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_training(self, strategy: Template, *args, **kwargs):\n",
    "        \"\"\"Called after `train` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_eval(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `eval` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_eval_exp(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `eval_exp` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_eval_exp(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `eval_exp` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_eval(self, strategy: Template, *args, **kwargs) -> CallbackResult:\n",
    "        \"\"\"Called after `eval` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "class Knn_DinoPlugin(BasePlugin[Template], ABC):\n",
    "    \"\"\"ABC for BaseSGDTemplate plugins.\n",
    "\n",
    "    See `BaseSGDTemplate` for complete description of the train/eval loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def before_training_epoch(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `train_epoch` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_training_iteration(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before the start of a training iteration by the\n",
    "        `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_forward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `model.forward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_forward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `model.forward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_backward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `criterion.backward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_backward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `criterion.backward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_training_iteration(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after the end of a training iteration by the\n",
    "        `BaseTemplate`.\"\"\"\n",
    "        print('plugin')\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def before_update(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `optimizer.update()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_update(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `optimizer.update()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_training_epoch(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `train_epoch` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_eval_iteration(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before the start of a training iteration by the\n",
    "        `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def before_eval_forward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called before `model.forward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_eval_forward(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after `model.forward()` by the `BaseTemplate`.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def after_eval_iteration(\n",
    "        self, strategy: Template, *args, **kwargs\n",
    "    ) -> CallbackResult:\n",
    "        \"\"\"Called after the end of an iteration by the\n",
    "        `BaseTemplate`.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ih6YdsQ6BMxD",
    "outputId": "7f0201d3-1210-4927-d6d0-4ea74d562984"
   },
   "outputs": [],
   "source": [
    "# from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "# from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "# storage_p = ParametricBuffer(\n",
    "#     max_size=60000,\n",
    "#     groupby='class',\n",
    "#     selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "#     # selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "# for i in range(5):\n",
    "#     strategy_state = SimpleNamespace(experience=benchmark.train_stream[i])\n",
    "#     # print(len(benchmark.train_stream[i]))\n",
    "#     storage_p.update(strategy_state)\n",
    "#     print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "#     # print(f\"class targets: {storage_p.buffer.targets}\\n\")\n",
    "\n",
    "# storage_p = ParametricBuffer(\n",
    "#     max_size=60000,\n",
    "#     groupby='class',\n",
    "#     selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "#     # selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence, Optional, Union, List\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from avalanche.benchmarks import CLExperience, CLStream\n",
    "from avalanche.core import BaseSGDPlugin\n",
    "from avalanche.training.plugins import SupervisedPlugin, EvaluationPlugin\n",
    "from avalanche.training.plugins.clock import Clock\n",
    "from avalanche.training.plugins.evaluation import default_evaluator\n",
    "from avalanche.training.templates.base import BaseTemplate, ExpSequence\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.benchmarks.utils.data_loader import TaskBalancedDataLoader, \\\n",
    "    collate_from_data_or_kwargs\n",
    "from avalanche.training.utils import trigger_plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN_storagePlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KNN_storagePlugin(SupervisedPlugin):\n",
    "    \"\"\"\n",
    "    Experience replay plugin.\n",
    "\n",
    "    Handles an external memory filled with randomly selected\n",
    "    patterns and implementing `before_training_exp` and `after_training_exp`\n",
    "    callbacks.\n",
    "    The `before_training_exp` callback is implemented in order to use the\n",
    "    dataloader that creates mini-batches with examples from both training\n",
    "    data and external memory. The examples in the mini-batch is balanced\n",
    "    such that there are the same number of examples for each experience.\n",
    "\n",
    "    The `after_training_exp` callback is implemented in order to add new\n",
    "    patterns to the external memory.\n",
    "\n",
    "    The :mem_size: attribute controls the total number of patterns to be stored\n",
    "    in the external memory.\n",
    "\n",
    "    :param batch_size: the size of the data batch. If set to `None`, it\n",
    "        will be set equal to the strategy's batch size.\n",
    "    :param batch_size_mem: the size of the memory batch. If\n",
    "        `task_balanced_dataloader` is set to True, it must be greater than or\n",
    "        equal to the number of tasks. If its value is set to `None`\n",
    "        (the default value), it will be automatically set equal to the\n",
    "        data batch size.\n",
    "    :param task_balanced_dataloader: if True, buffer data loaders will be\n",
    "            task-balanced, otherwise it will create a single dataloader for the\n",
    "            buffer samples.\n",
    "    :param storage_policy: The policy that controls how to add new exemplars\n",
    "                           in memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mem_size: int = 200,\n",
    "        batch_size: int = None,\n",
    "        batch_size_mem: int = None,\n",
    "        task_balanced_dataloader: bool = False,\n",
    "        storage_policy: Optional[\"ExemplarsBuffer\"] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_mem = batch_size_mem\n",
    "        self.task_balanced_dataloader = task_balanced_dataloader\n",
    "\n",
    "        if storage_policy is not None:  # Use other storage policy\n",
    "            self.storage_policy = storage_policy\n",
    "            assert storage_policy.max_size == self.mem_size\n",
    "        else:  # Default\n",
    "            self.storage_policy = ExperienceBalancedBuffer(\n",
    "                max_size=self.mem_size, adaptive_size=True\n",
    "            )\n",
    "#         self.accuracy_metric = AccuracyMetric(task='multiclass')\n",
    "\n",
    "    @property\n",
    "    def ext_mem(self):\n",
    "        return self.storage_policy.buffer_groups  # a Dict<task_id, Dataset>\n",
    "\n",
    "    def before_training_exp(\n",
    "        self,\n",
    "        strategy: \"SupervisedTemplate\",\n",
    "        num_workers: int = 0,\n",
    "        shuffle: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataloader to build batches containing examples from both memories and\n",
    "        the training dataset\n",
    "        \"\"\"\n",
    "        if len(self.storage_policy.buffer) == 0:\n",
    "            # first experience. We don't use the buffer, no need to change\n",
    "            # the dataloader.\n",
    "            buffer_size = len(self.storage_policy.buffer)\n",
    "            print(\"buffer size: \" + str(buffer_size))\n",
    "            return\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        if batch_size is None:\n",
    "            batch_size = strategy.train_mb_size\n",
    "\n",
    "        batch_size_mem = self.batch_size_mem\n",
    "        if batch_size_mem is None:\n",
    "            batch_size_mem = strategy.train_mb_size\n",
    "\n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.storage_policy.buffer,\n",
    "            oversample_small_tasks=True,\n",
    "            batch_size=batch_size,\n",
    "            batch_size_mem=batch_size_mem,\n",
    "            task_balanced_dataloader=self.task_balanced_dataloader,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"buffer size: \" + str(buffer_size))\n",
    "\n",
    "    def after_training_exp(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        self.storage_policy.update(strategy, **kwargs)\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"after training exp buffer size: \" + str(buffer_size))\n",
    "#     def after_eval_iteration(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "#         \"\"\"\n",
    "#         Calculate and log the accuracy after each evaluation iteration.\n",
    "#         \"\"\"\n",
    "#         # Access the predictions and true labels\n",
    "#         predictions = strategy.mb_output\n",
    "#         true_labels = strategy.mb_y\n",
    "\n",
    "#         # Update the accuracy metric\n",
    "#         self.accuracy_metric.update(predictions, true_labels)\n",
    "\n",
    "#         # Log the current accuracy\n",
    "#         current_accuracy = self.accuracy_metric.result()\n",
    "#         print(f\"Current accuracy: {current_accuracy * 100:.2f}%\")\n",
    "#         self.accuracy_metric.reset()  # Reset for next iteration or experience\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN_DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KNN_DINO1(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.train()\n",
    "        print(len(self.adapted_dataset))\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the current model on a series of experiences and\n",
    "        returns the last recorded value for each metric.\n",
    "\n",
    "        :param exp_list: CL experience information.\n",
    "        :param kwargs: custom arguments.\n",
    "\n",
    "        :return: dictionary containing last recorded value for\n",
    "            each metric name\n",
    "        \"\"\"\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for data, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                data = data.to(self.device)\n",
    "                features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/55/enbo/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "from avalanche.training.plugins import ReplayPlugin\n",
    "\n",
    "\n",
    "replay_plugin = KNN_storagePlugin(mem_size=60000, storage_policy = storage_p)\n",
    "dino_model = DINOFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Module,\n",
    "# #         optimizer: Optimizer,\n",
    "# #         criterion=CrossEntropyLoss(),\n",
    "#         train_mb_size: int = 1,\n",
    "#         train_epochs: int = 1,\n",
    "#         eval_mb_size: Optional[int] = 1,\n",
    "#         device=\"cpu\",\n",
    "#         plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "#         evaluator: EvaluationPlugin = default_evaluator(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/55/enbo/miniconda3/envs/thesis3.7/lib/python3.7/site-packages/avalanche/training/templates/base.py:219: UserWarning: Plugin <__main__.KNN_storagePlugin object at 0x7f4c58873190> implements incompatible callbacks for template <__main__.KNN_DINO1 object at 0x7f4c58853750>. This may result in errors. Incompatible callbacks: {'before_train_dataset_adaptation', 'after_eval_dataset_adaptation', 'after_train_dataset_adaptation', 'before_eval_dataset_adaptation'}\n",
      "  f\"Plugin {p} implements incompatible callbacks for template\"\n"
     ]
    }
   ],
   "source": [
    "cl_strategy = KNN_DINO1(\n",
    "    model=dino_model,\n",
    "    train_mb_size=512, # 32\n",
    "    train_epochs=1,\n",
    "    eval_mb_size=512, # 16\n",
    "    device=device,\n",
    "    evaluator=eval_plugin,\n",
    "    plugins=[replay_plugin]  # Use the KNN plugin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl_strategy = Naive(\n",
    "#     resnet_model, torch.optim.SGD(resnet_model.fc.parameters(), lr=0.01, momentum = 0.9),\n",
    "#     CrossEntropyLoss(), train_mb_size=32, train_epochs=1, eval_mb_size=16,\n",
    "#     # eval_every=500,\n",
    "#     device=device,\n",
    "#     evaluator=eval_plugin,\n",
    "#     plugins=[CustomReplay_SD(mem_size=60000, storage_policy = storage_p, \n",
    "#                              image_folder= 'saved_data/sd_turbo_i2i_50all_step20')]\n",
    "#     )\n",
    "# knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "storage_p = ParametricBuffer(\n",
    "    max_size=60000,\n",
    "    groupby='class',\n",
    "    selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "    # selection_strategy=RandomExemplarsSelectionStrategy()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 36, 5, 20, 54]\n",
      "-- >> Start of training phase << --\n",
      "2500\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7f4c58853550>\n",
      "buffer size: 0\n",
      "_before_training_epoch\n",
      "training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7f4c58853550>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 2500\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:33<00:00, 33.52s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9540\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.40s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.26s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.29s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.47s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.99s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.85s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.80s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.35s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.35s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.40s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.51s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.54s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.50s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.44s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.43s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.44s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.41s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.46s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:03<00:00,  3.71s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0477\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 0.9800\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.8900\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 0.9800\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.9500\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 0.9700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "None\n",
      "Start of experience:  1\n",
      "Current Classes:  [13, 45, 83, 19, 22]\n",
      "-- >> Start of training phase << --\n",
      "2500\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7f4c586f4450>\n",
      "buffer size: 2500\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.ReplayDataLoader object at 0x7f4c588730d0>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 5000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:10<00:00, 10.90s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9020\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:06<00:00,  6.10s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.8480\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.80s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.77s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.70s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.68s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.66s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.83s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.78s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.76s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.74s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:06<00:00,  6.04s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:06<00:00,  6.68s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.79s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.78s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.63s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.79s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.61s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.95s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.92s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0875\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 0.9600\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 0.9400\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.8500\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.8700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 0.7600\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 0.9000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.8800\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.9100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 0.8700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.8100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "None\n",
      "Start of experience:  2\n",
      "Current Classes:  [33, 73, 16, 26, 62]\n",
      "-- >> Start of training phase << --\n",
      "2500\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7f4db6b00610>\n",
      "buffer size: 5000\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.ReplayDataLoader object at 0x7f4ce2877710>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 7500\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:18<00:00, 18.93s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.8920\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:09<00:00,  9.31s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.7920\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.85s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.8060\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.58s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.80s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.26s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.08s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:07<00:00,  7.97s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.35s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.10s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:07<00:00,  7.76s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:07<00:00,  7.81s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:09<00:00,  9.15s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.69s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.41s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:07<00:00,  7.62s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.65s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.51s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:09<00:00,  9.01s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:08<00:00,  8.51s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1245\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 0.9600\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 0.9200\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.8400\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.8100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.8700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 0.7400\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.6100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.9300\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 0.8600\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.7200\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.9300\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 0.8400\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.7800\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.8700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.7700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "None\n",
      "Start of experience:  3\n",
      "Current Classes:  [34, 98, 74, 53, 24]\n",
      "-- >> Start of training phase << --\n",
      "2500\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7f4db6b11f10>\n",
      "buffer size: 7500\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.ReplayDataLoader object at 0x7f4ce2877710>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 10000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  10000\n",
      "cuda\n",
      "100%|| 1/1 [00:25<00:00, 25.31s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.8540\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  10000\n",
      "cuda\n",
      "100%|| 1/1 [00:10<00:00, 10.44s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.7380\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  10000\n",
      "cuda\n",
      "100%|| 1/1 [00:10<00:00, 10.79s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.7800\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  10000\n",
      "cuda\n",
      "100%|| 1/1 [00:10<00:00, 10.13s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.8260\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  10000\n",
      "cuda\n",
      "100%|| 1/1 [00:10<00:00, 10.21s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    test_re = cl_strategy.eval(benchmark.test_stream)\n",
    "    results.append(test_re)\n",
    "    print(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increase batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "def profile_training():\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_training\"):\n",
    "            for experience in benchmark.train_stream:\n",
    "            # Example: Run your training or evaluation loop here\n",
    "                cl_strategy.train(experience)\n",
    "                cl_strategy.eval(benchmark.test_stream)\n",
    "\n",
    "        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# Call the profiling function\n",
    "profile_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ruhDj4_LBy1o"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
