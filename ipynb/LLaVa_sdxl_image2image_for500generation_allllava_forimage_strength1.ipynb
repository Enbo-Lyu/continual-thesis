{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7bed34-8b0f-4a14-9682-ecfa8624ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.16\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3881a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current default GPU index: 0\n",
      "Current default GPU name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "if torch.cuda.is_available():\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"Current default GPU index: {current_gpu}\")\n",
    "    print(f\"Current default GPU name: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535be0ec-7017-4b24-8bba-1bc38c66b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'feature_extractor': [None, None], 'image_encoder': [None, None]} were passed to StableDiffusionXLImg2ImgPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'feature_extractor': [None, None], 'image_encoder': [None, None]} are not expected by StableDiffusionXLImg2ImgPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784a9e295dbd46edbeb09799028c6299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'reverse_transformer_layers_per_block': None} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionXLImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionXLImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.21.4\",\n",
       "  \"_name_or_path\": \"stabilityai/sdxl-turbo\",\n",
       "  \"force_zeros_for_empty_prompt\": true,\n",
       "  \"requires_aesthetics_score\": false,\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerAncestralDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"text_encoder_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"tokenizer_2\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "import torch\n",
    "\n",
    "pipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece21fe",
   "metadata": {},
   "source": [
    "## image generation with sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cc4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import torchvision\n",
    "\n",
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader, ReplayDataLoader\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger, TensorboardLogger\n",
    "\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "\n",
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "from types import SimpleNamespace\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "\n",
    "# all imports\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset, ConcatDataset, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim.lr_scheduler # ?\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, CenterCrop, RandomHorizontalFlip, Resize\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d08336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b973f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "#                                          download=True)\n",
    "\n",
    "# name_list = trainset.classes\n",
    "\n",
    "# integer_to_name = {i: name for i, name in enumerate(name_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfe5e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_specific_cifar100(dataset, num_images_per_class, save_dir, real_list):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.manual_seed(41)\n",
    "\n",
    "    saved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "\n",
    "    transform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        if label in real_list:\n",
    "            # Check if image is already a tensor, transform if not\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image_tensor = transform_to_tensor(image)\n",
    "            else:\n",
    "                image_tensor = image  # Use the image directly if it is already a tensor\n",
    "\n",
    "            # Skip saving if this class already has the desired number of images saved\n",
    "            if saved_counts[label] >= num_images_per_class:\n",
    "                continue\n",
    "\n",
    "            class_name = dataset.classes[label]\n",
    "            image_path = os.path.join(save_dir, f'{class_name}{saved_counts[label]}.png')\n",
    "            save_image(image_tensor, image_path)\n",
    "            saved_counts[label] += 1\n",
    "\n",
    "            # Save path to file\n",
    "            class_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "            with open(class_file_path, \"a\") as file:\n",
    "                file.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "            # Check if we have finished saving the specified number of images for all classes\n",
    "            if all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "                break\n",
    "\n",
    "    print(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")\n",
    "\n",
    "def count_txt_files(directory):\n",
    "    \"\"\"\n",
    "    Counts the number of .txt files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The path to the directory to search for .txt files.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of .txt files in the directory.\n",
    "    \"\"\"\n",
    "    txt_count = 0\n",
    "    # List all files and directories in the specified directory\n",
    "    for entry in os.listdir(directory):\n",
    "        # Construct full path\n",
    "        full_path = os.path.join(directory, entry)\n",
    "        # Check if it's a file with a .txt extension\n",
    "        if os.path.isfile(full_path) and entry.endswith('.txt'):\n",
    "            txt_count += 1\n",
    "    \n",
    "    return txt_count\n",
    "\n",
    "def count_png_files(folder_path):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter and count files that end with .txt\n",
    "    txt_files_count = sum(1 for file in files if file.endswith('.png'))\n",
    "    \n",
    "    return txt_files_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b059e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "benchmark = SplitCIFAR100(n_experiences=20,\n",
    "                          seed = 41,             \n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbf9b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36, 0, 54, 5, 20], [22, 45, 13, 83, 19], [26, 73, 16, 62, 33], [34, 98, 24, 74, 53], [10, 94, 51, 4, 32], [38, 81, 50, 40, 41], [30, 89, 69, 64, 21], [84, 14, 88, 49, 68], [6, 80, 57, 65, 46], [9, 91, 48, 72, 31], [76, 7, 47, 8, 1], [61, 75, 63, 18, 86], [59, 70, 43, 85, 95], [27, 93, 35, 25, 82], [44, 56, 67, 66, 37], [60, 11, 2, 78, 52], [97, 39, 55, 3, 99], [29, 71, 23, 28, 90], [87, 15, 92, 17, 77], [12, 42, 96, 79, 58]]\n"
     ]
    }
   ],
   "source": [
    "orders = benchmark.classes_order\n",
    "order_list = [orders[x:x+5] for x in range(0, len(orders), 5)]\n",
    "print(order_list)\n",
    "\n",
    "order_sample = [order[3:] for order in order_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1541ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return the data.\n",
    "\n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def process_labels(json_data):\n",
    "    \"\"\"\n",
    "    Process the labels from the JSON data.\n",
    "\n",
    "    :param json_data: The JSON data containing labels and class names.\n",
    "    \"\"\"\n",
    "    for identifier, class_name in json_data.items():\n",
    "        print(f\"ID: {identifier}, Class Name: {class_name}\")\n",
    "\n",
    "def replace_spaces_with_underscores(class_names):\n",
    "    modified_names = []\n",
    "    for name in class_names:\n",
    "        # Replace spaces with underscores\n",
    "        modified_name = name.replace(' ', '_')\n",
    "        modified_names.append(modified_name)\n",
    "    return modified_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ef6250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "60\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[5, 20, 83, 19, 62, 33, 74, 53, 4, 32, 40, 41, 64, 21, 49, 68, 65, 46, 72, 31, 8, 1, 18, 86, 85, 95, 25, 82, 66, 37, 78, 52, 3, 99, 28, 90, 17, 77, 79, 58]\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR100\n",
    "\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform = transform_train = Compose([\n",
    "    # Resize(224),\n",
    "    # Resize(384),\n",
    "    # RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    # Normalize(*stats,inplace=True)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,download=True, transform=transform)\n",
    "\n",
    "name_list = trainset.classes\n",
    "integer_to_name = {i: name_list[i] for i in range(100)}\n",
    "syn_classes = [5, 20, 83, 19, 62, 33, 74, 53, 4, 32, 40, 41, 64, 21, 49, 68, 65, 46, 72, 31, 8, 1, 18, 86, 85, 95, 25, 82, 66, 37, 78, 52, 3, 99, 28, 90, 17, 77, 79, 58]\n",
    "real_classes = list(set([i for i in range(100)]) - set(syn_classes))\n",
    "print(len(real_classes))\n",
    "\n",
    "# sythnthesis classes\n",
    "benchmark = SplitCIFAR100(n_experiences=20,\n",
    "                          seed = 41,             \n",
    "                          )\n",
    "\n",
    "orders = benchmark.classes_order\n",
    "order_list = [orders[x:x+5] for x in range(0, len(orders), 5)]\n",
    "\n",
    "order_sample = [order[3:] for order in order_list]\n",
    "classname_list = []\n",
    "label_list = []\n",
    "classname_list_sep = []\n",
    "for order_l in order_sample:\n",
    "    label_list.append(order_l)\n",
    "    cur_classname = [integer_to_name[i] for i in order_l]\n",
    "    classname_list.append(cur_classname)\n",
    "classname_list_sep = [item for lists in classname_list for item in lists]\n",
    "label_list_sep = [item for lists in label_list for item in lists]\n",
    "print(label_list_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca41d9e9-a267-484c-9bc4-c10ee1a9d0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_to_syn_id = {5: 87, 20: 84, 83: 51, 19: 89, 62: 70, 33: 59, 74: 63, 53: 16, 4: 55, 32: 26, 40: 9, 41: 89, 64: 24, 21: 29, 49: 12, 68: 12, 65: 80, 46: 11, 72: 9, 31: 42, 8: 48, 1: 26, 18: 24, 86: 22, 85: 9, 95: 73, 25: 87, 82: 92, 66: 88, 37: 94, 78: 44, 52: 47, 3: 97, 99: 75, 28: 9, 90: 81, 17: 22, 77: 26, 79: 80, 58: 89}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59262939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_folder = 'saved_data/cifar_train500/'\n",
    "# count = count_txt_files(output_folder)\n",
    "# print(f\"There are {count} .txt files in the folder.\")\n",
    "# count = count_png_files(output_folder)\n",
    "# print(f\"There are {count} .png files in the folder.\")\n",
    "\n",
    "# save_specific_cifar100(trainset, 500, 'saved_data/cifar_train500_2real_fori2i', label_list_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed410e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified folder does not exist.\n",
      "There are 0 .txt files in the folder.\n"
     ]
    }
   ],
   "source": [
    "def count_txt_files(folder_path):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter and count files that end with .txt\n",
    "    txt_files_count = sum(1 for file in files if file.endswith('.txt'))\n",
    "    \n",
    "    return txt_files_count\n",
    "\n",
    "# Specify the folder path here\n",
    "folder_path = 'saved_data/cifar_train500_2real_fori2i'\n",
    "\n",
    "# Call the function and print the result\n",
    "count = count_txt_files(folder_path)\n",
    "print(f\"There are {count} .txt files in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fce296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def extract_prompts_fromtxts(file_path):\n",
    "    prompts = []  # List to hold the prompts\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip() != 'USER':  # Ignore lines that are just 'USER'\n",
    "                # Split the line at the first colon and strip whitespace from the prompt\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) > 1:\n",
    "                    prompt = parts[1].strip()\n",
    "                    prompts.append(prompt)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def create_sdxl_data_fixed_prompts_randommultiple(class_dict, image_size, prompt_file_dict, generator_seed, num_image_replay=50, folder_name=\"/content/sd_images\"):\n",
    "    # Create the folder if it doesn't exist\n",
    "    random.seed(42)\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        if os.path.exists(class_file_path):\n",
    "            with open(class_file_path, \"r\") as file:\n",
    "                existing_images_count = len(file.readlines())\n",
    "            num_images_to_generate = num_image_replay - existing_images_count\n",
    "            \n",
    "            if existing_images_count >= num_image_replay:\n",
    "                print(f\"Class {class_name} already contains {num_image_replay} images. Moving to the next class.\")\n",
    "                continue  # Move to the next class if enough images already exist\n",
    "        prompt_file_path = os.path.join(prompt_file_dict, f\"class{id}.txt\")\n",
    "        current_prompt_list = extract_prompts_fromtxts(prompt_file_path)\n",
    "        \n",
    "        indices = [random.randint(0, len(current_prompt_list)-1) for _ in range(num_image_replay)]\n",
    "        current_prompt_list = [current_prompt_list[item] for item in indices]\n",
    "\n",
    "        # Generate the new images\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            for j, prompt in enumerate(current_prompt_list):\n",
    "                print(prompt)\n",
    "                image_name = class_name + f\"{j}.png\"\n",
    "                image_path = os.path.join(folder_name, image_name)\n",
    "                new_image = pipe(prompt=prompt, generator = generator_seed, num_inference_steps=1, guidance_scale=1.0).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(image_path)\n",
    "\n",
    "                file.write(f\"{image_path} {id}\\n\")\n",
    "\n",
    "                print(f\"Generated image {image_name} for class {class_name}\")\n",
    "\n",
    "\n",
    "def create_sdxl_data_fixed_prompts(class_dict, image_size, prompt_file_dict, generator_seed, num_image_replay=50, folder_name=\"/content/sd_images\"):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        if os.path.exists(class_file_path):\n",
    "            with open(class_file_path, \"r\") as file:\n",
    "                existing_images_count = len(file.readlines())\n",
    "            num_images_to_generate = num_image_replay - existing_images_count\n",
    "            \n",
    "            if existing_images_count >= num_image_replay:\n",
    "                print(f\"Class {class_name} already contains {num_image_replay} images. Moving to the next class.\")\n",
    "                continue  # Move to the next class if enough images already exist\n",
    "        prompt_file_path = os.path.join(prompt_file_dict, f\"class{id}.txt\")\n",
    "        current_prompt_list = extract_prompts_fromtxts(prompt_file_path)\n",
    "\n",
    "        # Generate the new images\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            for j, prompt in enumerate(current_prompt_list):\n",
    "                print(prompt)\n",
    "                image_name = class_name + f\"{j}.png\"\n",
    "                image_path = os.path.join(folder_name, image_name)\n",
    "                new_image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=1.0).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(image_path)\n",
    "\n",
    "                file.write(f\"{image_path} {id}\\n\")\n",
    "\n",
    "                print(f\"Generated image {image_name} for class {class_name}\")\n",
    "                \n",
    "                \n",
    "                \n",
    "def create_sdxl_data_fixed_prompts_randommultiple_img2img(class_dict, image_size, prompt_file_dict, startimage_file_path,\n",
    "                                                          generator_seed, num_image_replay=50, folder_name=\"/content/sd_images\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    not done, todo: provide image list as current_prompt_list\n",
    "    \"\"\"\n",
    "    # Create the folder if it doesn't exist\n",
    "#     random.seed(42)\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "        \n",
    "        path_file = os.path.join(startimage_file_path, f\"class{id}.txt\")\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        with open(path_file, 'r') as file:\n",
    "            startimage_file_paths = [line.split()[0] for line in file.readlines()]\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        if os.path.exists(class_file_path):\n",
    "            with open(class_file_path, \"r\") as file:\n",
    "                existing_images_count = len(file.readlines())\n",
    "            num_images_to_generate = num_image_replay - existing_images_count\n",
    "            \n",
    "            if existing_images_count >= num_image_replay:\n",
    "                print(f\"Class {class_name} already contains {num_image_replay} images. Moving to the next class.\")\n",
    "                continue  # Move to the next class if enough images already exist\n",
    "        prompt_file_path = os.path.join(prompt_file_dict, f\"class{id}.txt\")\n",
    "        current_prompt_list = extract_prompts_fromtxts(prompt_file_path)\n",
    "        \n",
    "        random.seed(42)\n",
    "        indices1 = [random.randint(0, len(current_prompt_list)-1) for _ in range(num_image_replay)]\n",
    "        current_prompt_list = [current_prompt_list[item] for item in indices1]\n",
    "\n",
    "        random.seed(41)\n",
    "        indices2 = [random.randint(0, len(startimage_file_paths)-1) for _ in range(num_image_replay)]\n",
    "        current_init_image_list = [startimage_file_paths[item] for item in indices2]\n",
    "\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            for j, prompt in enumerate(current_prompt_list):\n",
    "#                 print(prompt)\n",
    "                \n",
    "                starting_image_path = current_init_image_list[j]\n",
    "                init_image = load_image(starting_image_path).resize((512, 512))\n",
    "\n",
    "                image_name = starting_image_path.split('/')[-1]\n",
    "                image_path = os.path.join(folder_name, image_name)\n",
    "                new_image = pipe(prompt=prompt,image = init_image, \n",
    "                                strength = 0.5, \n",
    "                                num_inference_steps=40, \n",
    "                                generator = generator_seed, guidance_scale=2).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(image_path)\n",
    "\n",
    "                file.write(f\"{image_path} {id}\\n\")\n",
    "\n",
    "            print(f\"Generated images for class {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf59f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sdxl_data_img2img_matching_image_prompt(class_dict, image_size, prompt_file_dict, startimage_file_path,\n",
    "                                                          generator_seed, num_image_replay=50, folder_name=\"/content/sd_images\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    this is specified to matching pair of lavva prompt and images\n",
    "    \"\"\"\n",
    "    # Create the folder if it doesn't exist\n",
    "    random.seed(42)\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        if os.path.exists(class_file_path):\n",
    "            with open(class_file_path, \"r\") as file:\n",
    "                existing_images_count = len(file.readlines())\n",
    "            num_images_to_generate = num_image_replay - existing_images_count\n",
    "            \n",
    "            if existing_images_count >= num_image_replay:\n",
    "                print(f\"Class {class_name} already contains {num_image_replay} images. Moving to the next class.\")\n",
    "                continue  # Move to the next class if enough images already exist\n",
    "        prompt_file_path = os.path.join(prompt_file_dict, f\"class{id}.txt\")\n",
    "        current_prompt_list = extract_prompts_fromtxts(prompt_file_path)\n",
    "\n",
    "        # Generate the new images\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            for j, prompt in enumerate(current_prompt_list):\n",
    "                print(prompt)\n",
    "                image_name = class_name + f\"{j}.png\"\n",
    "                \n",
    "                starting_image_path = os.path.join(startimage_file_path, image_name)\n",
    "                init_image = load_image(starting_image_path).resize((512, 512))\n",
    "\n",
    "            \n",
    "                image_path = os.path.join(folder_name, image_name)\n",
    "                new_image = pipe(prompt=prompt,image = init_image, \n",
    "                                strength = 0.5, \n",
    "                                num_inference_steps=4, \n",
    "                                generator = generator_seed, guidance_scale=0.5).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(image_path)\n",
    "\n",
    "                file.write(f\"{image_path} {id}\\n\")\n",
    "\n",
    "                print(f\"Generated image {image_name} for class {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96860e49-5da4-482e-bef6-41a63901fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sdxl_llava_random(class_dict, image_size, generator_seed, num_image_replay=50, prompt_folder = \"saved_data/llava_saved_data_310/llava_prompt_50\", folder_name=\"/content/sd_images\"):\n",
    "    random.seed(42)\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        if os.path.exists(class_file_path):\n",
    "            with open(class_file_path, \"r\") as file:\n",
    "                existing_images_count = len(file.readlines())\n",
    "            num_images_to_generate = num_image_replay - existing_images_count\n",
    "            \n",
    "            if existing_images_count >= num_image_replay:\n",
    "                print(f\"Class {class_name} already contains {num_image_replay} images. Moving to the next class.\")\n",
    "                continue  # Move to the next class if enough images already exist\n",
    "\n",
    "        indices = [random.randint(0, 9) for _ in range(num_image_replay)]  # Assuming there are 10 prompts\n",
    "        prompts = generate_prompts(class_name, indices)\n",
    "\n",
    "        # Generate the new images\n",
    "        with open(class_file_path, \"a\") as file:\n",
    "            for j, prompt in enumerate(prompts):\n",
    "                print(prompt)\n",
    "                image_name = class_name + f\"{j}.png\"\n",
    "                image_path = os.path.join(folder_name, image_name)\n",
    "                new_image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=1.0).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(image_path)\n",
    "\n",
    "                file.write(f\"{image_path} {id}\\n\")\n",
    "\n",
    "                print(f\"Generated image {image_name} for class {class_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3acacfe-d6ff-4196-89eb-995eed5f2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_fromtxts_real2syn(file_path):\n",
    "    path = []\n",
    "    prompts = []  # List to hold the prompts\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) > 1:\n",
    "                img_path = parts[0].strip()\n",
    "                prompt = parts[1].strip()\n",
    "                path.append(img_path)\n",
    "                prompts.append(prompt)\n",
    "    return path, prompts\n",
    "\n",
    "def create_sdxl_data_img2img_matching_image_prompt_real2syn(class_dict, image_size, prompt_file_dict, startimage_file_path, integer_to_name, dict_syn_to_real_id,\n",
    "                                                          generator_seed, num_image_replay=50, folder_name=\"/content/sd_images\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    this is specified to matching pair of lavva prompt and images\n",
    "    \"\"\"\n",
    "    # Create the folder if it doesn't exist\n",
    "    random.seed(42)\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for id, class_name in class_dict.items():\n",
    "        # id, class_name are synthetic id and classname\n",
    "        print(\"Generating images for class \" + str(id) + \": \" + class_name)\n",
    "\n",
    "        class_file_path = os.path.join(folder_name, f\"class{id}.txt\")\n",
    "        existing_images_count = 0\n",
    "\n",
    "        # Check how many images already exist for the class\n",
    "        \n",
    "        num_images_to_generate = num_image_replay\n",
    "        \n",
    "        real_id = dict_syn_to_real_id[id] # this is the real class id that bert thinks is most similar to the current syn class\n",
    "        real_class_name = integer_to_name[real_id]\n",
    "        prompt_file_path = os.path.join(prompt_file_dict, f\"class{real_id}.txt\")\n",
    "        current_path_list, current_prompt_list = extract_prompts_fromtxts_real2syn(prompt_file_path)\n",
    "\n",
    "        # Generate the new images\n",
    "        with open(class_file_path, \"w\") as file:\n",
    "            for j, prompt in enumerate(current_prompt_list):\n",
    "                # print(prompt)\n",
    "                syn_prompt = prompt.replace(real_class_name, class_name)\n",
    "                # print(prompt)\n",
    "                syn_image_name = class_name + f\"{j}.png\"\n",
    "                # real_image_name = real_class_name + f\"{j}.png\"\n",
    "                # starting from real image\n",
    "                # starting_image_path = os.path.join(startimage_file_path, real_image_name)\n",
    "                starting_image_path = current_path_list[j]\n",
    "                init_image = load_image(starting_image_path).resize((512, 512))\n",
    "\n",
    "                # generated image is synthetic name\n",
    "                generated_image_path = os.path.join(folder_name, syn_image_name)\n",
    "                # int(num_inference_steps * strength)\n",
    "                new_image = pipe(prompt=syn_prompt,image = init_image, \n",
    "                                strength = 1, \n",
    "                                num_inference_steps=10, \n",
    "                                generator = generator_seed, guidance_scale=8).images[0]\n",
    "                resized_image = new_image.resize(image_size)\n",
    "\n",
    "                resized_image.save(generated_image_path)\n",
    "\n",
    "                file.write(f\"{generated_image_path} {id}\\n\")\n",
    "\n",
    "                print(f\"Generated image {syn_image_name} for class {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79226b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_integer_to_name = {key: integer_to_name[key] for key in label_list_sep if key in integer_to_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6114085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 'bed', 20: 'chair', 83: 'sweet_pepper', 19: 'cattle', 62: 'poppy', 33: 'forest', 74: 'shrew', 53: 'orange', 4: 'beaver', 32: 'flatfish', 40: 'lamp', 41: 'lawn_mower', 64: 'possum', 21: 'chimpanzee', 49: 'mountain', 68: 'road', 65: 'rabbit', 46: 'man', 72: 'seal', 31: 'elephant', 8: 'bicycle', 1: 'aquarium_fish', 18: 'caterpillar', 86: 'telephone', 85: 'tank', 95: 'whale', 25: 'couch', 82: 'sunflower', 66: 'raccoon', 37: 'house', 78: 'snake', 52: 'oak_tree', 3: 'bear', 99: 'worm', 28: 'cup', 90: 'train', 17: 'castle', 77: 'snail', 79: 'spider', 58: 'pickup_truck'}\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(specific_integer_to_name)\n",
    "print(len(specific_integer_to_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b4a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def list_txt_files(folder_path):\n",
    "#     # Check if the provided folder path exists\n",
    "#     if not os.path.isdir(folder_path):\n",
    "#         print(f\"The folder path '{folder_path}' does not exist.\")\n",
    "#         return\n",
    "\n",
    "#     # List all files in the folder\n",
    "#     files = os.listdir(folder_path)\n",
    "    \n",
    "#     # Filter out .txt files\n",
    "#     txt_files = [file for file in files if file.endswith('.txt')]\n",
    "    \n",
    "#     # Print the .txt files\n",
    "#     if txt_files:\n",
    "#         print(\"TXT files in the folder:\")\n",
    "#         for txt_file in txt_files:\n",
    "#             print(txt_file)\n",
    "#         print(len(txt_files))\n",
    "#     else:\n",
    "#         print(\"No TXT files found in the folder.\")\n",
    "\n",
    "# # Replace 'your_folder_path_here' with the actual folder path\n",
    "# folder_path = 'saved_data/ImageNet/ImageNet_sdxl_llavaprompt_1300_3real_i2i'\n",
    "# list_txt_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8787116-c25c-4ce0-8c00-619b0ac764a3",
   "metadata": {},
   "source": [
    "## mixed data llava multiple generation image2image 500 real2syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecc909",
   "metadata": {},
   "source": [
    "## strength 1 guide 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e9db5-17f1-4650-b70b-0c4695bbde33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (88 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lost in thought while standing in front of the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class 5: bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (88 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lost in thought while standing in front of the bed.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff10989f29ba4b67a7961cb0a2bdbe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['create a relaxing and enjoyable atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['create a relaxing and enjoyable atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed0.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b549614db4af4cebb32f61c0942a6219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed1.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eca959987404aab965085f48f35f89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed represents a classic piece of technology from the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed represents a classic piece of technology from the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed2.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e689aff1184f4a73b9e68186aabe4ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed3.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3110adb90723424eb5d669a913c9775c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and it serves as the focal point of the room, bringing the action of the baseball game to life.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and it serves as the focal point of the room, bringing the action of the baseball game to life.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed4.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89467ce6601a43c9b5ca41eb10ffac41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and cozy setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and cozy setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed5.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e07a10b9a514bb9a248c69eed23733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['away. the scene conveys a sense of relaxation and leisure, as the people are likely enjoying their time watching the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['away. the scene conveys a sense of relaxation and leisure, as the people are likely enjoying their time watching the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed6.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bbbcfa4dc1460f845dc40253b1695a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed7.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b4ba207d4453d8949009be7ded8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed8.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4f0a5f7e9b47a3a115d84f3d317820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely to be enjoying their favorite shows or movies on this small, yet functional, device.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely to be enjoying their favorite shows or movies on this small, yet functional, device.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed9.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275607d89cc47d0813748e046f37f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed10.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637fe9687fd444a593fbb5704e891d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed11.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b874edea5d432b888b333fa87cd9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed being the focal point amidst the clutter.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed being the focal point amidst the clutter.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed12.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b560719b0842b7bf7b90be380c3667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['times.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['times.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed13.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67bcfc5822644b6a358c4cf2d95297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed14.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bdc9cbe0484e55b936d2feae678d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed15.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069d0c53e4b94e11b94ec685e43690fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed16.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97119efb46a4c379ccce8f642f038d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of nostalgia and a connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of nostalgia and a connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed17.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55ebb9def394a4fa44536ab3cd44243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['point for entertainment and relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['point for entertainment and relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed18.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33915c3b7a364273af0ad4a003e8d40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed19.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce75b004a2943b2add1db20893a1a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era of entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era of entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed20.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8c12a459dc40ef873ea6f165638564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment and leisure for the occupants.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment and leisure for the occupants.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed21.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1a1b3ddb845849d5bbc9e6b1ca90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed22.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4095cc8f1a87420b9f0f09aadaec38bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed23.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b726086d8249a88b2ed619874ac5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed24.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fa0611fec54b7fa89b5b48e1ea9570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed25.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec304cc6cee49c88b27ef7686515b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed26.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea0b4bef0ad4cd6b4090ddcfbad19b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed27.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a901cbdec34c21a7ea1c831593784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed28.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a82330c32143f687283964a335e9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"atmosphere may be influenced by the bed's presence, as it could be a source of entertainment or a means of communication for the people in the room.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"atmosphere may be influenced by the bed's presence, as it could be a source of entertainment or a means of communication for the people in the room.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed29.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e90c508936b402fa3de948ee7fb1141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed. the overall atmosphere of the image is one of entertainment and leisure, as the bed is the focal point of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed. the overall atmosphere of the image is one of entertainment and leisure, as the bed is the focal point of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed30.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3d20ac5e764efe966884b66462e560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed31.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef511eea79a8436a8780daf64d831317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed32.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06627a479f64eb687adb4e5d6bd167e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in front of the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in front of the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed33.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a4e928731439bbfe07af0c6ce90b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed34.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0229e385fd24da6bfa1712d1ffafe63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed35.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0253071e4e4df3b249a4e568866897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed36.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79c5ac47f5542a5acb35995b3f6aaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed37.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962cd2d676a0433f966daeff995ab811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed38.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d9e467c5f54871a58c7e8ab271b520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"image, drawing the viewer's attention. the room appears to be a comfortable living space, with the bed serving as a central entertainment hub.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"image, drawing the viewer's attention. the room appears to be a comfortable living space, with the bed serving as a central entertainment hub.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed39.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5980976184e049d291867af4581c4703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed40.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b86ddfc20f47febfe364dd1fabfc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed41.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9084adecd144d36a3d9cb953f3844e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment purposes, such as watching movies or playing video games. the overall atmosphere of the image is one of leisure and relaxation, as the viewer is likely to associate the bed with their own personal experiences of watching their favorite shows or movies.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment purposes, such as watching movies or playing video games. the overall atmosphere of the image is one of leisure and relaxation, as the viewer is likely to associate the bed with their own personal experiences of watching their favorite shows or movies.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed42.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e64dbf05884fd8a2234b6876909eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wall provides a clean and neutral backdrop for the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wall provides a clean and neutral backdrop for the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed43.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e975fde8e423463ca4940f19d428cc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed44.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c98b2dffb34731948f90d1ce3a51c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed45.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46e02a174ed4c42bfb006d30c7a1f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed46.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ade83e4383413fb3b06725d97d0584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed47.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85868253c5a437d9ffd6a9e72c4cb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed48.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad30e3035794ab6a404f2672de61a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"capturing the viewer's attention and inviting them to imagine the rest of the room and the person's activities.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"capturing the viewer's attention and inviting them to imagine the rest of the room and the person's activities.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed49.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b74bc5db3db4d67b9c71e388fc82d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed50.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c4e274a9254062940fafe7d98a5464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed51.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ebfcb8823b4f3cb599d32281ab110e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed52.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5106cd246a4f989ea1178fdd7cb3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed53.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb768bb29a53420e805d2e40e86420f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gone era of bed technology.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gone era of bed technology.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed54.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5e73ef601944769f43941b9eaabe3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed55.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c01f13677c4c358988a130f322ec5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed56.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a811fcbefdf44ff81fffbbf7e647006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed57.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1254a18df9694be69e7caf04b40aaa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the screen adds a sense of relaxation and leisure to the overall atmosphere of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the screen adds a sense of relaxation and leisure to the overall atmosphere of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed58.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b43bc37751045038a7340840757f7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer. the overall atmosphere of the image is cozy and inviting, as if one is about to enjoy a meal or a movie in the comfort of their own home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer. the overall atmosphere of the image is cozy and inviting, as if one is about to enjoy a meal or a movie in the comfort of their own home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed59.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3c7601c02a41a2bf968292f376ad9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed60.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65426f1c7f1d49349b135ffb3b252298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed61.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21618ba9072b4d89a8aac2a15875825d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed62.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ee12a3621b440d96967fbc458ab8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed63.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1eb63d254f4b03a15431493ba643f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed64.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c00c9be6734c6f9d7dda558cc4f7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed65.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adb4e63a0a14cbf860a09d03e71aca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed66.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82341d5540344729d3643ab400976ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed67.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63c8d48681f44c5bbaa65b2a8082cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed68.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677a9162c4ca4b729c49397a95525832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed69.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f989da9633874e30a0d791e097887268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed70.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f994c509f54ee7ba66a2c2463dfa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relaxation and focus on the content being displayed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relaxation and focus on the content being displayed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed71.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ee2e6ad884bd184ce00351fc79fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed72.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501a4d4f595e4d069949cf13af1b96c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed73.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6227d6ce299344a590947206470de56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed74.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3331539115f74141bc3c2d728ada0f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed75.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55393af326b54c10b855aa565dd8585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed76.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b680c897a964682ac66082a68743eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed77.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d89d47ac64142748c9cf85231d0b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed78.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031242f2c36f4f9ea4d97282dcac29cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to the past, as the older bed model is displayed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to the past, as the older bed model is displayed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed79.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1214906884244a079489fbc5d1ca8765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['visually appealing and harmonious scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['visually appealing and harmonious scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed80.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34730f6366184452b33e9eb2783d7af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed81.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ff78ef78f64f9ea13b4cbd1ea0f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era of entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era of entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed82.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9830459fe617489ea05045b023e49ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed83.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651e125c3ab04f7c8f4950072341251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed84.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f16493936764d9fbc1a3e8189925453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed85.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3f1e5b46b948adbc203eb3d0c1f8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed86.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8169a9185ec94995882f70e5c3ba5f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed87.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc961590313b4bdda020621963443535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed88.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee2364bdd5e427ca33491a54b446309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed89.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404510da50f9419d891fba96dd087d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed90.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6cffa35f0543158a0fc03bf3baf489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed91.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3a825a470c4deeb22cedfa41ad7572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['action of the baseball game.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['action of the baseball game.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed92.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e4ded4d7f44728a49bbae1f2a074c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of health and well - being, as well as a touch of whimsy and creativity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of health and well - being, as well as a touch of whimsy and creativity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed93.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539737ae515e4be685e92439a0b23a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed94.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae48c050062449a690a3e1bce391a360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed95.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2608df8ded448399906c8d0a36f3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it a focal point for the occupants.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it a focal point for the occupants.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed96.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1480e61699624b4593cdb394473cfb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed97.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f158f5bb695943df97246913bdd9143b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed98.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ead408e4104901b09bee1aa58ae509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed99.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36f46f7a64844f0a4d9df02979fe4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed100.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd6786ee1c74252af8954fc161ec8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed101.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053e5302b63041b79daffd3472350e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed102.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7365c4e8e16c43b48a622898e1f8ef5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed103.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0282e648ae174b46a5051c438ee89612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed104.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4936f5f736488e9aac091af310b57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed105.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d793309242a4ba6a16121aa7aa9b60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['those in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['those in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed106.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f55bd4aaf643f884a4899892418695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['form of entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['form of entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed107.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd3e0829eca4b489faa6f2bd634725c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed108.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54516c6209046818ea29453fee90aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed109.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9375aa40b34c6080601b98b0ca78d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the dark.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the dark.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed110.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a9c59eb12d4ab3b39191d276b0a996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed111.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc2ed7e0e64f6582b8920b88106533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', but the picture on the screen adds a touch of history to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', but the picture on the screen adds a touch of history to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed112.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d3ee5f5be44e6b9b3505318d61f441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed113.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627d63111d5041ee8b842764e5e5b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed114.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6494a5bbdb4e06a7b6f96e0d7c9ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed115.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001d3e592be9466f8d4c1ab54873d06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed116.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4e48e00a37469a8cfbff0192de17ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed117.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9659e97fd784d9f8b109272ee29c5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed118.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38c3d1a96634d2cb6d30e58c4897573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed119.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf849e2a932b4025a87805ebe66ce989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed120.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff688a38e4c4416abd0664fd2e18dd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed121.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0960d031ca4d4945a009508690917575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed122.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0ed44ec8d94f42b1db147ddddb0864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed123.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0b503563a646189b4b9acf26ba5baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed124.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb1898975b47daac7d71cdb232e77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed125.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698296dbbfd546248ffb19acdfb84c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed126.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91bff5d834c43efbb1249c2f1427c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"man's swinging motion is the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"man's swinging motion is the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed127.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579579b635d541c6861b40c4c456c1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and warmth to the overall atmosphere of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and warmth to the overall atmosphere of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed128.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fb3e38b62842c7b45e07040c6a3126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed129.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af07dfd06884371b0fbbdb0e81aab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed130.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679efdb19cbf4549aa062cc6b0eedb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed131.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a7d4a3b3c74fd5b0614cbf7c73dfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sunset, adding a touch of serenity and beauty to the atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sunset, adding a touch of serenity and beauty to the atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed132.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4785c971a5734fda97e1f59382a7bc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the focal point of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the focal point of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed133.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b107b2539f1e4f008a636eff7afb6578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['anyone watching.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['anyone watching.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed134.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d82cc09e4d04ec68383f2099c6346fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed135.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae55afc2fd94a5481b567cce1d39787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed136.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc2deb36c524569937e7bc8297200d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed137.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f84fbaf11c4b63b8d5f750175b8ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed138.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1669830a2f24ce48560d67de4e97ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed139.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98e683d3a43487cb10ea7893627deb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed140.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03da85b38d0043c38c3bf0a4058dd67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"to capture the viewer's attention.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"to capture the viewer's attention.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed141.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfb50ad1d114935aeb220dda705db2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the visuals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the visuals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed142.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab00a6d31b8e4d4a97c23a712382abbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed143.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52aab424b944b1b96cd217e658dcecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed144.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f61dd61d1e2433dbce5228b925b5d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed145.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9002e0f2174465a9dfbe966188b6384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that it is a central element in the space, possibly serving as a source of entertainment or information for the people living in the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that it is a central element in the space, possibly serving as a source of entertainment or information for the people living in the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed146.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2382e89953964f888180689ec3633acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is the only object in the frame, and it is not in use.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is the only object in the frame, and it is not in use.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed147.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5984a351357474c85fad8fca34f37d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ambiance of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ambiance of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed148.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9add7b0a271a42168d15e55d29e53026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed149.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da0dc0228d548f48c77b088eeacd566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['fully absorbed in the content on the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['fully absorbed in the content on the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed150.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498e96e960d249d19e40272bee7093dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed151.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7c7b4a49e248fdb68d031edde5932e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed152.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935b3386605345e4b41b74b18ef77faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed153.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a7c3c8520a4823844b096b693bc020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed154.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d411509044eb4bc140f74cb262aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creates a visually appealing and engaging atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creates a visually appealing and engaging atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed155.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfced3450e14b8b9ca8f62903ae0b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed156.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519a168b50c64df391fe782e191e469b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed157.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad623e2573544399c09eb4c107ed534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed158.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3f43eb926c49ca9365dbb7cc531277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed159.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509fd3ead6ba43c1bee4d188e986df51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed160.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877e32b4b7394e98bf4fadcd34e024b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['showcases a group of people enjoying their time at the beach or pool.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['showcases a group of people enjoying their time at the beach or pool.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed161.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a74fc319d664ddfb906644a0229a0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stand, and it is the center of attention in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stand, and it is the center of attention in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed162.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7121022d8fc43e6b74017cdc89a785a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed163.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ba46b7259640f39aea38cf4648de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed164.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88489279ce76484dbc24fef0025bbf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed165.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5e93ea05394b13b481d51b720c2a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed166.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a891d991b94642bf974fadde2f0f73b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed167.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c7d9247b1a4d109493c320e4384a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or people visible in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or people visible in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed168.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba92051f08f4e30a0aa4f8dcbf1f5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed169.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb8c8393e5f4daaa04cb6f9c955513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed170.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981131d0a47b4189bda8964e02a401e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed171.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2255854c517c4c9ba3fb3e4bf241a79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be at ease in their living space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be at ease in their living space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed172.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c3b4d021d94fea8e8fb24cf971abcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed173.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33148feecb5e4029b503434db414679f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed174.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527d5b0344b84d3fb771547f54419c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed175.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1c99d5be1c437387c5d8750a1ba1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed176.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fee9e050db745f9b67eb741239e5fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed177.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25525a552578403393d8c58f658ca6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed178.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cb3a318aa447a8a8932a86c2609831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed179.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb94d3b169364fa0a208b049a8210eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"it a practical and entertaining feature for the car's occupants.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"it a practical and entertaining feature for the car's occupants.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed180.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e722041887e4f95b0c9fac233bed35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed181.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0da1056ea1420ca5a390d756e7e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it evokes a sense of nostalgia and a connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it evokes a sense of nostalgia and a connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed182.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dbe740c03b403d8e02bc3acfa17b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and elegance of the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and elegance of the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed183.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9409f0889e2349a5855ac943d05c234e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed is a common household appliance that many people enjoy using.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed is a common household appliance that many people enjoy using.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed184.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5200e930b31437a9273f130a75384fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed185.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f46ff0186ee47579466a1529b0e648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed186.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e58cad68cb54ecca059cb9160e85ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed187.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad574d3f94042aaac538ee4e9ef5059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['displayed on the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['displayed on the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed188.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa86e6515a9a419084d4c2f4488fa998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed189.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3b4449de374e0084b86e7eead6f04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed190.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6d74fd27e4e1793049237ff145a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed191.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74341afcdca48bebf21f86629022fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed192.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e853aafe76c5450da8d289755cd7e570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed193.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2e2fae915a4e839143cc0f87db139f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed194.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabf74f9602e494bb4105aac94125c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['living area, with a dining table and a chair visible in the background. the overall atmosphere of the image is nostalgic and cozy, as it showcases a classic piece of home entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['living area, with a dining table and a chair visible in the background. the overall atmosphere of the image is nostalgic and cozy, as it showcases a classic piece of home entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed195.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c88adb38e452b93b24224bc09a039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relaxation, as the bed serves as a focal point in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relaxation, as the bed serves as a focal point in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed196.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e69ccca41a745f896642f841d59b3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relax and enjoy the content being displayed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['relax and enjoy the content being displayed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed197.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b195b350570242b7b4d7b653bdc050e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed198.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a23e2cd3a746c9ae9e6d1e1da9a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed199.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c42f10b35d4f48ba927dec93e4ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', where someone can relax and enjoy watching their favorite shows or movies.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', where someone can relax and enjoy watching their favorite shows or movies.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed200.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cceab8e59d4b989b5c39d206b8e9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed201.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3208880bbf8945a4906408c11664550f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room. the overall impression is one of comfort and familiarity, as if the viewer is stepping into their own home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room. the overall impression is one of comfort and familiarity, as if the viewer is stepping into their own home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed202.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ad24659b9247cb8a42d5712f6a4074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['program, and serves as the centerpiece of the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['program, and serves as the centerpiece of the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed203.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d47feeb9da046e996450107b21223d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed204.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780dca4231f3437db1663cd0c22d422d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed205.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59494f3f2d8045edbf22cac20135e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed206.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2d3dcedd5c43569c7b52798743f65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed207.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb260ec3b3d4c2fadc13e0d6cf88afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed208.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c299161a121e49698ea0c992f14004a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed209.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a57926c2e425eb991caeaa24ab553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of adventure and outdoor recreation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of adventure and outdoor recreation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed210.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e66eaf33ca74bb0a60723dfbbfb3d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed211.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a0a20a5972444eb4b4505e7996e31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"curiosity, as the rocket picture on the bed sparks the viewer's imagination and curiosity about the world beyond our own planet.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"curiosity, as the rocket picture on the bed sparks the viewer's imagination and curiosity about the world beyond our own planet.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed212.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b5f4e5298645f187894701c74ec737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed213.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966751a1aa6040669158e2ddd0908cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed214.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ad2e4b584e42babafb84e497a6ab6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed215.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f67319630de47ccb6a5e0581966088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed216.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94efd7b6fd74c768684f1849f415b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed217.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c065d41ba1418ca35bb8474de21a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed218.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb5ebe68abb4b8d88c3b2388769f39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed in a comfortable setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed in a comfortable setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed219.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc19d5e5b52477da481f2f16f570cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed220.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec80d54b0f448cab713514c1fe13b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed221.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d170b0b4c6c40688c84565199d5059f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed222.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2923cdb6775f4690af1892a32ca84c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['being the main focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['being the main focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed223.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2a898b6e1e4715857a2b1dcd70acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed224.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04df5ef2ece74f1aa70925c27f3a2d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed225.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df1421b15b641aab68df82080111452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ke a sense of the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ke a sense of the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed226.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a743cd6011ef49c996b4fbd3b52f4ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['personality to the image, making it stand out from more conventional, monochrome beds.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['personality to the image, making it stand out from more conventional, monochrome beds.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed227.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77c575eed2549b385c1a6e5ffa4eff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed228.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe228a064d3548a6afb98ce9d7bbe084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed229.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec25b09be0949fa8f340e1b44287e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed230.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6076dc6502354f03abfdc6bd59170b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed231.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f84f3d81a048a6a579212edd570ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed232.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0825f8d67ca2453aab1d54603b6000c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed233.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558707fa94c248cc9e65ea65166b73d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed234.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8180f3d6fb2341cdb4c183c534001ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed235.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16385983a5143239ba8a194520d5a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed236.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecfb249bfe64dba8e60cfbbc6196fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed237.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db933ff023247359a55bcc61b651fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to - date information.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to - date information.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed238.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66202cc8ea88445bb2bd601c308d6ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed239.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e843a322644fcf8eec0e791cef0a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and relaxation, as the viewer is drawn to the serene landscape displayed on the bed screen.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and relaxation, as the viewer is drawn to the serene landscape displayed on the bed screen.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed240.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3812f6941a97456d8f4f94b719837868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the old - fashioned bed represents a bygone era of technology.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the old - fashioned bed represents a bygone era of technology.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed241.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6f9d7adc134a89833301c33b77bfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed242.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a858edd9eff44190bb888a196a8abb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed243.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f06e3d14b7443f8dd51e425d46a139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed244.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae9a6b39ccb4793929c3299741e09f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed245.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ce4bbf57854649a51f4f3f039568dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed246.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64760d6d1db8446181a0fcb5759d5ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed247.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b144ba3a239496589042184d892846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and its presence adds a touch of modernity and sophistication to the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and its presence adds a touch of modernity and sophistication to the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed248.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00885344e4454f97e9526a91f4bae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed249.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a86a3161254f68949d0ef6c37305ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of modernity and sophistication.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of modernity and sophistication.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed250.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc54c56e6f142f8be76fdc3dc662b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['engaged in a discussion or presentation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['engaged in a discussion or presentation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed251.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456de492b96640669e443fb17960e398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed252.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9582a5f357496580adaba67c2d4b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed253.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763b9d680b674005aa7aafe9a97208bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed254.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4b22a6f94a491bb8d1e1d46e433765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed255.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5997f5fdaeef4a28b5c83c31597eac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed256.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40a3b023c144eaf8ef38a8706d54bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming and inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming and inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed257.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44751d4f174342818b6e01ab0da11943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed258.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de03f0bee30a4580b610e54045c5327a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed259.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d762d982da1489c93f4631b94a50dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed260.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e50a69b20ea4c64b893e6fbfc0a75c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed261.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471cc1a052bb4214b5d57533028bbae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed262.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b4694126641e2ab7b372bd2b16675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed263.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cded0f57308645e69ca0a0b2688d2989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed264.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd04e1beb6604cfeb36dc276ee688d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['objects around it create a sense of modernity and technology, highlighting the role of the bed in contemporary society.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['objects around it create a sense of modernity and technology, highlighting the role of the bed in contemporary society.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed265.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445ef4ed74104fd091d1fbdf815bad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed266.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d712efe44d742f2b22d5f7a0209aea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['view while sitting comfortably in front of the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['view while sitting comfortably in front of the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed267.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5895976f2a04b69b69fc25c9af493c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed268.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b90beeb7524c149c2106fd10e6e135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['emotion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['emotion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed269.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9143807d4418492390c14a22d965c592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed270.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b3913263246bba383c9205b0e3d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed271.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404b691b68f94032ac5b451e7951ba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the person swimming in the pool seems to be enjoying their time in the water.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the person swimming in the pool seems to be enjoying their time in the water.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed272.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19e17b4f6da45c59dd7171bf165feff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed273.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d155a6bedfc418aa80c86a5aa42391c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed274.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea421579fe84ecba90f72c83bd3738a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main focus of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed275.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19d406d1d794c738284b0d4a4f9a677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed276.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10f7ad5564046919509d608dadc44f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shows or movies.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shows or movies.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed277.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fa5da3d9c24516850c75bba653749f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed278.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba2739aa78d41749a5d3a395c06470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the room appears to be a living room, with a comfortable and relaxed atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the room appears to be a living room, with a comfortable and relaxed atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed279.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3d517a966943d1a507a479ede0fd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed280.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e9eb541b704f20b23ada04f45e0ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the bed serves as a source of entertainment and relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the bed serves as a source of entertainment and relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed281.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85155afaf8e5474f9e311e9bb16b35eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that of leisure and entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that of leisure and entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed282.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d612cd0e1a465b95ceb97a8a3e6dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the bed seems to be a relic from a bygone era.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the bed seems to be a relic from a bygone era.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed283.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d115d1d0024cac8475bb802d7509bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its bright blue screen adds a touch of modernity to the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its bright blue screen adds a touch of modernity to the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed284.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3a332a79004697a26567582889ff00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed285.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2cae3351514a36995e59721a1ec416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed286.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761145d228004dea877e16c69cd68572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be entertaining and interactive.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be entertaining and interactive.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed287.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39793bc5f1e147f9b4f1556350a5944f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed creates a visually appealing and inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed creates a visually appealing and inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed288.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a21103c8a5941b9a071a7ab7541c66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed289.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7da3e3c8b946408db5b05002586814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall aesthetic of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall aesthetic of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed290.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f375dbffcd954a45ba3e09f573e2975f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed291.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9244aa9065b4df8a9541787ae264b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed292.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6178ad3f734548b439ef759cc93073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a long day.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a long day.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed293.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3c339ead28460dac9231537831c832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed294.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08537be73ff84ca88d3496160f8c0c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere, making the bed a central and welcoming element in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere, making the bed a central and welcoming element in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed295.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7665f024ba4ab6b42a52f503439eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed296.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460c721a238644cf8891d39d796cd6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed297.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890bc6b44810411ca4b8e657c6b72840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be a fan of the sport.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be a fan of the sport.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed298.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2242089a7421450da0a66a1386d3e8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a focal point for entertainment and relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a focal point for entertainment and relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed299.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9043820a564a558a5c2f4d280404d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed300.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0292d1dae7944458c6d0cc524fc9ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed301.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb4098210984d1f84f0799fc0cfdbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed302.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b29b83126bd43009084a04cabde70e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed303.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd7fc891ad64d99a1763a9ba0896142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed304.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef689bb71d494cb7a7a1b4df200f3fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed305.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29967ea52f4b47a187b4dddc1f212480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed306.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875e8585ce6442e19831b66e91630d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a focal point in the room, providing entertainment and information to those who gather around it.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a focal point in the room, providing entertainment and information to those who gather around it.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed307.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b6d455901543a3a155ff53dc5cafde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed308.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb33e94199c440593a54ee12b11445c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their own home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their own home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed309.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e83148415847dfaca7e72d79e358a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the person playing the game is likely having a great time.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the person playing the game is likely having a great time.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed310.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97f2e8aaeb548d1bdf0010ac8ad58dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed311.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17ab67dd8ba49058e7fd39d1cf2b46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed312.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b500198d2f4850892b4acf1c104d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed313.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea7211bce7d4aa6867074f4beb8a429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed314.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f51a67298c4b44a38a2f242e4883e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['users.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['users.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed315.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5440fb904369421799f37d157ea92e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed316.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eadaece41314265b4223a2bfbeb1a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed317.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f146bf9cb3de48ddb51b5b87a04c05db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed318.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bd5343aa754900bfde1d3d840dc5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed319.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc766370ab9941c2976934b02f9172d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed320.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94647377b9a24bc7ba64c4e01611701a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed321.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8494a9b74b4c0aab55b6b6c01222e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed322.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3061fccc0e4b7eab850c53f9333272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed323.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f72ac43fe2c4340af226c74510c39b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of connection and entertainment to the gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of connection and entertainment to the gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed324.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7297e9b9caa40a6aab9f4da6c994358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed being the main focus of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed being the main focus of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed325.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ddbc9e0d24441dae0787ffc3963fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed326.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aea0df5cf549e5af2a515b4ca12d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed327.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d64a5cb8f7840b2af9d357a6e653ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a central piece of entertainment or information.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a central piece of entertainment or information.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed328.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290744b03720496ebfa5204fc1471cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed329.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd06ab61b9a456991dc137099df4d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed330.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41686da02304dae961c3a3f8044e3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed is showcasing an exciting and dynamic visual experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed is showcasing an exciting and dynamic visual experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed331.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630534d2e5db42ab9ce36afd3fd40b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed332.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe07795113ef46ada8293a43e8a99fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed333.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2760309bad4f86a50dc2eeba16111b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed334.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c39b42e919473c8e4c2ae84dcd6546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed335.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59439ac9a8294b719251adb536da22d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed336.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a0a467a3ef4f97a84c52ad3139a032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a source of entertainment for those in the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a source of entertainment for those in the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed337.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b09b943a7349d8ba296ec24b9b5647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tered, with the bed being the main point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tered, with the bed being the main point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed338.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb87e99d0bb94e03b637f74fbc3ad9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed339.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2470a7abe1674136822337f1d3049ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed340.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0729a8db6a694410a7acd5ea781b0a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ory experience provided by the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ory experience provided by the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed341.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7eb238de844b47be1bf94a839e5d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed342.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382cea53781749d7be5bd48984e0b537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed343.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b757f5ff1964936a50472f90a6da47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it showcases a classic piece of technology that has been a staple in many homes for decades.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it showcases a classic piece of technology that has been a staple in many homes for decades.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed344.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f901b62b6341698d7abd64c0bbed08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting, possibly serving as a focal point or a source of entertainment for those in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting, possibly serving as a focal point or a source of entertainment for those in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed345.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d28672247854e1f9c51539232f022f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point of the room. the overall atmosphere of the image is one of comfort and relaxation, as the bed serves as a source of entertainment and leisure for the people in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point of the room. the overall atmosphere of the image is one of comfort and relaxation, as the bed serves as a source of entertainment and leisure for the people in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed346.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5300151ce4c3c8c3bbc1ea7bcd831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room, and its presence adds a modern and contemporary touch to the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room, and its presence adds a modern and contemporary touch to the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed347.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c00c689e94427fb61ab8768bff250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed348.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dfc8cd961c4cf39f30ece579f32be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed349.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82623c4abf114662867b2ff9e3899760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure time.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure time.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed350.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf824ec2995b405091a8fb9287b70502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed351.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a8e0ca65ec4057a84b11dd687ac789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed352.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9e5747e3804589b7f4a3b7a23d23ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of nostalgia and a connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of nostalgia and a connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed353.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16f1f6358d246a5924c35982e833e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed354.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22bdca4a0bb4c41b403abe3071bb4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed355.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924c3b0856be492c8a5bc73859dad15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed356.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d0b6a38fa34532a140b20698bae7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed357.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9778b577ab7542fa8a188aa6aae38148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed358.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72a996ff95344e9adb64319bc428b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed359.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9fad73646f4679a62a850620c4ef70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is calm and inviting, as the bed is the main subject and the building picture adds a sense of depth and interest to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is calm and inviting, as the bed is the main subject and the building picture adds a sense of depth and interest to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed360.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2df37baffc349e5b185c449a4ad7fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed361.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e419e1eb7f2340dabff108a976951e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed362.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3983a8d2754ea39049e0a594cade65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed363.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d142f4121ff48e7926349c9ddd23e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['displayed on the screen is the topic of their discussion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['displayed on the screen is the topic of their discussion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed364.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a73f09d994748a1ff7fb669d4d104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nostalgia and a connection to the past, as the bed is a remnant of a bygone era.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nostalgia and a connection to the past, as the bed is a remnant of a bygone era.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed365.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a100c79978b742be997206264370ebca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed366.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37e73b00dc64f06a1e569236da4cfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of modern technology and convenience, as the bed serves as a source of entertainment and information.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of modern technology and convenience, as the bed serves as a source of entertainment and information.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed367.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd4ff59c91490caa1dce577b55b9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed368.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65809d9a26ac42fe8841a2d269e1133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of nostalgia, as the black and white picture and the old - fashioned bed evoke a sense of the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of nostalgia, as the black and white picture and the old - fashioned bed evoke a sense of the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed369.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352962c94d184eed936310207d5b2e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['attentive, as the bed is the main source of entertainment or information for the people in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['attentive, as the bed is the main source of entertainment or information for the people in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed370.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c7ef9ce04348518eb113fc0c395677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed371.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637f12aa65574a279fa9e395a8bba6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed372.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e28fcf71f74904b5a977b3f0d00665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed373.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7365f2b82cc842c1892f0aa043b1ebfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed374.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdaac18671941619b2a2381d660d416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed375.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0068abecd32c46e9a19bfcdde6f599a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfortable living space, where one can relax and enjoy some leisure time.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfortable living space, where one can relax and enjoy some leisure time.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed376.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b79fd37743a4d51ad68d39b16d44ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['taking a break from their daily routines to unwind and appreciate the natural beauty of the beach.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['taking a break from their daily routines to unwind and appreciate the natural beauty of the beach.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed377.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508607f2e041462fae1922735fa92ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed serving as the central element in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed serving as the central element in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed378.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddc056b4f56440c882327346e21cc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed379.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42c27a38863447aa6e606099656062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and minimalism, with the bed being the main subject of the photo.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and minimalism, with the bed being the main subject of the photo.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed380.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17731588aa4649e590b0d05ab557d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed381.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ef654b6d214bd2ab307d91f7692a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed382.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101f58ae4734a8cb845d53f84bba64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of contemporary living and entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of contemporary living and entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed383.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b84d0e3113403a8ba919120b67134d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of interest in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of interest in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed384.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016ff01a31144c51ba57b7313d096db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed385.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb41d614960b48b58523ba48d19d969a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfort and relaxation to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfort and relaxation to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed386.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e640cc6cb24db7b88f0a4c032c37b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting atmosphere, with the wooden walls and ceiling adding a sense of coziness and comfort.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting atmosphere, with the wooden walls and ceiling adding a sense of coziness and comfort.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed387.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d6e140680a413190f79b45b84f80b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed388.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a249bf18abf64bb9bb971f2a707ccc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['visually appealing and inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['visually appealing and inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed389.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664dbaf6620a495592e729a6134c753a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed390.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ef4dcb5b7c4bdabbd28db4b4d931e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed serving as the focal point of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the bed serving as the focal point of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed391.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd79e954f5c42499f82bca46b8206d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed392.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0b23bbefd348be82e8c3eaada31e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed393.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e5677374204530a5f7f5a0768957b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed394.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f377334e308a4ddab5fc647119d03b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed395.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b8d021cfb64a70b769b4a979b61373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed396.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96c12fe51414760b274ec4ae5edef13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed397.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975cc5ff9cb94e259beab874347c7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed398.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e12abfb06664abebcc4731ec1ed8a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed399.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe65196ed5f47af87ce778dd6b69f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed400.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5744562792d3455c85aacb1c3de3df96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed401.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32db47cdac1b472c849b918769468fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed402.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627222e256e740fba276ed124fb177aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed403.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20efb191b616487697ea78e9e22fcc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed404.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c3eedd80cd48a3b3550a78d2efdd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed405.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5055ee881c2e4b7d908c62c5b614e56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed406.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d09e0c6ae547e4adb8a91ab9f19c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed407.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239898bc84d24f32bed3229c4afabae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['phone suggests that the person using the space might be engaged in work or leisure activities, while the cup could indicate that they might be enjoying a beverage during their time at the desk.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['phone suggests that the person using the space might be engaged in work or leisure activities, while the cup could indicate that they might be enjoying a beverage during their time at the desk.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed408.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e9ec5bbc5a4294b35fba3ef4afebbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['source of entertainment and enjoyment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['source of entertainment and enjoyment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed409.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8489a446d8df493a8588d54ebbdb9b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed410.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c4a991a5fd49419c668508408abfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed411.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1b3ca482c46dd930746d1466b57c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed412.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b2d7c7eb342edb7ed68e21d705ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed. the overall atmosphere of the image is one of modern technology and contemporary design.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed. the overall atmosphere of the image is one of modern technology and contemporary design.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed413.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a86ddddba7f42e78dbfb105b654650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed414.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c34075febcb4024b366f348df68db10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment and relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entertainment and relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed415.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695e4fe4a48e43709ce16ab8fa0ae28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed416.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9692afca9b274e0f9cdc28fcc4571b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed417.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8694f1c283894e16b2db5e0fa885582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed418.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fa9c9ab3ac4230822430a3a4b3c8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed419.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e0aee23d6c4108ab23f4624d5d1d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed420.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af5158ad9054f23818f11418b3762c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed421.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab12f6f8cffb4b708217497a6e85514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed422.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3466bbb92d4766bcd59b3d199ae215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are enjoying their time together in front of the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are enjoying their time together in front of the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed423.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad1a18aad21455f854378a50758bb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed424.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208324b136e549b88b30a6b991b3776b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed425.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344042bc993f419294e8db6943b18f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of technology from the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of technology from the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed426.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83da44992a594e4abe85214f47c7b69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed427.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c20f891c540ecb12f9f950a25d325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed428.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ace94f08dd43188db152ffd20fc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['green background and the captivating content displayed on the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['green background and the captivating content displayed on the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed429.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cd0e7f5d4f4ad3bd8cbe680ff040c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the road.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the road.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed430.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7606a6215734f2a8fdc908bad102215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beach scene on the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beach scene on the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed431.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b0b1b3e44544f48a6125f5ae495afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed432.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444fe24862d84b0aa6df5a54a015a2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed433.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec530e88bc347e2b9eeb39b420c12b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a dog resting on it, creating a sense of relaxation and contentment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a dog resting on it, creating a sense of relaxation and contentment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed434.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcc5c50a27642e9a07f35dff1a7bbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed435.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5895f3b5094175885e72487decad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed436.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21bf1a07a704f869f100832bdc709e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed437.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84af6add7a124b7fac5ad473e091348c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed438.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b5178a7a86431f87d874201c9f2382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be used. the overall atmosphere of the image is one of nostalgia and a glimpse into the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be used. the overall atmosphere of the image is one of nostalgia and a glimpse into the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed439.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd36647b2724d22bf769926ce3cf3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed440.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a1a3e9c03b494da20b657e262f2b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed441.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb04f25e32e404d805170d4d2515ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed442.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ea391462bc4c64b10338cf71fe2958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and uncluttered, with the bed being the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and uncluttered, with the bed being the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed443.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28747cc73a424d828e70ba03947167ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed444.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f9dbf25ed4283bd13da877956bee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment of the outdoors.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment of the outdoors.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed445.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2056223c1b4a473bbe93a41d59c6a21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed446.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236a0936def544978f174c140414b038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed447.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426f1e15bcc408cbfe4957a6f377631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed448.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea54a5d99e448c782a7524fdcc20653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ensuring that it remains secure and upright. the overall atmosphere of the image is one of comfort and relaxation, as the bed serves as a source of entertainment and leisure for those who spend time in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ensuring that it remains secure and upright. the overall atmosphere of the image is one of comfort and relaxation, as the bed serves as a source of entertainment and leisure for those who spend time in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed449.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c671d857bc704035a5f099ce689395f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['objects evoke a sense of the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['objects evoke a sense of the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed450.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355e11a5647343d3b7a8738f707d3d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image, creates a visually appealing and engaging scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image, creates a visually appealing and engaging scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed451.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57b76002d614dbb9483d0857806a9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed452.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec09f46d6ac64fbeb01fbfdf051d7aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed453.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3984aecf1d41208472c691f6c16794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['use for many years.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['use for many years.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed454.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba67888afdf4bde83e793b620c764be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed455.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4998d881eb745cc98c8bbf324f405cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['moment shared by the people in the picture.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['moment shared by the people in the picture.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed456.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04771796ae44b15992520c06c55fe01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed457.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42e271f93624b6d970495f36dd25a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed458.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab35012700f44d5a5cb02f0287d12b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['suggests that it is not currently in use, and the overall atmosphere of the image is one of simplicity and minimalism.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['suggests that it is not currently in use, and the overall atmosphere of the image is one of simplicity and minimalism.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed459.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83a692b03e441a882e759f141e13f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed460.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493e4a7441184aeb933bb4e13299ef09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of watching or viewing the photo.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of watching or viewing the photo.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed461.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e0b25217df45c098db687c2c52efcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed462.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601f6f8c71694cd9a7a119438e43a662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed463.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fafb1507f2446789a1289ce71e3ab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of comfort and relaxation in the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of comfort and relaxation in the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed464.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32c726033d74dc49b18061cdee1ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be the main focus of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be the main focus of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed465.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec1f3319bb1401786064ec631c212a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed466.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90aa49f16dd3439f9e2910601157b308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ocean from the comfort of their own home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ocean from the comfort of their own home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed467.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da12e94ed39a46d887b580f80d1985ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed468.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f1580884cd4898a9a6569ccf3ded89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed469.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941689af105a43f4aac613e8493d7fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed470.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c9a8219924068b70c6ab9aeb41502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['impression is one of relaxation and leisure, as the viewer can enjoy the beautiful sunset on the screen.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['impression is one of relaxation and leisure, as the viewer can enjoy the beautiful sunset on the screen.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed471.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edec633506348ec9ca807d79e7ecee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for entertainment purposes, possibly by someone who enjoys gaming.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for entertainment purposes, possibly by someone who enjoys gaming.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed472.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10b60bee52f4bce831c739223555de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed473.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66cacf0698747fa82e0c0fee2d23f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed474.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9727c74ed74074b0b57712e21dc0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed475.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070cc225981b476c9162d931ef6dfc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed serves as a central element in the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the bed serves as a central element in the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed476.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b9ba57cf80432ea26658ffa438c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed477.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf122733f664437aba7f5dfda2dbd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed478.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460875938a1249f5a708bf3a9541ed01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed479.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbd43a7f20f416d881396cd9163a0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aesthetic of the device.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aesthetic of the device.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed480.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59e668f554141e59e5c08981e4c778b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a comfortable and inviting living space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a comfortable and inviting living space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed481.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85afe0a485254bcab4d2644413a429db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed482.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbac2f178964ae3aff9a2efd79e6520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed483.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b3dd616a1e4d5fb3b18b867b502f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed484.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7b7b32521e491e9c8353c84a85470c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed485.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9588ff02d9294a89a40c26798430d5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed486.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fdee15ea994a01a5ed07a53319a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it is the main focus of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it is the main focus of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed487.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5da26d39874e20be98c98eb961ef5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and uncluttered, with the bed being the main focus of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and uncluttered, with the bed being the main focus of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed488.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73654c2c3e9c421686c401a03e4de7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of bed design and technology.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of bed design and technology.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed489.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa250113a824e19b0f631f675a9d445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed490.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f04e8c51a146a794ec8cc6354df5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed491.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b829a196b5094a52b28c066cd735d378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', perfect for relaxing and enjoying the bed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', perfect for relaxing and enjoying the bed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed492.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da163b3ef03c481487586b1d513dc469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus of the image, and it conveys a sense of modern technology and entertainment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus of the image, and it conveys a sense of modern technology and entertainment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed493.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f08dfe34b14589a277e592e379b89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed494.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06cf3490363406ba6b4605c4d661c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed495.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f302e73e4010496bbcce4a74675c7306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed496.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad197f3fd10c48d8a3ca66859ac1f200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['screen seem to be enjoying their time.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['screen seem to be enjoying their time.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed497.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6002e60da8944e9894c81d9956272b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sports event or a celebration.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sports event or a celebration.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed498.png for class bed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5bf4c9da6b496b81d45ca77d67b08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image bed499.png for class bed\n",
      "Generating images for class 20: chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06b781591544d2a93f9ea89d2ce1225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting people to gather and enjoy the pleasant atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting people to gather and enjoy the pleasant atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair0.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da0303e386b45fba6a3989e3a69bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair1.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f7072f75ca4805b6e356bd141fc3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair2.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605271e71a034bf9820f008bc733b755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair3.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb626e5648c84a63920337516997de2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or spending time with friends and family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or spending time with friends and family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair4.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8ab20b62cf4e53af568c277c206f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warm and inviting atmosphere, perfect for a cozy reading or studying session.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warm and inviting atmosphere, perfect for a cozy reading or studying session.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair5.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b70e399cf94c38bb9d18ababba3123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair6.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ad61027ac46298423df190e8b7880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the atmosphere of the image is one of warmth and hospitality, inviting people to come together and enjoy the delicious spread.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the atmosphere of the image is one of warmth and hospitality, inviting people to come together and enjoy the delicious spread.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair7.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e221765e8af042f08c9e577f2bee29fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair8.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fff173032574d02843a3011fa744054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair9.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3dbff53b6b4525ad0df194d7092971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair10.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1306ef55b3f0466fa331535a6a36e9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, as it is set up for a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, as it is set up for a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair11.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ca8512f7964831a398606acea6bd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and energy to the scene, making it an appealing and lively setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and energy to the scene, making it an appealing and lively setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair12.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e8417014b84a6caed17728d18c49ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair13.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839c8e5e5a3346d3aa043a2fd97b6316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair14.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8998e5cff84d9b976aef39e2240e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and appetizing dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and appetizing dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair15.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f690a24f4254033b4ee25bc9810962a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the carpeted floor creates a warm and inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the carpeted floor creates a warm and inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair16.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c546ee4de6443db3848fd7fa7f5d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair17.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8851442e5ede483fb0fd772ac7a445d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair18.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9759addb6fbb4951aa4f7c757d9fae71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair19.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f9fbdd41e540fe939c427bea3fb19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair20.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e15aa730ea44af87b8f1e7868dbafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be shared among friends or family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be shared among friends or family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair21.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ec8af7198849b7be0b62ef1565350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair22.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9b350aeef44a369bafc7a3e016417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair23.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c461c05148845d6bb523fdd74ac0f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a dining area or a living room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a dining area or a living room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair24.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08174174032743c787e0149bd64a2353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for a picnic or a casual gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for a picnic or a casual gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair25.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb62c59c9ba427f8bbe9401aab19179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals or conversations.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals or conversations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair26.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e16d5a981df473e823a2e7dccd45db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair27.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5af1dacd53a42e89d92e3c6aadb9854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair28.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef3ad136dcf44c18921ad3fed92cc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a special meal.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a special meal.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair29.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198fb9bc2ca342d383adc7bc39cc0ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a meal or gathering. the overall scene conveys a sense of comfort and togetherness, as people are likely to come together around this chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a meal or gathering. the overall scene conveys a sense of comfort and togetherness, as people are likely to come together around this chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair30.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23de45bdc78145409bf2b7796d249bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evening at home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evening at home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair31.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4739fde3cec4359bdc1198a94eee334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the wooden chair being the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the wooden chair being the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair32.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acbe08eb5a945aaad87af344c659280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair33.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01455f3abed4e45ae39ac149387da11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair34.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb8ef4f3674f6c81ee811f7a8d75e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['floor.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['floor.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair35.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c0307fc2d14159abf3fc2068a0ecf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the scene is relaxed and inviting, as people can enjoy their food in a casual and comforchair setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the scene is relaxed and inviting, as people can enjoy their food in a casual and comforchair setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair36.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7abe4e435d411589f165de684056ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\", possibly enjoying a meal or a conversation. the atmosphere of the image is warm and inviting, with the wooden chair and the man's presence creating a sense of comfort and relaxation.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\", possibly enjoying a meal or a conversation. the atmosphere of the image is warm and inviting, with the wooden chair and the man's presence creating a sense of comfort and relaxation.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair37.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da229a61d1e4cbdb7c2e253cd7fd5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is set up for a casual meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is set up for a casual meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair38.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9ab4ddf412432898d5e47ed6493818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair is adorned with a beautiful floral arrangement, adding a touch of elegance to the scene. the overall atmosphere of the image is warm and inviting, as it depicts a cozy dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair is adorned with a beautiful floral arrangement, adding a touch of elegance to the scene. the overall atmosphere of the image is warm and inviting, as it depicts a cozy dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair39.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2089ed288ec7480c8cd51f411466af92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair40.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751cd78522674809b4d0f5282abd6f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair41.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbc0d429ebd4cb89ba99205484a8bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and variety, with the chair serving as a central hub for these objects.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and variety, with the chair serving as a central hub for these objects.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair42.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2d0018d8b94b8e97f504838363209f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair43.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67aba3b71d744758e5ca9427734dbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair44.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed0830d21e643cab800cbe426eaa18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair45.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5908a6167e184ab38665cac8b10ab38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of unity and connection among the group.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of unity and connection among the group.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair46.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922b9f154f6c4e1d876387c88bdfe0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair47.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b348d929fd45ec9a5469b134687c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, and cup creates a cozy and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, and cup creates a cozy and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair48.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2915a7b82a70479e917c26f227af0fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is prepared to host a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is prepared to host a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair49.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9a1b0d19904c38a9b4a4d919835ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair50.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9940d8c43747a5bc1ea79f88dc66a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['white background highlights its shape and texture.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['white background highlights its shape and texture.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair51.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c774920fe94d72bca186ebd34cfffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a comforchair and intimate setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a comforchair and intimate setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair52.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221c08f4f4f7470285feccfa0f94e94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room appears to be warm and inviting, with the chair serving as the focal point of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['room appears to be warm and inviting, with the chair serving as the focal point of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair53.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27edf0e29501483f990e4e07c6f659b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair54.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2535650d53d1467aac80f3b952186cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tered.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tered.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair55.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99197acadcf4097b06bf28efc572939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfort to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfort to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair56.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fa7d23e9244a7d81869e203f2329d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a way that encourages people to gather around the chair, creating a sense of community and togetherness.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a way that encourages people to gather around the chair, creating a sense of community and togetherness.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair57.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8227fbc0504bbfa37cd90d0f4aeb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['events.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['events.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair58.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb2569580364be18fda49ae619b2b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair59.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8600ac0aa34dbb908362632b7a3eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair60.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623b5326c5f049d28db59ddf3c394ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['minimalistic, with the focus on the chair and its supporting legs.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['minimalistic, with the focus on the chair and its supporting legs.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair61.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0705af0abcc847939525a814364ce42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair62.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c611df71ce41fe87047d8492b48f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['prominent object in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['prominent object in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair63.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1ab6d52a4d4e7c836ea0535d93674d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair64.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94887a2cf194611ac6783378841c7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a couch and a chair, creating a cozy and inviting environment for a meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a couch and a chair, creating a cozy and inviting environment for a meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair65.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1718cf18b734dcfa0dd47e6c2738561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair66.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3ad74796f945dd8cce5c66c1007157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair67.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427b6d12d47465495d6fa95216a3123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"overall design. the room's atmosphere seems to be inviting and comforchair, with the chair serving as a focal point in the space.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"overall design. the room's atmosphere seems to be inviting and comforchair, with the chair serving as a focal point in the space.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair68.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1502d119410b4564961fa789ecdaa6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair69.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9be7734db75417cab2a6d3d2f725a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair70.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b545967a99ee4a5f93a18b8c17f373bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the composition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the composition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair71.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b575eeec57148b8a401dba3c0e9ae25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair72.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25dfbd1df3c4934bee14357cbef6447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair73.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443765ef2a8a466489cddafe03cfb0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming and comforchair atmosphere for gatherings and meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming and comforchair atmosphere for gatherings and meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair74.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b985e114ab4f90bc785e02e89828e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere for those who will be dining at the chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere for those who will be dining at the chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair75.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b3c21f6a3e464581715708006c004a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the wooden elements creating a cozy and comforchair environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the wooden elements creating a cozy and comforchair environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair76.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1674c307d3d94e62a1ef651dedb01c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair77.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fade769081c481fa0d9476ede88c99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair78.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59b1c33e3af4783b448066c9b5e5571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair79.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544fc1f8599e4aaaaa24ec0db16f8f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden chair and chairs creating a cozy and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden chair and chairs creating a cozy and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair80.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357175a7545946eebca023433f05362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair81.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417c94360e61435491ace9cfd95b2103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair82.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4379bd6851a54440ae6e20e5eaa1bcf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance and sophistication to the setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance and sophistication to the setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair83.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663c99abbc824851a6d80b4d3c562333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair84.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c87ba0afd345e8922b5907299fdd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the wooden chair serving as the main focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the wooden chair serving as the main focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair85.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc60de3898c64cd0aab69ac587ecf1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair86.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d0ac971d3a45b98a397e7d3d4288e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bright and inviting space. the atmosphere is one of warmth and comfort, as people gather around the chair to enjoy their meal together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bright and inviting space. the atmosphere is one of warmth and comfort, as people gather around the chair to enjoy their meal together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair87.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316ead991f5241f1b6e33df6c0a38187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['few other objects, including a bottle and a cup, which are placed on the chair. the overall scene gives off a sense of simplicity and functionality, with the chair serving as the central point of the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['few other objects, including a bottle and a cup, which are placed on the chair. the overall scene gives off a sense of simplicity and functionality, with the chair serving as the central point of the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair88.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb23722c44384bfc99c3740b3f7b5442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of warmth and comfort, as the wooden chair and chair evoke a sense of coziness and familiarity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of warmth and comfort, as the wooden chair and chair evoke a sense of coziness and familiarity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair89.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d804af47b8d4a4285604d79e48cf995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is inviting and cozy, perfect for a relaxing meal or gathering with friends and family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is inviting and cozy, perfect for a relaxing meal or gathering with friends and family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair90.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1642c98888c4f2da74ead40351d8a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['people gather around the chair to share a meal and engage in conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['people gather around the chair to share a meal and engage in conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair91.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8fcdccb52f47dbae7bf601410b409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the white background creates a visually appealing contrast, highlighting the chair as the focal point of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the white background creates a visually appealing contrast, highlighting the chair as the focal point of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair92.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c334adf213aa451fae060e721a8adbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair93.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee3e036614e469e85b2edd6629a0b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair94.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6888e7bdcf48f19a6c6a8f6c4cecd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair95.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c47aa6620742708e5b380f0e3df97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the room is cozy and welcoming, perfect for a romantic dinner or a gathering with friends and family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the room is cozy and welcoming, perfect for a romantic dinner or a gathering with friends and family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair96.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a23c1bb91d5410fb9334df227fd7764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair97.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55012432ed75420397240dca7063b7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair98.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508babe10cc4443daff950b27618e555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair99.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6650783bd7884ac1a990c957e45738e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair100.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a7291ba74c4ac6bcf9760ef3f10e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"overall atmosphere is warm and inviting, encouraging people to come together and enjoy each other's company.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"overall atmosphere is warm and inviting, encouraging people to come together and enjoy each other's company.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair101.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107d8b5810424a99a6de4fef081c5af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for people to gather and engage in conversation or other activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for people to gather and engage in conversation or other activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair102.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4bac00a4904212884bab4c190be944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of warmth and energy in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of warmth and energy in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair103.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f0ba4545464ad6ae4d8c18558bd8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair setting, inviting someone to sit and enjoy the drink.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair setting, inviting someone to sit and enjoy the drink.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair104.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc6b396ee54479faadf6aff5690e29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of relaxation and enjoyment, as people gather around the chair to engage in friendly competition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of relaxation and enjoyment, as people gather around the chair to engage in friendly competition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair105.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec06340ca03744a6ba12d6455a06569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and leisurely activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and leisurely activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair106.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87e273f8d324875bd1de992173416d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair107.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc0e0966adf46058d333ef0a1f4688c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['white background creates a visually appealing and harmonious scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['white background creates a visually appealing and harmonious scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair108.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93540a2226cf476985aa706423d9ee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair109.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08effdab22f54d71afc70c17c3038138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair110.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffb7fbae60a4933a9c932325750649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair111.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a669064687482f8e43e1e51bf50d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are arranged in a way that encourages people to gather and spend time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are arranged in a way that encourages people to gather and spend time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair112.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0333c41b3087425b8e8c285e65e9aa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a dining area, with the chair serving as the focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a dining area, with the chair serving as the focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair113.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf29a1d5d18e4ed2a2d6c60160348dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs around the chair further emphasizes the communal aspect of the gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs around the chair further emphasizes the communal aspect of the gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair114.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cf6b54599e4bfabb63f6ff7cb3fa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere, making it an ideal place for gatherings and social events.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere, making it an ideal place for gatherings and social events.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair115.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35516d142e64b0595bcc1db8e82f607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair116.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7a99b888594042b0ce5e64be4728c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair117.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee0315f2b364df9b3483a758ee45c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair118.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a78542fa014e718ded18b07ee7190d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meal.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meal.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair119.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f402029cfb54254be2079843c3cfaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair120.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b3205e6dc344ee9c273d022f87db42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair121.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e21f27bd24407b66727984397a089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair122.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338a86be8c1040fbaa69d3ad198c2cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a welcoming and functional space for gatherings or meals. the combination of the wooden chair, carpeted floor, and chairs creates a pleasant and intimate environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a welcoming and functional space for gatherings or meals. the combination of the wooden chair, carpeted floor, and chairs creates a pleasant and intimate environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair123.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0953757719405b9300836898a2d88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['served. the overall atmosphere of the scene is one of preparation and anticipation for an upcoming social occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['served. the overall atmosphere of the scene is one of preparation and anticipation for an upcoming social occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair124.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c92684ec2c4df8ab2a0ddcac32bbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair125.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3328e642cb8a492cb92a23f1bd82af8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair126.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1ff11fbacb478ebe917e6b3e80ee7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene appears to be a social gathering or a casual meeting, with people sitting around the chair, engaging in conversation or enjoying their drinks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene appears to be a social gathering or a casual meeting, with people sitting around the chair, engaging in conversation or enjoying their drinks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair127.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123dd1010de046aba9e7df80bd73a1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ready for a meal or gathering. the overall atmosphere of the room is inviting and comforchair, with the chair serving as the main point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ready for a meal or gathering. the overall atmosphere of the room is inviting and comforchair, with the chair serving as the main point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair128.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6d069866344bc95fee80e214d7577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is warm and inviting, with the red chair and chair creating a cozy and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is warm and inviting, with the red chair and chair creating a cozy and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair129.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c1b02f39214334855d7849c2ccfbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the space. the combination of the green chair, wooden top, and the carpeted floor creates a warm and inviting environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the space. the combination of the green chair, wooden top, and the carpeted floor creates a warm and inviting environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair130.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d62dddfbe544e2893c326571b2fa3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair131.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e2cb6729824186bc3d94b6d29be29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair132.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb6eaa14b2741b5aa664aca2387e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs create a warm and comforchair atmosphere, inviting people to gather and enjoy a meal together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs create a warm and comforchair atmosphere, inviting people to gather and enjoy a meal together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair133.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abae53b8ea3f44c49573f51adf510fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair134.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2664c51f09ff490a8845d5e0ae5cd30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ready for people to gather and enjoy a meal together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ready for people to gather and enjoy a meal together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair135.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2824a196eb1e4b0991f0cfe9f60de425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair136.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f8d20c8d204bf6aee4e35a9d81f438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair137.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf55c13449644a590fb675019e419b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['flowers creates a warm and inviting ambiance.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['flowers creates a warm and inviting ambiance.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair138.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdc1b1c86a74f7f80ba69e3d504bcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance of the chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance of the chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair139.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e8326975874373a7a6dbd1974ac6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair140.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51dcc023e4d436ca507000c12972033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair141.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf3e68cbc4744bbb4a60ece23969b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair142.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecf617142924d078443d30a7ed4cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair143.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00478d5b2cf84034881609eccc7b74c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair144.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bfa43350244e69a3f79d5c47bbce62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair145.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61a4aeb13354d9ab742dfbc788373db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair146.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ab931b8970408d9a59ec9228eacdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair147.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08459f241e074fe98724c73c2b901d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair148.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9275d8a577a464fa69f1a8d2d5c86e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair149.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9659d0c395db4139bbc1d6f618b40efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair150.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3422f84d33f643eb847231252da72106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair151.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0bad067d124d1d942747c35f512157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair152.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259e7ef4302341029e94d2d907a081c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair153.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9884f731f3406080bd8c07f97c550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair154.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab7d04bef104607920fd05cd0bc8927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair155.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b453565e358c42ff9220b1b5b0a3c932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair156.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9e8b5fd7464b6295577ed42fd8c8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair157.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2503218cd1984367bde6a1466539ca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['together and enjoy a meal or conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['together and enjoy a meal or conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair158.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9028edc8cf2a45639ad4d588b24fffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['simple and elegant presentation of the round chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['simple and elegant presentation of the round chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair159.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641b825403f545c0b216b7ea73c03568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the focal point for gatherings and conversations.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the focal point for gatherings and conversations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair160.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c4a1e5bc09440695a8e33bbeddffe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair161.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eb27421af54a39af1e8596149ba42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall ambiance of the scene is warm and welcoming, with the chair as the focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall ambiance of the scene is warm and welcoming, with the chair as the focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair162.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c40fd006874dceb52145b6cb5a8a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair163.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2f2f7dda92478280584cda05230af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comforchair and inviting atmosphere for a meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comforchair and inviting atmosphere for a meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair164.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08324fcdbe643158a2af66c1bb716d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair165.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5798c7ae3555462d8770cad262a5096b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair166.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681d86483e3e43c0add756015ad65948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall impression is one of warmth and functionality, with the chair serving as a central element in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall impression is one of warmth and functionality, with the chair serving as a central element in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair167.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a2387b4a884ee7a1e05e647a6feeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of abundance and enjoyment, as the chair is filled with delicious treats.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of abundance and enjoyment, as the chair is filled with delicious treats.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair168.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a53b846d24256994cf9105ec8e01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the scene is warm and welcoming, as the chair is ready to host a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the scene is warm and welcoming, as the chair is ready to host a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair169.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dbda2421e0481499b5dfa8112b53e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer to imagine enjoying a meal or a conversation in a cozy and welcoming atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer to imagine enjoying a meal or a conversation in a cozy and welcoming atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair170.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d1278c6eb4c2aaeff069e004bc49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"enjoy each other's company. the combination of the red top, wooden construction, and the comforchair seating arrangement make this chair a focal point in the room.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"enjoy each other's company. the combination of the red top, wooden construction, and the comforchair seating arrangement make this chair a focal point in the room.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair171.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4b04307318422596c4e979a241c496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene. the overall atmosphere of the image is one of abundance and richness, with the chair serving as a focal point for the viewer.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene. the overall atmosphere of the image is one of abundance and richness, with the chair serving as a focal point for the viewer.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair172.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2c8c936f214657ad35b4ed2ea01a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of celebration and joy, as the cake is likely to be part of a special occasion or event.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of celebration and joy, as the cake is likely to be part of a special occasion or event.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair173.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fba0497a3304baa9e20d021691cf96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scheme and the black background creating a visually appealing and stylish setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scheme and the black background creating a visually appealing and stylish setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair174.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7343ffc7cac847a0aa5c90d5cbdde94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leisure.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair175.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cb45175c894575862a4e5fbb0f725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['top, wooden construction, and the assortment of cups and bowls gives the chair a cozy and welcoming feel.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['top, wooden construction, and the assortment of cups and bowls gives the chair a cozy and welcoming feel.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair176.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6396d4a5613148b0a02b4c3ab8b5a010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair177.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4b16c9ecbf4d5ca752dbf728a1cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair178.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8faa4d9bf7cd4fc8b563fda2400c181a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair179.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d913f332459f4ef887d67fed24bb0d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair180.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0b426e22aa442fbbd7d08cece40a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a point of interest in the otherwise calm and natural landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a point of interest in the otherwise calm and natural landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair181.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038898fe074648fa95c56a47e3aaa59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image suggests a busy and disorganized environment, with the chair serving as a central hub for various tasks and items.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image suggests a busy and disorganized environment, with the chair serving as a central hub for various tasks and items.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair182.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f68ef6020e48eb966001077096c334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair183.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2412f4ee6f214b5db7393a54f1d3d6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['studying, while the cup and bottle indicate that it could also be used for enjoying a beverage or a meal. overall, the chair exudes a warm and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['studying, while the cup and bottle indicate that it could also be used for enjoying a beverage or a meal. overall, the chair exudes a warm and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair184.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690f6ce7ad6741e49a1039a6dc1707d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair185.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290213250e4c48c59330b72bd444e13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair186.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b09b57fbb494bc4a40a42dca1e586f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting, with the chair serving as the focal point of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting, with the chair serving as the focal point of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair187.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32105c888f14eb6a1ac069758a96d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair188.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5c1da2a2b045eca574b6dae8046f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['attendees.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['attendees.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair189.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5f5ae93157448f91f5ce76b9be33d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the focal point of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the focal point of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair190.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a7d72b38394876a508597842b596a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair191.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349789c6ff7e44e39aa39210fa2faa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair192.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d96101de20c4c41bcd9fd7728e0338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the chair serving as the central point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the chair serving as the central point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair193.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af489f5a49714bccbf558ea76e5f2270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['come together and enjoy a meal or conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['come together and enjoy a meal or conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair194.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13f4eb706ed489b8f67126046f04d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment, as the chair is filled with an assortment of delicious foods.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment, as the chair is filled with an assortment of delicious foods.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair195.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b834f1efaaa48b79183cdae505550a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining setup.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining setup.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair196.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ccac19135649748249977b09768c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ideal space for gatherings or relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ideal space for gatherings or relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair197.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20132ce8c4cb4d5799282d0265a3b3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair198.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a00871aee14d068e4d8bf842271f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair199.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5115e784878401ebb1695face9b13f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['orange color, and the white surface creates a cozy and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['orange color, and the white surface creates a cozy and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair200.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b81f47eca7d4611a6a32fc1363d2b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['depth and dimension in the scene. the overall atmosphere of the image is one of simplicity and elegance, with the wooden chair and chairs serving as the main elements of the composition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['depth and dimension in the scene. the overall atmosphere of the image is one of simplicity and elegance, with the wooden chair and chairs serving as the main elements of the composition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair201.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef20cde605084f74a1456f061c082844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair202.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c938d62772874596b1643a783ad5f9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair203.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059018d5072b4cb893d8250eb074ed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair204.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a70386704845f588f2cb4a71a3af88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair205.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe41384737f4860847035cdeb4c5366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair206.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca278a4aaf5464390e624b409a6ec17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair207.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebf24f4f6b4488b77b8663d22b6f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair208.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ede0629af34926af2f58e1936f18a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of warmth and togetherness, as the chair and chairs are set up for a shared dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of warmth and togetherness, as the chair and chairs are set up for a shared dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair209.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9b8f6994d34740bfe25247a7cde2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gathering area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gathering area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair210.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b182658617148778b7ba7f61289640a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair211.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0f3a92c993422cb34c49a265cb7fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair212.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3594832a3bf044a49de8e36717c77f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the room is simple and functional, with a focus on the practical use of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the room is simple and functional, with a focus on the practical use of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair213.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecf082b871142eeac0debbfa530acea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['guests to enjoy their time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['guests to enjoy their time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair214.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c47910f678347778c1f2aeab9aaf5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair215.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5acbfeac6d24e1e8615fec83217db2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair216.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49e93d87a95470a87647ea720847c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining area where people can gather and enjoy meals together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining area where people can gather and enjoy meals together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair217.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53496e41a5e24860aa97576c8a8508f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair218.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435b48171c3e44eab1a20cbc85ae7bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair219.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2fe6ae1eda489eaad8ea9e2ca09482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair220.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8488d71bd94d94847f2a21dd39cfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['experience more enjoyable.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['experience more enjoyable.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair221.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0449207657ca4eb5a71a27db7e610ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair222.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f83dd0ca82f4c789a156f3d8f5413af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair223.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660f6f733e154acea2c1e0442c47b06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair224.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5f754ebdc642f28a03a47d3721b68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cabin or a rustic setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cabin or a rustic setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair225.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ef8023b84a4c70b29041d4fed02d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair226.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1319958e70f94e4e9a37977635ae0e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair227.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17e68bc5fa345dc90d45d3e23098f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a gathering spot for people to enjoy meals or engage in conversations.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a gathering spot for people to enjoy meals or engage in conversations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair228.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1959366127349f3bb3f58146a9f14e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is warm and inviting, with the red chair serving as the centerpiece of the setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is warm and inviting, with the red chair serving as the centerpiece of the setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair229.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf637ebd56d41e3842579c66b3cdc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is filled with an abundance of food and drinks, suggesting a festive and joyful event.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is filled with an abundance of food and drinks, suggesting a festive and joyful event.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair230.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4dab416a1140d793280cdab3a54ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair231.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3682baf0cb42068b8258efeed2903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting feel welcoming and intimate.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting feel welcoming and intimate.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair232.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf4d04eaaa24895929bbd0c9d4a5d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair233.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587aeaabcbb4461dbbf9463370782b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"and enjoy each other's company. the combination of the round chair, green centerpiece, and the assortment of items on the chair creates a warm and welcoming environment.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"and enjoy each other's company. the combination of the round chair, green centerpiece, and the assortment of items on the chair creates a warm and welcoming environment.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair234.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7962e7b73d8043219cb9aa34605c3b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair235.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc7a8d749f94a9396fac1c833e2fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair236.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0d016d4a364a939441007dbeb1df37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, adding a touch of color and freshness to the setting. the overall atmosphere of the image is warm, inviting, and comforchair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, adding a touch of color and freshness to the setting. the overall atmosphere of the image is warm, inviting, and comforchair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair237.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a08a13e69349539de01bfdfc44b06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a meal. the overall atmosphere is one of comfort and relaxation, as people gather around the chair to enjoy a meal together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a meal. the overall atmosphere is one of comfort and relaxation, as people gather around the chair to enjoy a meal together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair238.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca6c92fe50f4579823534eb3eb37c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the vase is situated in the middle of the chair, adding a decorative touch to the setting. the overall atmosphere of the image is one of a cozy and inviting dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the vase is situated in the middle of the chair, adding a decorative touch to the setting. the overall atmosphere of the image is one of a cozy and inviting dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair239.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58865d17fd4f60b6536fe956bfb872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of warmth and togetherness, as people come together to share a meal or engage in conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of warmth and togetherness, as people come together to share a meal or engage in conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair240.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937107caf6cd4e44acee6f8ef546fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair241.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f024f6c6ae74cdfa813307daa14357c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"to the room's eclectic atmosphere.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"to the room's eclectic atmosphere.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair242.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe791d972074ed6b9d3409d5635f153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair243.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19dc3c43f0f437ea9ee9fb51835cb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair244.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5e44f1d51b4f08b0e079c5730a730d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['red top, and the presence of the bottle and wine glass creates a warm and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['red top, and the presence of the bottle and wine glass creates a warm and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair245.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad5a54bac4a453b9c4e638910d4417f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair246.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4dc957267c4bb69ec886ff80689ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus on the wooden chair and its placement on the white surface.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focus on the wooden chair and its placement on the white surface.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair247.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33375b43aa464bdc96b64343e4a2d13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair248.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8f8202422545bfabfc342c22fa13ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair249.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f7f22114c74c869d1e563c674fa9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is warm and inviting, with the wooden elements and the red top of the chair adding a touch of color and vibrancy to the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is warm and inviting, with the wooden elements and the red top of the chair adding a touch of color and vibrancy to the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair250.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144b9bb1699543df8bb071aaab15dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair251.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe355515520f42c78fb919c01476dfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair252.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7aac41e6c542f0b610c49368456461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the atmosphere of the image is one of a cozy and inviting dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the atmosphere of the image is one of a cozy and inviting dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair253.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a3d1fd87ca4f7488afbbe448b9d945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair254.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e57065fffc44761b60a1ca5bc3d7878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair255.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2222a2ee2ab04d3c82425e42c288d601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair256.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5ae454d03e48cdb68fd1ae09d62c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['share in the experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['share in the experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair257.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb8f6ad0c0445de87a198d20b6e0898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair258.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9505071aaec463cb3565c2011ec729a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair259.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba23a9a90f0041e9a0edacb1e1a33088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth and companionship.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth and companionship.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair260.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f6cd41af9640849c13b769a91647ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair261.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d6a37f4f04c07a38c62bfcdd6210f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth or a connection to nature, given the natural material of the chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth or a connection to nature, given the natural material of the chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair262.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f391b8e07880445ca84f2e6675617947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair263.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b4d70dd43c483095cadb0932233da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting. the room appears to be a dining area, with the chair being the focal point of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting. the room appears to be a dining area, with the chair being the focal point of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair264.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b53043615641d099b7ae526b99e3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting creates a sense of tranquility and leisure.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['setting creates a sense of tranquility and leisure.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair265.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654e61f99fa44147b6acbc5d6d73822f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['emphasizing its presence in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['emphasizing its presence in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair266.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285f3bde402d48f9b38146bcb19ebfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair267.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f3c0939c5d4639933fa21543024743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair268.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd78a60c0854b9bbab79dc198379c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair269.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b153a2fba80f4f309ffc878b651885cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair270.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2658602f4b5b49ff8e1a1bddbfb011c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair271.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127efeaaf3e24a37a13feab9e999df9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['industrial environment of the concrete floor.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['industrial environment of the concrete floor.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair272.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b662021a288942f2b7cfcdb7d07d5cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair273.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4a37c956e946a393d83b0e10fe1ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair274.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa6358212d04b54bb703c1900d3d80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair275.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12195a075ae4d32a1aed8c0c22b1152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stability.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stability.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair276.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4dc92659ac44538437b6fa872b647a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair277.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f472f909bb415f94183bb3b0239246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair278.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f2c6aff5d04998941c6c1ea91d5230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ation and elegance, making the space an ideal setting for various activities, such as dining, working, or simply relaxing.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ation and elegance, making the space an ideal setting for various activities, such as dining, working, or simply relaxing.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair279.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec92945e5be244858607258f023eeacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['making it an ideal place for people to gather and enjoy meals together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['making it an ideal place for people to gather and enjoy meals together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair280.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63755475216541b79e578d0a3e8ef7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair281.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a2c5788b314bfd800a093de2e17aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elements, creates a visually appealing and appetizing setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elements, creates a visually appealing and appetizing setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair282.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4dc6bb865a4c38a4e9b58995cd94c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair283.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84994c9dbf54c0187ffe6067b5f39dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair284.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d490d5a29a642ec9820116dac891aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair285.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e508dca1a73d43cba6152a8cb8116894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comforchair and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comforchair and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair286.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21614e6c08742599c77942e70bece34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a centerpiece for the display of the vase and flowers.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a centerpiece for the display of the vase and flowers.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair287.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5625721c7a2045ea9571be8b20b524ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as dining, working, or simply as decorative elements in the landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as dining, working, or simply as decorative elements in the landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair288.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bb00676ddb42b38644ccf707ce0319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and striking color.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and striking color.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair289.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92544929f574175b83f34ac1eeb2e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair290.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0845a1fa85ed4e59be72807034033e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair291.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a58c30aac74d8da6a4d0b3a488680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a way that suggests it is ready for a meal or gathering. the overall atmosphere of the image is warm and inviting, with the natural light from the window illuminating the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a way that suggests it is ready for a meal or gathering. the overall atmosphere of the image is warm and inviting, with the natural light from the window illuminating the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair292.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39060dc0800a45bc87879e432de6928a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is one of simplicity and elegance, with the white chaircloth and the red apples serving as the main focal points.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is one of simplicity and elegance, with the white chaircloth and the red apples serving as the main focal points.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair293.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348ad8cc982a463b823819751b468a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cozy and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cozy and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair294.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef9ab7058ee433e972c33b2cca11abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"ity to the overall design. the chair's presence in the corner of the room suggests that it may be used for dining or as a gathering spot for conversation and relaxation.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"ity to the overall design. the chair's presence in the corner of the room suggests that it may be used for dining or as a gathering spot for conversation and relaxation.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair295.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224551b09e294411b55b8541c4ab8fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair296.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6b6c7a2499468e8ec1c545c6639e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair297.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8740c8132b14ac0899368c3c9c6d5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair298.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155ff03622cb42aa94aef48862cc08ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and rug creates a comforchair and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and rug creates a comforchair and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair299.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d7f50957bf45048ef06aac57abef73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair300.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fabf533b1b46f990bd9992664bdc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair to enjoy meals and conversations.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair to enjoy meals and conversations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair301.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040a14a21c5348f08826c8b37343d8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a warm and welcoming atmosphere, perfect for a meal with friends or family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a warm and welcoming atmosphere, perfect for a meal with friends or family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair302.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16220350e5e4759a3503bdb9eb9a8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and a spoon, which contribute to the overall aesthetic and atmosphere of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and a spoon, which contribute to the overall aesthetic and atmosphere of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair303.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04db4d12bf84c0da0047142f9402f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural ambiance to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural ambiance to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair304.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396ed2e601be4edd8c6d4b2f4aec7f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is set up for outdoor dining or gatherings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the chair is set up for outdoor dining or gatherings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair305.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1146506498c4e9a8ba801d2d16ddc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair306.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b4b31a06c049c59e03e8bbf4ce3eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair307.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dc8cb3e4054f09896b313c879a7bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair308.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a12b36f7154c8d982b74962252e149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['flowers serving as the main elements that convey this feeling.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['flowers serving as the main elements that convey this feeling.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair309.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a172bafdf9014180880a48a08fc72570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair310.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be40b32a38b74f31a4f8789d85f9e2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"background highlights the chair's presence and adds depth to the image.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"background highlights the chair's presence and adds depth to the image.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair311.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69970cfed1174529ab5e39e7b84b78e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair312.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89562935fa64d10a9f2745dec1b11d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the chair exudes warmth and comfort, making it an ideal spot for gatherings and meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the chair exudes warmth and comfort, making it an ideal spot for gatherings and meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair313.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899239421b2e4467a9de9b4efa7f5b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair314.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ee00e5a2f743afab39cea4dae575ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['allowing natural light to illuminate the space. the combination of the wooden chair, red finish, and the surrounding chairs creates a cozy and welcoming environment for dining or socializing.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['allowing natural light to illuminate the space. the combination of the wooden chair, red finish, and the surrounding chairs creates a cozy and welcoming environment for dining or socializing.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair315.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fba570b3e834095a97c22c83448feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair316.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a160829962b54d7fb1e96d5ec579c142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair317.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8f2583548c4ec496543c157b5a4c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair318.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a38d7ae7a754fbfb56c5e20f60b8c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair319.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243259897414475ea72287fe9be11466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creating a welcoming and comforchair atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creating a welcoming and comforchair atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair320.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4730c5857b48cc82df025e71db41ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and welcoming space, perfect for gatherings and meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and welcoming space, perfect for gatherings and meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair321.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b040cd2e2b2444485f92f7862ffdefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the focal point of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the focal point of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair322.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f32a261032b4df2968788d5249acfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair323.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce964fb9a70f4b80b5fa3192aac5d103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair324.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff6e218ff642bf96aef773e1832072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is warm and inviting, as it showcases a cozy dining area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is warm and inviting, as it showcases a cozy dining area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair325.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06acb292957249288bfc727f48eb94dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair326.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4017ba9c689a434d9bbcd620aff25182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. overall, the image conveys a sense of comfort and relaxation, with the chair and chairs inviting people to sit and enjoy their time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. overall, the image conveys a sense of comfort and relaxation, with the chair and chairs inviting people to sit and enjoy their time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair327.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d38dc0431734a0d90fcf2a61e7158a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair328.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796ee22ebc9f4a64aac69451243c2a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair329.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abfaa21f6b1471b978af9ea45628f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is inviting and appetizing.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is inviting and appetizing.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair330.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5d35404404492fbdedb9afc1d21fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair331.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e92d27a08d74da293ef49b84e4f49ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the black and white chair and the black chair creating a sense of depth and intrigue.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the black and white chair and the black chair creating a sense of depth and intrigue.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair332.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caaa614ad2b42f4aeaab799e6f2d13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['person seemingly enjoying their time at the chair.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['person seemingly enjoying their time at the chair.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair333.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db41232b705b4d11a94e0aeb02c56301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair334.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd420b0808104195acaf1957b112d270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair335.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec89484656c4f599159921546ec0a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair336.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5db9f0f8d604676b7c13fef7c59a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair337.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2fb38bd6464dc58e37cc8681217f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs create a warm and inviting ambiance, perfect for a cozy meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chairs create a warm and inviting ambiance, perfect for a cozy meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair338.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a837db53e93543269c2ef24ddc233f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair339.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a87dd7b65847a2adf0b6a7bf81ca7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair340.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bca1ffe871847b29567da15d2095068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair341.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cd00d1140d430586102bf0730f8a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair342.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118c215329124bc69d7a7b5b133be3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is cozy and inviting, with the wooden chair and chairs creating a warm and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is cozy and inviting, with the wooden chair and chairs creating a warm and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair343.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d517267cd9394853af058490113ed699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creating a sense of warmth and hospitality.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creating a sense of warmth and hospitality.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair344.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5169020b6b57470eab43b411458a0658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['perfect for enjoying a meal or engaging in conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['perfect for enjoying a meal or engaging in conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair345.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01216bd3148484f98508ab2b2feace0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or even as a display.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or even as a display.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair346.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a10169439149208fcd0b80943f0116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair347.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb313d45e0214dcb8c027dd8c4c59b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair348.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cade2c0964f54a55af793cc3a95eebea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair349.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0856d10074d44955ae6f29c8116af6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair350.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fc9619d32e4985b735a647094c045e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair351.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87577db2f0b643e0b2e8cf6004c51215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of abundance and enjoyment, as the chair is filled with a diverse assortment of dishes, likely prepared for a special occasion or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of abundance and enjoyment, as the chair is filled with a diverse assortment of dishes, likely prepared for a special occasion or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair352.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426a57ecbd9d41ffa8613f7098e682e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair353.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad139cfdefd6487b916d101a021e296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair354.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b519a43bd4495290ee0d59ea1a8ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair355.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fd3d694ab849328b6f5ab845c7e93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, with the chair being the main object of interest in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, with the chair being the main object of interest in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair356.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0cf32d9df5428091f3aa8e47d4c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair357.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db98456ea32544cabae9fa468e633a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the chair being the main subject and the white background providing a simple and uncluttered backdrop.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the chair being the main subject and the white background providing a simple and uncluttered backdrop.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair358.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22102f1c5484dc58d94991a058b0f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['various activities such as dining, working, or displaying artwork. the overall atmosphere of the image is one of warmth and comfort, with the red wooden chair serving as a central element in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['various activities such as dining, working, or displaying artwork. the overall atmosphere of the image is one of warmth and comfort, with the red wooden chair serving as a central element in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair359.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e0f39243ad40e1908dbddbb8a3a471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair360.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34137823b50e41f8a237d90035e7b0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair361.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9796303c0047718d16634db1f08161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair362.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efb6baf3c594a32908ebf16eb25e7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene. overall, the image conveys a sense of relaxation and enjoyment, as people gather around the chair to share a meal and spend quality time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene. overall, the image conveys a sense of relaxation and enjoyment, as people gather around the chair to share a meal and spend quality time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair363.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7673b4cacca4fe0a60267678fffcbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair364.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c787459e414337901e203fb78d0f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair365.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeee0dbe3f9401fb5239b5459b16aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of delicious treats.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of delicious treats.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair366.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec1db82d7b8457bb14958ba4dce6262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for gatherings or meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for gatherings or meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair367.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec626ce0ce44954836219f9efacc2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['people to gather around and enjoy a meal or a conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['people to gather around and enjoy a meal or a conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair368.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c7625366c5418a82b791a8787c3008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['unique design and striking color combination.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['unique design and striking color combination.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair369.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6443c222e1477b9d8da51901dbb1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair370.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061177b2a16d42cd8e3c36b465146514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair371.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e08f06c21042fdbe743c935930052b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair372.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a730eb8ee64ea5b2ef0bc0f458d0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair373.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bec11cef4cd4fd0b2381175b533db98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, as it depicts a cozy dining area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting, as it depicts a cozy dining area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair374.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce41dd31f4c4656943a8e19503d0c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair375.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27405c2ffed4747adcfd2cc1a29f273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the white chair and chairs creating a sense of simplicity and elegance.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the white chair and chairs creating a sense of simplicity and elegance.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair376.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82cae34d176a4449a6077ba95add8748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the dining area. the scene conveys a warm and inviting atmosphere, perfect for a family meal or a gathering with friends.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the dining area. the scene conveys a warm and inviting atmosphere, perfect for a family meal or a gathering with friends.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair377.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10e868ff48f4ca48ec07612080fda18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a warm and inviting atmosphere, making the area an appealing place to gather and socialize.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a warm and inviting atmosphere, making the area an appealing place to gather and socialize.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair378.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70640b9bf4364e839068bf10d1e5a134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair379.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26b70f07117426c8a00d8cee2fdfa9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair380.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24122a0178c4ff992c4ba4559ff981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair381.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe8b4d8bb324ea5b85404ca879ba1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the chair. the overall atmosphere of the image is warm and inviting, with the vibrant colors of the chair and the cozy arrangement of the objects.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the chair. the overall atmosphere of the image is warm and inviting, with the vibrant colors of the chair and the cozy arrangement of the objects.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair382.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c10203711b49b78751625b5cafe1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair383.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265ba289b04649b79d9adf5844c629e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair384.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad629c8432848a1abc32606fc70e52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warm and inviting, with the wooden elements creating a cozy and comforchair environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warm and inviting, with the wooden elements creating a cozy and comforchair environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair385.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b585dc2343b9456092a1254a524f826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of comfort and functionality, as the chair is designed to provide a comforchair space for dining or other activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of comfort and functionality, as the chair is designed to provide a comforchair space for dining or other activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair386.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85244db3a7e4316bfc55d7560ceeaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair387.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec544a480d54e60863b59e18c27e91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair388.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54438c59e9d24dca97ef23cfb63ecb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"relaxation and leisure, as people gather around the chair to enjoy each other's company.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"relaxation and leisure, as people gather around the chair to enjoy each other's company.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair389.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5534bbde23e247d0a0b70263d79664f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair390.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e687a54384496986ebd97a282122bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair391.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9098bd5a8bea46338316ae4b9b5ca115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of comfort and relaxation, as the chair is set up for a meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of comfort and relaxation, as the chair is set up for a meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair392.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1291a55aca4084acc6d1f568876619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair393.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345975145ccc4c16bca1d1144a027d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair394.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3769341926479a9f901a084c5b82ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of warmth and comfort, inviting people to gather around and enjoy a meal or a conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of warmth and comfort, inviting people to gather around and enjoy a meal or a conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair395.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886c18a6bfce447598af440f7bc5f079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance and sophistication, with the purple seat and wooden chair creating a visually appealing and inviting space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['elegance and sophistication, with the purple seat and wooden chair creating a visually appealing and inviting space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair396.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ec09a5a42f496eac0cd06e5b1463d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aesthetic. the combination of the round chair and the chairs creates a sense of warmth and togetherness, making it an ideal setting for gatherings, meals, or relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aesthetic. the combination of the round chair and the chairs creates a sense of warmth and togetherness, making it an ideal setting for gatherings, meals, or relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair397.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0cb0990e2e4faf8b6425a2fdff8c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is clean and minimalistic, with the chair being the main subject and the white background providing a stark contrast.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is clean and minimalistic, with the chair being the main subject and the white background providing a stark contrast.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair398.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b979406f73d414fa02749d26e7100d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and chairs arranged in a welcoming manner.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and chairs arranged in a welcoming manner.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair399.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08a6367626e466290bf4d4fc9501e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair400.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944f4fcb6f96430b9d038101f7c7f6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and comforchair, with the chair and chairs serving as the focal point of the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and comforchair, with the chair and chairs serving as the focal point of the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair401.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d758818f054ca788798bb7e4c7b3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is warm and inviting, with the vibrant flower arrangement serving as a focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is warm and inviting, with the vibrant flower arrangement serving as a focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair402.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf802274d584b6caecfbd05e3108b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and uncluttered.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and uncluttered.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair403.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c67d298be6461aa3bc32fc3203419e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the wooden chair, potted plant, and the cozy seating arrangement creates a warm and welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the wooden chair, potted plant, and the cozy seating arrangement creates a warm and welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair404.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaa21601aed4c91ade4f10e13c69af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is inviting and comforchair, with the wooden chair serving as the central point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is inviting and comforchair, with the wooden chair serving as the central point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair405.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bac829b67f4ae09c227a04137b0864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair406.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191e53359b6e4c9987ee3bc99a72a9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meal or gathering.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meal or gathering.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair407.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3214202bf6004fb4ac184a0763290e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair408.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a2bb1a20a140d3bc5bd5129e17c659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair409.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b21cb4b68d4e8a9c49e3e49b5ac1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair410.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5909459d60ec463e9ad580af5f1f51cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a gathering or a special occasion.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is prepared for a gathering or a special occasion.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair411.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2677c582095c4eb38ea1d52efef12ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['conveys a sense of elegance and sophistication.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['conveys a sense of elegance and sophistication.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair412.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb86745117547f486d61ff3d6f9cbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair413.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2c657c5ce341b38d6e2f2ed4ef5d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in a restaurant or a home setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in a restaurant or a home setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair414.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1269202f1a73488db4793b4dbb5b982c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair415.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e6ec034aa749119eb2d8701c58c1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair416.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b22deef937b4644ba2566599095e2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair417.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea06365d41a4f0e97bee017cd943596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair418.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0db7c01012f4bc2aaf24ed4108af63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and welcoming.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and welcoming.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair419.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1731bbc2d0f0496fbd93d3f2edf34999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['subject and the background providing a clean and uncluttered environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['subject and the background providing a clean and uncluttered environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair420.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d927856aa44f6eb138359144088b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair421.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f284980f76844ab8fc817be4172330f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair422.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b069e7ae4349c690874ad514bfd454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and chair serving as the focal point of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and chair serving as the focal point of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair423.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b79a8645fc426283a9a3bf6dca9d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of a cozy and inviting dining space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of a cozy and inviting dining space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair424.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2bf2c036414839b731ac681079f763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair425.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dcb153ee4d40d9beb953d05bf02356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sizes its prominence in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sizes its prominence in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair426.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a3faf395eb4f999e1a62f089409dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair427.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d096b18b4d7a488ba9edff0c618cea8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair428.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90e1dd8f3b24f869a5f97b3b50d5195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['disarray, as the chair seems to be a central hub for various items and documents.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['disarray, as the chair seems to be a central hub for various items and documents.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair429.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56549c98265f4e4a99f3bf6a7053bb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, while the bowl is located closer to the right side. the cup is situated near the head of the chair, and the spoon is placed on the left side of the chair. the overall atmosphere of the image is warm and inviting, as it depicts a cozy dining area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair, while the bowl is located closer to the right side. the cup is situated near the head of the chair, and the spoon is placed on the left side of the chair. the overall atmosphere of the image is warm and inviting, as it depicts a cozy dining area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair430.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7704273a1e354783800e1c4f568f5d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair431.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ef883c1d8e41bb82ffdf82dc0d5b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair432.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fb322d5db643378c2abfab497195be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair433.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bf37730a4e4c5e879e54b2b379a0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair434.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eaade59a9441d4956c0aded3cddb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the chair on the chair adds a touch of elegance and sophistication to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the chair on the chair adds a touch of elegance and sophistication to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair435.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dbb766bc0441fc85e9edec56db7a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair436.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6b1a18e2c84b0eab051d2ba15e9279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair437.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1124429e3bd41e698ea781ca66b3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall scene conveys a sense of warmth and invitation, as the chair and chairs are set up for a pleasant dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall scene conveys a sense of warmth and invitation, as the chair and chairs are set up for a pleasant dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair438.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f3192fdcce4b06930479d694ca254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair439.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd776c6bb97d497ea8fbf66d4f19a170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair440.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ce569125424d308c29b88c4240f8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair441.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e105f3798fc6410e8076dc1241e68197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair442.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a368faf1064407bd4475efb1665a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair443.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469f3ff4d29d4cd19aad66f16ffcbf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair444.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60abebd878ff483d9bb31a3422d12af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment, as people gather around the chair to share a meal or conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['enjoyment, as people gather around the chair to share a meal or conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair445.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4975d8d5954bdd9a931ba7595788b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a delicious cake and a cozy dining setup.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a delicious cake and a cozy dining setup.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair446.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d69c1222ec8465f95644cfbc68df643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair447.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c724bdbe042436280bcf85e3adbc20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seems to be cozy and inviting, with the chair serving as a focal point for relaxation and socializing.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seems to be cozy and inviting, with the chair serving as a focal point for relaxation and socializing.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair448.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d420d8be574453a04b8688b358b839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['clock, creating a cozy and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['clock, creating a cozy and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair449.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab010cf3c844489b8e04b8378dc0c33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a meal together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a meal together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair450.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495d503a08f409788caf4f7461dbba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair451.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c366920383ae423da2540d2829c48a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of relaxation and comfort, as the massage chair is the central element in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of relaxation and comfort, as the massage chair is the central element in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair452.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9820f03f28fe4608b80f1115070493da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['size and design make it an ideal choice for hosting gatherings or events. the combination of the wooden chair and the white background creates a clean and elegant atmosphere, perfect for a sophisticated dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['size and design make it an ideal choice for hosting gatherings or events. the combination of the wooden chair and the white background creates a clean and elegant atmosphere, perfect for a sophisticated dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair453.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1371241dde64586916cf855db3b66bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['welcoming environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair454.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e93b87fc674d2abac2ee0c629443e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['situated near the chair. the overall atmosphere of the room is bright and inviting, with the natural light from the window illuminating the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['situated near the chair. the overall atmosphere of the room is bright and inviting, with the natural light from the window illuminating the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair455.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c604f17f25849209780aeec1a9e9f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\". the overall atmosphere of the image is one of warmth and comfort, with the wooden chair and chairs inviting people to gather and enjoy each other's company.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\". the overall atmosphere of the image is one of warmth and comfort, with the wooden chair and chairs inviting people to gather and enjoy each other's company.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair456.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43092acda4b4ce99b784299b63726ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the chair to share a meal and engage in conversation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the chair to share a meal and engage in conversation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair457.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab97a948991420f8b8581c96204c88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the space.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the space.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair458.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07f6de55f054329b2f72df1ac4acfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['point in the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['point in the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair459.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd935adadd6d483e823536fbb4daef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden chair serving as the main object of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden chair serving as the main object of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair460.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16d9d8cf4274d1f8c22caf3834c0f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space for people to gather and spend time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['space for people to gather and spend time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair461.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a336e9f393e49cea369f796a01cfe56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair462.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42988d41bd3445fbf642b2a9e9aed8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of warmth and nostalgia, as the wooden chair evokes a sense of comfort and familiarity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of warmth and nostalgia, as the wooden chair evokes a sense of comfort and familiarity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair463.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9e8d749a4c417b87cb8d2248424566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair464.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bed6a7c4de4cd49296b7b4cb2d494d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair465.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085ffad40fc241218cfd61b63e442058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair466.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cc0dfec96c4528b79b8f44741bdd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair467.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c5179f8d324041842801a3c3d8b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting, as the chair is filled with an abundance of sweet treats, likely to be shared among friends or family members.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and inviting, as the chair is filled with an abundance of sweet treats, likely to be shared among friends or family members.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair468.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23de432a571a4174b149967dc5863ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and family.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and family.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair469.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb7ac1a84f542558919dce9062d8ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair470.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e60794668834225b1c5bc086028e99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals. the atmosphere is inviting and comforchair, with the chair being the centerpiece of the room.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['meals. the atmosphere is inviting and comforchair, with the chair being the centerpiece of the room.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair471.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1228944c05a34e81b86c615c0d773b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair472.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88673c43103b46caae7cd69635dfa8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair473.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbf90c06ce84b8fb477481294c2f38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair474.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34472c75cdd4b3faf73964cb0415210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair475.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6c9913974d457f894c703f2b61a426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting ambiance.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['inviting ambiance.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair476.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e59631e1d0644c9a9724f32e240aca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair is well - stocked with food and inviting for guests to partake in the buffet.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['chair is well - stocked with food and inviting for guests to partake in the buffet.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair477.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4299fc189bf4d0c84fa46df9ea97b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair478.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00721a64f904383ab7fc01210912a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['vibrant colors of the chair and the items on it create a lively and inviting atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['vibrant colors of the chair and the items on it create a lively and inviting atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair479.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c5d820307e4e58b7d2321c92132845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of comfort and relaxation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of comfort and relaxation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair480.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04eddb3265c4d8ea652f69fcf646f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere is warm and inviting, perfect for a pleasant dining experience.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere is warm and inviting, perfect for a pleasant dining experience.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair481.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9994bd58bd4265bd91c332c41248e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of comfort and relaxation, as the chair is set up for people to gather and enjoy their meals.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of comfort and relaxation, as the chair is set up for people to gather and enjoy their meals.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair482.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079552bc6d7f45d7800c2758ded287c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden texture of the chair also adds a sense of natural and rustic charm to the overall aesthetic.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden texture of the chair also adds a sense of natural and rustic charm to the overall aesthetic.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair483.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9e850f47ab4d0d8a7a4f015350070a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair484.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febe12b64d5b4f9da93b267a722d350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene is one of a cozy and inviting dining area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene is one of a cozy and inviting dining area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair485.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d72f919f504cca84f329466f251dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['encourages conversation and interaction, making the space ideal for social gatherings or meetings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['encourages conversation and interaction, making the space ideal for social gatherings or meetings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair486.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238c0fe2eea644ec927e5ed395e1d708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image suggests a casual and relaxed environment, with the two men enjoying their time together.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image suggests a casual and relaxed environment, with the two men enjoying their time together.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair487.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d142ca0b48449109eb0844e20317f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair488.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c65b6eab3748e8a5a52e83d7ed2d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair489.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9810d273764a405d8cff565b2ecfe3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair490.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf9dd9c0a34808826594d742b88c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere is one of abundance and freshness.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere is one of abundance and freshness.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair491.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e34af3aa034b39968012d1c5087962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['togetherness and camaraderie among the people present.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['togetherness and camaraderie among the people present.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair492.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08ab151dbb94b64a1c2249fb97185a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warmth and relaxation, as people can gather around the chair to enjoy meals, conversations, or simply unwind in the comfort of their home.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['warmth and relaxation, as people can gather around the chair to enjoy meals, conversations, or simply unwind in the comfort of their home.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair493.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06906b887c8469186cb9fb01e1200ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair494.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd8dd6428e64f658aa6326222c23ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair495.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e802d85eea1456eb0f03c5e0e80b18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair496.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a3f6f7889c4d7f9b2c1bd36f3a523d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair497.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a19035406354361b486993b1de9a9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['designed to accommodate multiple people for various activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['designed to accommodate multiple people for various activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair498.png for class chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676d12f8e24b40389042452efbc8e763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers seem to be thriving in their environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers seem to be thriving in their environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image chair499.png for class chair\n",
      "Generating images for class 83: sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7408763033784f9baf5fb26d7bea84d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a serene and peaceful atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a serene and peaceful atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper0.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3aa7f5474349a0b67d92234ee8ca72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and intrigue, as the viewer is drawn into the world of these fascinating fungi.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and intrigue, as the viewer is drawn into the world of these fascinating fungi.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper1.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a3e56557349ed9483222850260aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper2.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2295fc6b677740198994e04f8790be19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper3.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0890e120c7c42178998bfc3ef636707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with its surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with its surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper4.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b823dacd01e747f886dd80278f453b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper5.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077cefeb35914fdaaa3b3a83ed4c698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper6.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610edf0989b34c0cb926945cbc3afc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper7.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff3acdcba5c42dea70edac69138033a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper8.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a34ae59fad4a42b81fce8f3fcb9971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper9.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56991e3235254bcb9fa06d45b020b39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growth of the sweet _ pepper in its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growth of the sweet _ pepper in its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper10.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21a9531a0744c8ea65a12c0eb2162d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper11.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e717b6c0ae414ea5b287c467dffecc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['backdrop of the forest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['backdrop of the forest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper12.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fec657bad8c442b91a0022916750a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper13.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ac70c293f749a1a7d15a755e2ec343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper14.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938a942c646246d38d3ddbdaa9570768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and serene backdrop.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and serene backdrop.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper15.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678b067813b2466c8bb16485d791775b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper16.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7649e99be3a7441c97ea9ea3318b23be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper17.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d57e06e4c540a0a6cfce5fe1b2a559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper18.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a86f0c0ef4a4843b9917fdf74450212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper19.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3746896368e34d36b1f516141b751cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['forest, creating a serene and peaceful atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['forest, creating a serene and peaceful atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper20.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7410729cde274614a05b257551930698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper21.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b0caeb3faf47e09ccbea89b4bcb34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity and fascination, as the viewer gets a detailed look at the unique and captivating sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity and fascination, as the viewer gets a detailed look at the unique and captivating sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper22.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63b3cd43fff4f96a8a3927a7f83888d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a focal point in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a focal point in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper23.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27676c3f61c44ad8c3c9884f031e1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper24.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79652e4a60f47f2a596ba417a49f46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a captivating centerpiece.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a captivating centerpiece.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper25.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618f918ef70f4b5bbea32aec9584c41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper26.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20111a3e22c494e971ae89bdc1edf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper27.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b34d9652b354b0a94148e9013312b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper28.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3980d1b8b72484d95c294fe1d7035e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper29.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e674d81b7bf4b6a88514fc200c9d6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper30.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e4c8628b8e43fba193449f656de034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper31.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a559362da1c14b3bad38a7e2b340518e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper32.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1399e836ae4146fe9c3eeba753929a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper33.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045c28e9733f495ea412f7ee36d87f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper34.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41ceee307bf4dd6bce20d9841582828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of intrigue and curiosity, as the viewer is drawn to the unique and captivating appearance of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of intrigue and curiosity, as the viewer is drawn to the unique and captivating appearance of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper35.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64921da84394c4199852a08f5fd701f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper36.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd41df479cb244b1a5a652f229332e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a unique and interesting element in the landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a unique and interesting element in the landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper37.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd13a9b023947a9b271638e049e7d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty and intrigue, as the sweet _ pepper seems to be a part of the surrounding environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty and intrigue, as the sweet _ pepper seems to be a part of the surrounding environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper38.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d84fdac4a24c94b66916a3193c5cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cap and adding to the overall visual appeal of the scene. the sweet _ pepper appears to be growing in a natural environment, possibly in a forest or wooded area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cap and adding to the overall visual appeal of the scene. the sweet _ pepper appears to be growing in a natural environment, possibly in a forest or wooded area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper39.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6de26156f34d4dba4e419e3a9c1918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, with the sweet _ pepper serving as a captivating centerpiece.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, with the sweet _ pepper serving as a captivating centerpiece.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper40.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c331b544e54762a34ca39599383864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper41.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5958c622f33345ebae543bfef75655aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper42.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3a675ca9464f2f9e8a63f3538cff74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper43.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc295ae1bdd48ee82553487069412eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper44.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0baaba868a744f1691d7d3130a8e4feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ peppers and their surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ peppers and their surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper45.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2173a9cf7f1f477cba6663d139c47692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper and its environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper and its environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper46.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd6d3eaeabd45bea3ff60c6dbfe83a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper47.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a3ee6a748b4d84a7e4e02a124dee7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper48.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ba7a34c6504be6ba641fe42e67ba23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper49.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a8d403786e4bddbca01de3c22f0a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper50.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6734d2057b4597893418994d14e2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of the forest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of the forest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper51.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0e3422103b4ff99325474c98402710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper52.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a49ca0b05140d586cb3b8d4ff3e266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper53.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d8dc4d689444ee992ee04a89b6d174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject in the picture.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the main subject in the picture.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper54.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8619dc2aa54f7a85f5b1a07331619c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the sweet _ peppers seem to be thriving in their natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the sweet _ peppers seem to be thriving in their natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper55.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8ba9ba8322412382a11752085523c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of serenity and harmony, as the sweet _ peppers coexist peacefully with their surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of serenity and harmony, as the sweet _ peppers coexist peacefully with their surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper56.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b849fbb842bb4c5ab14057d9e851041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image, evoking a sense of tranquility and connection to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image, evoking a sense of tranquility and connection to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper57.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a691d0a1aa034e2abc7c40c9faf42185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper58.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ea54e3bd5741a385c1d0b8feff2fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper59.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afff1d223774ca886495a0380066f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"part of the earth's ecosystem.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"part of the earth's ecosystem.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper60.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d02b018b9346a28a6b3e3c68aa8c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper61.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601cf24e643e4786a4d5f47ace691f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper62.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e793debcbd92431dbd8ff522e279f654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['foliage.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['foliage.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper63.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9989fc628f9a421fbbb9b5020b611fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper64.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3440e256264c4181722a1fcda8691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a focal point and a reminder of the wonders of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a focal point and a reminder of the wonders of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper65.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc55b47ef0c848c4b3bfc9f2b7393d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper66.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58f85f0d18140c6a90a3188b2d2554e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper67.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d785131b574f0791a9c91556910ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper68.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cdd2e67b9c4f0690713fce4ac891e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the sweet _ pepper serving as a natural and organic focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the sweet _ pepper serving as a natural and organic focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper69.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1596b4230af44c994450ec9cd2cccd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper70.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c279d1c195d4204ba20604fe667b157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['out as an unusual and fascinating natural element.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['out as an unusual and fascinating natural element.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper71.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82a338ea12348c896c33f5dea54579b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper72.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a75069d6e14b09adc34ebd44c1f13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper73.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eca89b213f496d97e6b7c3ef382ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cap and reaching towards the bottom of the image. the overall atmosphere of the image is one of natural beauty and intrigue, as the viewer gets a close - up look at the fascinating details of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cap and reaching towards the bottom of the image. the overall atmosphere of the image is one of natural beauty and intrigue, as the viewer gets a close - up look at the fascinating details of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper74.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f569bb3a89405b93353726a220f39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper75.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b270f7836fba4f5cbb734f9540761332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper76.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b555033f8d9438c9b846a1d1bb60304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper77.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bff654e2cdf4b329ab9347bdfac7b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper78.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cb8ce0ff8e490a8a951db89a77a0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of serenity and connection to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of serenity and connection to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper79.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3d9daab5b34c60a6b3376f7567396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['two fungi.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['two fungi.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper80.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d355e2c73747cda29f518d9411816e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper amidst the natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper amidst the natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper81.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc70609a51d848baa340f6c9ab5e2ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the sweet _ pepper being the main subject and the background being relatively plain.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the sweet _ pepper being the main subject and the background being relatively plain.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper82.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfee18428ff48678400dda971413fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper serving as a focal point in the natural landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper serving as a focal point in the natural landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper83.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d536c0873428d8fba650aa0b5d532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['being found in a forest or a similar environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['being found in a forest or a similar environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper84.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18dabcaa94348679a81640f9992d12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper85.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a4e63cb6ef4fe4828efe9e53d8c738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be the main focus of the image, with the surrounding greenery serving as a backdrop.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be the main focus of the image, with the surrounding greenery serving as a backdrop.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper86.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf372a8f83544f4ba61e3e92b21c53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper87.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847e357e581d49f8826bdf1fb1d2affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper88.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0167bc1bb0f427d97417291c74dda8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper89.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f23087e3a45c18429dc777d1f7346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper90.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32b6d40b934f718f1cc123028d41f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper91.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed37a8d40af4b71a0b98696f5826836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is one of tranquility and connection to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is one of tranquility and connection to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper92.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3096efc2b5d47a6be4bb3d5864afee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and vitality, making the sweet _ pepper stand out as a focal point in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and vitality, making the sweet _ pepper stand out as a focal point in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper93.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f5e967cd90449dba1c871c92ea200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shaded area. the overall atmosphere of the image is mysterious and intriguing, as the viewer is drawn to the unique and captivating form of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shaded area. the overall atmosphere of the image is mysterious and intriguing, as the viewer is drawn to the unique and captivating form of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper94.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0324b2b84e9f4e578a6f5d2208faa09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper95.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ccf2d1aed14c0184aa91f00a064465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper96.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cd1a5f046147a099e4818e9ff4e312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper97.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0c89ba3bfb495a8c9be85732f2ab0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ys a sense of abundance and variety in the sweet _ pepper species.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ys a sense of abundance and variety in the sweet _ pepper species.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper98.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9d77fdc19a4a24ac8520bdeed694fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper99.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0d51f13f024071ac06952edc7a4ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper100.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799e68c9882842fca817ac7ae0e348e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper101.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32141ddae7244f0a26f50249f3a64bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper stands out as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper stands out as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper102.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d0ea6e3a3a4445b94a5d7677025f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper103.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45244bd509904987b74d409f6a0a0e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the sweet _ peppers growing on a tree. the overall atmosphere of the image is serene and natural, showcasing the beauty of the sweet _ peppers in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with the sweet _ peppers growing on a tree. the overall atmosphere of the image is serene and natural, showcasing the beauty of the sweet _ peppers in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper104.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7123a375afd46eca65e2c9b07696eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a visually striking scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a visually striking scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper105.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c3b65add8045f78b1607aa3fd01dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper106.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63526c5971ad4147be83344835690cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper107.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9725977ec4fe68f4452eaa6e5b7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper108.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e358f8dcf314ea7bfd8e5bff4502d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and wonder to the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and wonder to the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper109.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f4bef5434144e1a6870be3ba055721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper110.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c57f578e9a4c62a8d87d79f0707748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creating a striking contrast between the cap and the stem. the overall atmosphere of the image is mysterious and intriguing, as the viewer is drawn to the captivating details of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', creating a striking contrast between the cap and the stem. the overall atmosphere of the image is mysterious and intriguing, as the viewer is drawn to the captivating details of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper111.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893fc46075e8406ab54902028a05d4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper112.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9647cded154be88b1b5036e59a227d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['textures and details, making the image appear more vibrant and engaging.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['textures and details, making the image appear more vibrant and engaging.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper113.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14c42973ca5431bbb767d9e4319bd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper114.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea9bf2e8f1f4a6c8984d9b1cf703575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper115.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e744333c372e44db877e6be60a58ea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper116.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9055852d84f14bf6fa37cd8bc91fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evoking a sense of wonder and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evoking a sense of wonder and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper117.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eec4ec53d66404190a47b5dd7c4edde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the photo is serene and peaceful, as it captures the beauty of the sweet _ pepper in its natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the photo is serene and peaceful, as it captures the beauty of the sweet _ pepper in its natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper118.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142168d993de4441b7de3a9b2f607f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image evokes a sense of nature and the beauty of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image evokes a sense of nature and the beauty of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper119.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73b3596d364caabade7fe263943e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper120.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9745d2948e4defabfa7e02cc9f0e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper121.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19ae89acf5f4e86ae7ac5b515758a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper122.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788a70c85a2b4e55927d5715d63a8d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper123.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3c093079704ad2b9104ecec1871a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers are presented in a unique and imaginative manner.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers are presented in a unique and imaginative manner.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper124.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3359d84bfad3438ab542c0e11f46ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper125.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b100821d54f6c81b8d90cb0988f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper126.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa81b51c974f69806f122816e86106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper127.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e56b6aa7e94df9bc3b0825f8367326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is serene and peaceful, with the sweet _ pepper serving as a reminder of the beauty and diversity of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is serene and peaceful, with the sweet _ pepper serving as a reminder of the beauty and diversity of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper128.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d45370165e4d268da4288770da223f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper129.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df773ec07a46d1af6ec5b1c9577a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of organic matter.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of organic matter.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper130.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c612d86ea9841b893add441895ea545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper131.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba02fababee4e9eb63f613ab5317b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper132.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db43ff9f823a4031b6070ceba1d66cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"it captures the beauty of the forest and the sweet _ pepper's presence within it.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"it captures the beauty of the forest and the sweet _ pepper's presence within it.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper133.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fae853a776b471fb00f63f94d8641bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper134.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698035c65c6a408db6acb235ce4cb0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['light reflecting off its surface. the overall atmosphere of the image is serene and peaceful, with the sweet _ pepper serving as a unique and interesting focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['light reflecting off its surface. the overall atmosphere of the image is serene and peaceful, with the sweet _ pepper serving as a unique and interesting focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper135.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28678e16681e4d70bb2a90b2c5234959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural beauty, as the sweet _ peppers seem to be a part of a larger, diverse collection of food items.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural beauty, as the sweet _ peppers seem to be a part of a larger, diverse collection of food items.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper136.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a151c322055b41ad8a6cd386bf9c08d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper137.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b64ba2f3a54ec190f762bb16013796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper138.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63476c9f254d4a70a67c526a6fc7be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['world of fungi and the diverse ecosystems they inhabit.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['world of fungi and the diverse ecosystems they inhabit.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper139.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1548e441824127b9f49aefdf3fc9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper140.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b8657a90dc45df96c534604a3a7186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper141.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2388ef2e10084d029ef52060d9ac3ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper142.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2e9dbf86d5430189bee0fea119fe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper143.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d1520466624070be23ba1d29c71f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stands out against the backdrop of the forest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stands out against the backdrop of the forest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper144.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed04cc07238b450788bd7f92dc6fd7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper145.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d79c13cb9c340c08686704653ca715c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the overall atmosphere of the image is one of natural beauty and serenity, as the viewer gets a glimpse of the intricate details of the sweet _ pepper's structure.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the overall atmosphere of the image is one of natural beauty and serenity, as the viewer gets a glimpse of the intricate details of the sweet _ pepper's structure.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper146.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b40c14c6a8c450ea036026e1fa9923d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth and earthiness, making it an interesting and captivating subject for the viewer.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a sense of warmth and earthiness, making it an interesting and captivating subject for the viewer.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper147.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee464f896fe447cb582ada932f70fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene, making it an interesting and visually appealing image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene, making it an interesting and visually appealing image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper148.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d97fc27670b4f04965c3da4a1c97df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper149.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba6e8978f624a819a229978c1f8ceb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper150.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15448602a5204bd2a3ba1ed9d76fb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que features of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que features of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper151.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290c5fe5a7de4f0c984e739153544905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper152.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d68b438adbd499695805797740d5198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['unique appearance and texture of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['unique appearance and texture of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper153.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ef885e922342cba8db5d569f2157ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper154.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca52ec88e6c40a4b30a934f2314e5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper155.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef769f68fbd47cebc299422467847c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['kingdom.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['kingdom.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper156.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd0aa081ee14b9c8fcfa9c754b2645a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper157.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc472f55f204436b7d443683233057f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper158.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7dfa183d724fb2af55f70c703d9ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper159.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0594c8126e0495d90435f9bcbed31f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper160.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698536352c0947f6bfd843edcb9208a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper161.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7091ed870e48fea37d2892eb3a07fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a natural and captivating subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper serving as a natural and captivating subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper162.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981e3b15b3334d18979c9e7545d887a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper163.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e480354be6a14a9baeed820f093fef9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['catching element in the natural landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['catching element in the natural landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper164.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3952705d27184c11964f938944a3d4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of freshness and naturalness.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of freshness and naturalness.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper165.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a902afc5a374a529234a577ad4cadac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper166.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12256e4961441a2ace6559592ab679a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, as the sweet _ pepper stands out against the white background.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, as the sweet _ pepper stands out against the white background.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper167.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7133d8a2629b4d8abc14d515ebdacf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper168.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c5367e0600448fabf7e4e429ff01c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leaves evokes a sense of wonder and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['leaves evokes a sense of wonder and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper169.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fa238be9924d78bd4c804989226daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper170.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc3f865e5a740168729a172e39041fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a lush backdrop.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a lush backdrop.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper171.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9f82ffcce84d95925629a7a07df87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper172.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d6b74ebf274ce6b3375844473e8f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in an unusual and potentially hazardous location.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in an unusual and potentially hazardous location.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper173.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20af5e1d63e494ab420ab2adf72dde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper174.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407ee129e90473b96b51d87a0996416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as one might find such treats in the wild.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as one might find such treats in the wild.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper175.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c23584ee9244f7b502ade7f2cc3e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of nature and the growth of the sweet _ pepper in its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty of nature and the growth of the sweet _ pepper in its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper176.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d014fc2707487b9e429c0bf0608b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of natural beauty and intrigue, as the sweet _ pepper stands out against the white background.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of natural beauty and intrigue, as the sweet _ pepper stands out against the white background.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper177.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739b003ccddb43e3b0c3ca18cb2d0377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper178.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb71eca1d7be4631882bf972b5895bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper179.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4f6207948f4a6fb6c205f4d988d042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a focal point for the viewer.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a focal point for the viewer.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper180.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3397f823eba4d5a97f146eab336017c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of natural beauty and fascination, as the viewer gets a close - up look at these intriguing fungi.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of natural beauty and fascination, as the viewer gets a close - up look at these intriguing fungi.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper181.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e521628b58b34a2aaaf3db8ecb67bd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper182.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b636628f0ac54021b2e2ab0696dc2c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper183.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa4dd4986e3425eb61218c9fc1e9901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in their prime and ready to be enjoyed.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in their prime and ready to be enjoyed.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper184.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e1cc2ad5c24d27b0964998da086151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it stand out. the overall atmosphere of the image is one of natural beauty and fascination, as the sweet _ pepper is a unique and intriguing subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it stand out. the overall atmosphere of the image is one of natural beauty and fascination, as the sweet _ pepper is a unique and intriguing subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper185.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307dcbc289c545fcbe55c8bdcfc26019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['part of the natural landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['part of the natural landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper186.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88084bcdc333454fa884e89a091cc04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper187.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5289c513a34c6297eb81f5ce2d6b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper188.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afce0cdecd34d8e8523290502c024d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sy and playfulness to the scene, evoking feelings of warmth and affection.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sy and playfulness to the scene, evoking feelings of warmth and affection.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper189.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce73f0010d14f6080660d2f5507a002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper190.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e4a77a2474b0aa8b6b552d05e0177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper191.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf513b1bf547b1a2c341ed698c50e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper192.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdd0971a45d4e318b71614fc20f1054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper193.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422cec1a437c43f5824c0f738d2f91a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper194.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa27465bb54f447b8feeace10d7200a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper195.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b975c54da75d40e4ace369beac0ebfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stillness and natural beauty, as the sweet _ peppers seem to be peacefully resting on the table.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['stillness and natural beauty, as the sweet _ peppers seem to be peacefully resting on the table.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper196.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f29600b01949ecad93c42ff4c81214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper197.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229b171140cf443ca41df7ebe3882ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper198.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2bd927bacf41ce92def3f31e68c88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper199.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05542a6db43d4102b1d79a08d13d250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural beauty and tranquility.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural beauty and tranquility.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper200.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9457286ecab4bc79ea0531737897731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper201.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9454846ff1df4946bae99e7aa4278973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the sweet _ pepper's growth.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the sweet _ pepper's growth.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper202.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe44b59a1d942c0a56daa5d6e1b0437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper203.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1f450e8e154d28b9ec5e80dca1128e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper, creating a visually striking image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper, creating a visually striking image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper204.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cff2620907d4b8db2b4aa853d77b47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper205.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586cd705a384d51994c01001947a190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper206.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b14108295754713b7eabf71976eb31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growing undisturbed in its natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growing undisturbed in its natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper207.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3218494a424e4b3098d0d9f261bb24cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peaceful element in the environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peaceful element in the environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper208.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185c98feccb4654b144d6e95fb436cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper serving as the main subject of the photo.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper serving as the main subject of the photo.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper209.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfa72c3c0e94775af540afd83388510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper210.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca02d3d81b143b4803ba2a05b7b2aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper211.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af685f0064cd42ac80804e0babed2715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer is drawn to the unique and captivating presence of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the viewer is drawn to the unique and captivating presence of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper212.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854f9a2a03f640c7a5afc22970782b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper being a prominent and eye - catching element in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper being a prominent and eye - catching element in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper213.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71dbfd3293e4006a899aed3b9d18d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper214.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887adab33ab4198bbb38da9b8db50dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper215.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a33034acb834da084af7bf066af2212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper216.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd4a38cbbd54a46827a2eae8b6bfa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper217.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e40a1c1228e436fb9e8eb48b8a23409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper218.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb0464d0882466cb56c1d186e799b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper219.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e954c1178c4e9d9f80efe2b8fa08e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene evokes a sense of curiosity and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene evokes a sense of curiosity and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper220.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabc075fe2b3452a9c0ace2aeb6413df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper221.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5f467db7a94e999023362103737974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evokes a sense of curiosity and appreciation for the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evokes a sense of curiosity and appreciation for the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper222.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83f15d815a145c4895e5580a29b7da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper223.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ae79852a8a457e831fcce37d61e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and vitality, evoking feelings of wonder and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and vitality, evoking feelings of wonder and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper224.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f56e32597234ad8a551286f2443c237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ionship or togetherness. overall, the image conveys a sense of nature and the beauty of the sweet _ peppers in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ionship or togetherness. overall, the image conveys a sense of nature and the beauty of the sweet _ peppers in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper225.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5e1a97df50489ab9f787ec3842eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper226.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41c3f8907df45b59c0f44700d44a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['presence.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['presence.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper227.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec66e86182cd47cd826472f4a78edce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ peppers seem to be thriving in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ peppers seem to be thriving in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper228.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b732e0b9bd74f7faa629560bf5f4a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper229.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9387cc3c9374dd1b2fb6dd1c61bf89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper230.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9650378ee3c844808ee5c5a86588e370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wildlife.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wildlife.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper231.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8bb4a21e847c6b8343513a88bc113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['backdrop of the rocky terrain.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['backdrop of the rocky terrain.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper232.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c2f618541c4575b425bdb63e8dcea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper233.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3f6dacebb446f3af2bae110cb33d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of freshness and natural beauty, as the sweet _ pepper is the main focus and stands out against the other elements in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of freshness and natural beauty, as the sweet _ pepper is the main focus and stands out against the other elements in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper234.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c237839d02aa4c0c8a558530631ddd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper235.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f273433afb7a41589f7cbe665e58c12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper236.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf0d733498451dbfe18f497af34fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interesting element to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interesting element to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper237.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5a1ca376964e77af33e0106df71dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their prime and ready to be consumed or used in various culinary creations.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their prime and ready to be consumed or used in various culinary creations.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper238.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a722da301b433ab7826496c8a7c6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper239.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4413462bc944fc48d6944997badb54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['both coexist in the forest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['both coexist in the forest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper240.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb27dcc3af8040e6b70c222c81412b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden table. the overall atmosphere of the image is calm and serene, with the sweet _ pepper seemingly growing out of the table.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['wooden table. the overall atmosphere of the image is calm and serene, with the sweet _ pepper seemingly growing out of the table.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper241.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d881bcb95414327acbd398f47530c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural beauty and tranquility.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural beauty and tranquility.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper242.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ece2e9a9004593b5bff6e2d9658328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural beauty, as the sweet _ peppers seem to be growing in a healthy and thriving environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and natural beauty, as the sweet _ peppers seem to be growing in a healthy and thriving environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper243.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2e2895d46e478c912556a60eeb003c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper stands out as the main subject in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper stands out as the main subject in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper244.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6814892cd15044fbb4d8d3f6ee3d89cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"as the viewer is drawn to the sweet _ pepper's unique and captivating appearance.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"as the viewer is drawn to the sweet _ pepper's unique and captivating appearance.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper245.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48c611cd8414d909c75f380cb071845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper246.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ff75819275498dad3e71614511d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point in the composition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['focal point in the composition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper247.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de89efd0ef24ee895877ab219643c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper248.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a506b1a7f03c4ffa953195f37dfc4f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper249.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2301fffe0643059db238c33cbebba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper250.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33404f66216c48ab9ec70f3055a2cbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['harmony with nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['harmony with nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper251.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbaebbf44ea479e998556f98cc1c43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the tree's natural growth.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"the tree's natural growth.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper252.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1747b3f226114d359f2cc336999c816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper253.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d22b579e5634a52a1692c1033de2265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the sweet _ pepper serving as a natural and captivating subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the sweet _ pepper serving as a natural and captivating subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper254.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb114bea9ed4a78988c4c9c7ce82996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper255.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2017be70844742909d7104b7fa642aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"part of the forest's ecosystem.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"part of the forest's ecosystem.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper256.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764f15a3c13647b48b5d02c15190ed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper257.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c339bb196db4a33ba95c68d2f1ee8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of natural beauty and mystery to the environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of natural beauty and mystery to the environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper258.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4440e5c502d417bb0819726576742a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper259.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e501de12830474bbf688288075fff0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper to grow.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper to grow.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper260.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751448ca94204aeca89ba86732492727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper261.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd1a7853bf34f39982f49eb68cda1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and contrast to the scene. the overall atmosphere of the image is one of natural beauty and tranquility, as the sweet _ peppers seem to be growing peacefully in their environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and contrast to the scene. the overall atmosphere of the image is one of natural beauty and tranquility, as the sweet _ peppers seem to be growing peacefully in their environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper262.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61fb3a3193d4a708eafa9ceb12fda95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper263.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcb2df3b09c492799ee25484a7236c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['undisturbed in its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['undisturbed in its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper264.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374baf302b6f41ea967fc620efebb633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['explore the world of fungi and the diverse ecosystems they inhabit.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['explore the world of fungi and the diverse ecosystems they inhabit.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper265.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cf87324c634f01be181abc5f7d7270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper266.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a3a35d945f42a8bf116252d22a1c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, as the sweet _ pepper grows peacefully in its environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity, as the sweet _ pepper grows peacefully in its environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper267.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848829d482474966997de4420543b6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper268.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2289c77999243159583d284c7ab7de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the combination of the sweet _ pepper and the plant creates a harmonious and visually appealing scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the combination of the sweet _ pepper and the plant creates a harmonious and visually appealing scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper269.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb2ddaf3a224100b945d8f7d5989d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper270.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bc4a5e354d4e3d97e5185b0abff09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper271.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0805d23054cc4a54b4378483175ff0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper272.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414f4ce5c19e4c69a514a15b24f6e9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity, inviting the viewer to explore the natural world further.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['curiosity, inviting the viewer to explore the natural world further.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper273.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d91d58735e4838bb9a99f15c559401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the unique and striking appearance of the purple sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the unique and striking appearance of the purple sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper274.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df79bd27fc4002b8141fc8943b18c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the natural setting create a serene and peaceful atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the natural setting create a serene and peaceful atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper275.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8353cd02904ba3a806038fbf01341d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper276.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380a0ad65b1f412fb4c44b8359370381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seems to be the centerpiece of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seems to be the centerpiece of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper277.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5aa7c5659c4bb389aa727388de78d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper278.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc40bedab01343b993a4b352c0ed1834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peacefully on the tree.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peacefully on the tree.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper279.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b381c066b44dfcb8ded7c668dca702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', inviting the viewer to explore the scene further.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', inviting the viewer to explore the scene further.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper280.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9d2b9150c34aafbf857bfca823d401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper281.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac27e981e5c4aaf94be69a82d053093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and peaceful, with the sweet _ pepper serving as a captivating focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and peaceful, with the sweet _ pepper serving as a captivating focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper282.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7590eb2b6f064f4b9da449beb5e5bc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of natural curiosity and appreciation for the beauty of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of natural curiosity and appreciation for the beauty of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper283.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4fb142ae644d98aabeaa32abd02f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper284.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c059e050f6b9404194b5f293c5a06808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of natural beauty and serenity, as the sweet _ peppers grow in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of natural beauty and serenity, as the sweet _ peppers grow in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper285.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26cc3a603424432889a8a7305906bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper286.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86348302bf404984a3618ef534f0d5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper287.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad4e2cca38943f48d99481c34fba793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to its unique and eye - catching appearance.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to its unique and eye - catching appearance.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper288.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d17d1cd72a415481b3fcae8c39836c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper289.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4088c8d37edf462a89035ed366c56bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper290.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4de6efbe20b4c98bd5f31c9a3142542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['harmoniously within the tree.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['harmoniously within the tree.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper291.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8c61c844334bbb964900b15b6ec9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty and tranquility, as the sweet _ peppers seem to be growing in a peaceful, natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty and tranquility, as the sweet _ peppers seem to be growing in a peaceful, natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper292.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fb24f6f2e446af9882d1850a36be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a unique and eye - catching element in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a unique and eye - catching element in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper293.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be64b62fc19e4b9aa2fa27507a1d4b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper294.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfeb23827f640f9af1cbcb0928965f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper295.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8698028e6f7345f281697b800a9dc145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is lively and captivating, showcasing the beauty of these unique and colorful sweet _ peppers.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is lively and captivating, showcasing the beauty of these unique and colorful sweet _ peppers.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper296.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef00efb6a3494bd0ab180a8e65f5a991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper297.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5380b4d823414909b2bb30f2b32c3847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it an interesting and captivating subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it an interesting and captivating subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper298.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f640050d42644704b81c127001d19b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper299.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a211ed8b9b842efbeb561a956df2880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper serving as a reminder of the beauty and diversity of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sweet _ pepper serving as a reminder of the beauty and diversity of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper300.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142445cdffc84fada0fb948b3757dedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper301.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1621b0df1a834292acf898e4cd739899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the viewer gets a detailed look at the intricate structure of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the viewer gets a detailed look at the intricate structure of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper302.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236a2a5689d8487883249513053ae8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and growth to the environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['life and growth to the environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper303.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2b9afbc6c54c44a4fc38d637245aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and their arrangement creates a sense of order and harmony.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and their arrangement creates a sense of order and harmony.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper304.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dd859e62e4429484f98204667b25d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interesting focal point in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interesting focal point in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper305.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce395118c3074ffc853d33696e6a6a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of natural beauty and serenity, with the sweet _ pepper serving as a captivating centerpiece.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of natural beauty and serenity, with the sweet _ pepper serving as a captivating centerpiece.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper306.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4698bee837e4f1081af8006cc47de83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper307.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e2ced0aa9e45d389fe827d6c3b1ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper308.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf23809593c431cb877002a80553a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['captivating scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['captivating scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper309.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a49d3c1c1b42d0805849445693ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper310.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db41881d745455e863212ef5e16c71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper311.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec26a69b51d4c45840bad08ea9b1ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper312.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f15c681fa048f8a00e4dce355e40d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper313.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb1797db0524a3cb0f07dcd278ebfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper314.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528a9d58fc4b416f9d8d88b040d8b3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper315.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4699326208c0406a9358de9ba4ee1eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper316.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8c9b966c814b50b2f9ad5d16c8cacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper317.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed72b69d486439fb456114ec81c36a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in a lush, green environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in a lush, green environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper318.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c41177b1e1454b8fa3cec628bc28a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper319.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be4b20d1cb49c38c4ac0febcd623d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['antly brown, with some variations in shades, giving the impression of a rich and diverse environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['antly brown, with some variations in shades, giving the impression of a rich and diverse environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper320.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d995117a61485e9b66307cd4cd04b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper321.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f834dc87ef4cdc9e8fe2ef21047072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ating its importance in the ecosystem or its potential culinary uses.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ating its importance in the ecosystem or its potential culinary uses.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper322.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb0146062994fd891aed049a77846bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for those who view it.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for those who view it.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper323.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1120aa2242344164bb8b9ddb075b84f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper324.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83458935d56145afb990748066746239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the environment they are situated in.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the environment they are situated in.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper325.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a0be34952a4dc29f764e9a11114c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the sweet _ peppers seem to be peacefully growing in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the sweet _ peppers seem to be peacefully growing in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper326.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c72e1047fc246c09132c3abb9c94781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper327.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761185f75f054ecf9c80e6ad93c6a96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their outdoor environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['their outdoor environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper328.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8674171b008f4893b023dfbb7daea087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is serene and peaceful, inviting the viewer to take a closer look at the magnificent sweet _ pepper and the surrounding landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is serene and peaceful, inviting the viewer to take a closer look at the magnificent sweet _ pepper and the surrounding landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper329.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e709accd448a29ecb4771103f45fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it an interesting and eye - catching subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it an interesting and eye - catching subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper330.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27701e0de3ae4df8baab00cbfb3f2986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper331.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad26c0dff8f743fb9490e660998dad5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper332.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002f50d3dc3245389a44d3a6e36f6991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper333.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79563691d514c8cb3e8d44e538be3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of natural beauty and the simple elegance of the sweet _ pepper itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of natural beauty and the simple elegance of the sweet _ pepper itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper334.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc88bab142b4db58cd2f888a8fb51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper serving as a captivating focal point.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper serving as a captivating focal point.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper335.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebbe923670145469328ea9468852822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"a part of the tree's growth.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"a part of the tree's growth.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper336.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9e129caffd4f94bb79c37e1b47b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"image conveys a sense of natural beauty and the intricate details of the sweet _ pepper's structure.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"image conveys a sense of natural beauty and the intricate details of the sweet _ pepper's structure.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper337.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228f6bac14ac457eb8bb241af702e727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper338.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f3be6b351445e09dc65021f0df9308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper seems to be a part of the peaceful, lush environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sweet _ pepper seems to be a part of the peaceful, lush environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper339.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051b6fc8124a4984afaa37f9a68904ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper340.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f67ae7f50e349868830ccdacae84d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers seem to be growing in a lush, vibrant environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peppers seem to be growing in a lush, vibrant environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper341.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887f27155b44d238d8ea65c49cfab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of intrigue and curiosity, as the viewer is drawn to the unusual and captivating display of these sweet _ pepper sculptures.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of intrigue and curiosity, as the viewer is drawn to the unusual and captivating display of these sweet _ pepper sculptures.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper342.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e050035e2a488f902aaeef96d10783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper343.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a902107dfc40bf909957bee4bd260c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper344.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0b9b9fe1bd4ef0baef7ce711828142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are predominantly earthy tones, such as browns and greens, which contribute to the natural and organic feel of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['are predominantly earthy tones, such as browns and greens, which contribute to the natural and organic feel of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper345.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4098e9ed5a664dbfb23b3979d477dc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper346.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c923b0ad89e426194fd0be83cd5c8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serenity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper347.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c95830a5b6497e9571109a68591e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['- catching element to the scene, making it a visually appealing and memorable image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['- catching element to the scene, making it a visually appealing and memorable image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper348.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3fde00de484d44bde718ce66133dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper349.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526b5548207841d2b0dc381daf2ab60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper350.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835181875ef6455fb0ba4a374f727dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural and captivating subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['natural and captivating subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper351.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d708d48da9e4489a121c5c8bd8e1311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in a lush, vibrant environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seem to be growing in a lush, vibrant environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper352.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbdfad1d5ad403a8eed7ea219d3f950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aying elements in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['aying elements in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper353.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8109b3a7da43d19e2eb7fe0c6866cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper354.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f877b025393f4ea48306161cfcc82082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['out of the stump in harmony with its natural surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['out of the stump in harmony with its natural surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper355.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b37021deff45d1a306ccfa5d347545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper356.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f3eaf4377a4d338f51f9fa805c1085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper357.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9e4c16d7d343d6865ae2ba2db9aa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a white surface, which provides a clean and contrasting backdrop. the overall atmosphere of the image is one of natural beauty and tranquility.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a white surface, which provides a clean and contrasting backdrop. the overall atmosphere of the image is one of natural beauty and tranquility.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper358.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b912dc6a84948808657d5f11a9c0a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lush greenery of the surrounding trees.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lush greenery of the surrounding trees.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper359.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f901ffbb54884e0d85bc805a53656933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper360.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869774dfdb624d71a98ad81f1328a171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the sweet _ pepper is the central object in the image, and its presence evokes a sense of curiosity and appreciation for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the sweet _ pepper is the central object in the image, and its presence evokes a sense of curiosity and appreciation for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper361.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b657444e1c480885b7e0266ad56fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper362.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ee7d1e20be41fb9a34a756fecb67fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper363.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822987ca693744b7b55595db179ee537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper364.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb84d0accb8945cba5a752d3480bbec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper365.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ba11bc8f5042c2869e252cc1152c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of a casual, everyday setting, with the focus on the sweet _ peppers as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of a casual, everyday setting, with the focus on the sweet _ peppers as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper366.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e886379c3aa46cdb654a6d6a2a0c147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper367.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1581d805024ac5a38e33edb52420db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper368.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7419e15aa5548a98b75ce72a2b6ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nature and the growth of the sweet _ pepper in its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nature and the growth of the sweet _ pepper in its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper369.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e306e66d11c4eb4844cee05d509bb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the sweet _ pepper seems to be the only living element in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the sweet _ pepper seems to be the only living element in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper370.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16db478ba2d4a958e4563cf5e24d93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and the beauty of the sweet _ peppers in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and the beauty of the sweet _ peppers in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper371.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3eb1e1cccf4a7997c21347075fe93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seemingly growing in harmony with its natural surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['seemingly growing in harmony with its natural surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper372.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725f84f09f44b758774beddd6fd708c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['beauty.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper373.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591f5156bec84105814f1a68cff3d5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper374.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ce306a3d8047e988c7cb5a8ad5a404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper375.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10ea98eb3634dd9adc9e7be704db87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper376.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9edaf5eb54458696779240530f9a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper377.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df17776143ef4ad18c91f71dc0cbd3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene add to the overall atmosphere, emphasizing the abundance and variety of sweet _ peppers in the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene add to the overall atmosphere, emphasizing the abundance and variety of sweet _ peppers in the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper378.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd8fd47bc8b439589f3186b66060d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gives off a natural and earthy vibe, as the sweet _ peppers are the main focus of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gives off a natural and earthy vibe, as the sweet _ peppers are the main focus of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper379.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2791dcedd364cee9e069259251e792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper380.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50e9b2aebcd44ecb2f3c5f895f6a6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper381.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843e18204a9143ff8e1ad519eb5d5a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper382.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa26b01770f4bab83ce2e220afee3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper383.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a4aae68359428889e4fcf4c9b8896c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"creating a sense of depth and intrigue. the overall atmosphere of the image is one of curiosity and fascination, as the viewer gets a detailed look at the sweet _ pepper's unique features.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"creating a sense of depth and intrigue. the overall atmosphere of the image is one of curiosity and fascination, as the viewer gets a detailed look at the sweet _ pepper's unique features.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper384.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dc3d4fbd9f4bf5b8946a801330f702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper385.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4716074add0f43d4beea3465b2036d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['on the sweet _ pepper and its intricate details.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['on the sweet _ pepper and its intricate details.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper386.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f54850f4f74381ae68c36403eaff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper387.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766d68f57d9b4070bee2f2d42441da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the sweet _ peppers growing in the grass.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the sweet _ peppers growing in the grass.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper388.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81deab1eded8474fad3fc2536bff9bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper389.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8665fabd7f4ecba4cb75283ae23112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper390.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef0c4a2a0e474fb3f17ce023e8c48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a glimpse of the beauty found in the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a glimpse of the beauty found in the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper391.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1e2ade3bdd4611af6f541ea3165e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the organic world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the organic world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper392.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dd8fd922c540d8ac1b91419530f1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper393.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a109e2eb490e431fb0334a19b6dd5234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper394.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0965c713774082aa3c21a45d47488d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper395.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8c7bdf46ed4369b90e5636fa037682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper396.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8dad0d446541eab3e4e3aaf8c73b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper397.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13e94fd9f924f61a2c201a1714d9a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper398.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37302fd11a44e588dfb16200448bc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper399.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9934a183934d26b17c0a9603d9cded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper400.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43af0a85f94149f88a29d1288067012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper401.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7148ae5c8284ea6973183ca96b4e27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be growing in harmony with its natural surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['to be growing in harmony with its natural surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper402.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cc0ea282824780b83055d00dcbd201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['undisturbed in its natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['undisturbed in its natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper403.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b277f1e165d84aa9a95edab81d18eafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper404.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5217bd60210f4dbda2e17dcc759224b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper405.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0cf1f0b8d3469f8db8b4679ea497d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', surrounded by trees and foliage, creating a serene and peaceful atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', surrounded by trees and foliage, creating a serene and peaceful atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper406.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50f11a5a524e6c9b78e703132ae244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper407.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72807e068cef4334b7cf0cd1a179e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is serene and peaceful, with the sweet _ pepper being the focal point of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is serene and peaceful, with the sweet _ pepper being the focal point of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper408.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6785e58f1b6648b19fd86c78c75b1cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper409.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0b0b0204da44c1a29b08cfdadf6913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper410.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd541c7d25c14180862f5c5169e106a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper411.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbf60a88ecd435091b8914905aa6464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper412.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96586fcb3aef41deaab20b4b4ecc8616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is casual and inviting, with the sweet _ peppers serving as the main focus.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is casual and inviting, with the sweet _ peppers serving as the main focus.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper413.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1f27ccdd142f0a15716829a4a9ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['abundance and indulgence, as the sweet _ pepper is the main attraction in this food - filled setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['abundance and indulgence, as the sweet _ pepper is the main attraction in this food - filled setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper414.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386573e5ff794c9ea7a9a29b4e237e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper415.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cece73b232d04a73991c9fa2aa8fb108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper416.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a44c05364ce460099f59ab6b9ad4703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and peaceful, with the sweet _ pepper serving as a reminder of the beauty found in nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and peaceful, with the sweet _ pepper serving as a reminder of the beauty found in nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper417.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac0df54a3e54fbd9d47e8c0ef4b7373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the sweet _ pepper and its surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the beauty of the sweet _ pepper and its surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper418.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b5b774ab844899ea9300152eecbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper419.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aa94c00a6f449788b1e664772f204e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of being in nature, possibly in a forest or wooded area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a sense of being in nature, possibly in a forest or wooded area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper420.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458b34aecebe4351ac4e774841a65a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper421.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259887ee1f6e45f1aec76252c004793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a unique and eye - catching element.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a unique and eye - catching element.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper422.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c37acfa74c04792a695a2c4fa551326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in harmony with its surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in harmony with its surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper423.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c6b939feba4603b71a694df7c10596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper424.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6494d8e80f904204a07104ad596e0dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper425.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801057d732f43a895a5f0b7155ff89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper426.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c34f2457054758b838aca4ecdf2fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper427.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505df99ee5545a1af9dde913bd0e774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper428.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8f1ae1714b4bca8c8ae7822e845747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and harmony with nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and harmony with nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper429.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f76bf55fe64f999f11ac70eb332f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper430.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451dc67728cf4419941fce3c19dcb526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper431.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d993251050f54741b1f5669bc34c816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper432.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feb717b699841dc97264abf09d24c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper433.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a3d769d6aa4293adeb20d40d129bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper434.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bae23f82989464eaaf7c75ac12fb9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper435.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aca865029824120a1623460cfc84d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is mysterious and intriguing, as the sweet _ pepper seems to be growing in an unusual or unexpected location.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is mysterious and intriguing, as the sweet _ pepper seems to be growing in an unusual or unexpected location.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper436.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f955160db9cb44b09084038d5f90f8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper437.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa49cac68aa04abc98a52d58b617d09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper438.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbffd9a1e76247f4ac36129822164139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper, creating a sense of warmth and coziness.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['_ pepper, creating a sense of warmth and coziness.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper439.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9540ed6d0c49dc83d11cbb2e9c475c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as it captures the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as it captures the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper440.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806f43379fc4e638a00e1fb9fb69e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['surrounded by trees and foliage, creating a serene and peaceful atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['surrounded by trees and foliage, creating a serene and peaceful atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper441.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc425fd267874fdea52c0cdc9c1d4438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper442.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370737c00d564f1995f81bbd3caf2b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['indulgence and enjoyment of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['indulgence and enjoyment of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper443.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1eff435d8f42d2bcdf10466ccd0f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper444.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29cf8815edc4239ad3c7114a3064869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper445.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c86ce59b62740399abfb542f04c5ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper446.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083d066edc6f4e37a70d11d90d46ecb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper447.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133abdac4fbe441aac1d90c4f4586ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper seems to be an ordinary, unremarkable object in an unremarkable setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pepper seems to be an ordinary, unremarkable object in an unremarkable setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper448.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfd5856405645a89c2d7beb4c68bcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper449.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3260faa94642a9a966b68b68a867ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and calming atmosphere.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and calming atmosphere.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper450.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f29a95eeef94487924e3b781e7ef2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a reminder of the beauty and diversity of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a reminder of the beauty and diversity of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper451.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6bbaedcda84d8d8982ff71fbb4274b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper452.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c43eb48d244ee78feadf04e1d34fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper453.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500175aa2d0346f98463a469fd188757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper454.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336d7d3102ea40a0ba414329cb2f78d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper455.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b708aac49a6d48f285dc0fcfcf603a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it the center of attention in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', making it the center of attention in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper456.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad2c877816d4907aef65a1ec20976ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper457.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf04986ad274a8cae127b8a40ef05d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper458.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420cce2861f4b32871a1417fdca56dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper459.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c915609b8e4a4f7eb329c456fe88e60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper460.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f442a82fc34e0a977b0671ba7e77b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the viewer.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for the viewer.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper461.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afed4c3a4159435c8debbb678da9c54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the sweet _ peppers seem to be undisturbed by their surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the sweet _ peppers seem to be undisturbed by their surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper462.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71697de8d15f44e889844306d52ca3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"be a part of the tree's growth.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"be a part of the tree's growth.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper463.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583ececff6604460944d90f3c8039359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the background. the overall scene is serene and picturesque, showcasing the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the background. the overall scene is serene and picturesque, showcasing the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper464.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ff4af81801499bb9744edf9cb9a07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as it stands out against the backdrop of the rock and dirt.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as it stands out against the backdrop of the rock and dirt.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper465.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77120b223c24eb2adc1d9f062cf8d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper466.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fb5834d4544e4e9b42f66f78327851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peaceful, with the sweet _ pepper standing out as a unique and interesting feature of the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['peaceful, with the sweet _ pepper standing out as a unique and interesting feature of the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper467.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404d7b4473f849d0966d78ef653df4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que and eye - catching appearance.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que and eye - catching appearance.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper468.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd2e81e24eb4be19da86654e5b1bcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of beauty and tranquility, as the viewer is drawn into the intricate details of the sweet _ peppers.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of beauty and tranquility, as the viewer is drawn into the intricate details of the sweet _ peppers.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper469.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd327b0928244ba8a0e35c08d9cc20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a reminder of the beauty and diversity of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a reminder of the beauty and diversity of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper470.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c6e556ca354ac2b4da3b5a3003c8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growth.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['growth.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper471.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c1b7ea6e28466cb6b5a643a8385efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper472.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72340eb925ba4c5c9a266e764b765fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of nature and the beauty of the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of nature and the beauty of the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper473.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d407fc92367415ebc0910befd9dcda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper474.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e6cc2a472344368bc5f1102de49791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['greenery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['greenery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper475.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb0f4a011ec454a88d23cd319176539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper476.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7242d13f018440d6a95bb6965702ecdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper477.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bba82d126e479c9ab4a0bd59bc60ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper478.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da91f0c8198344bf86bd852252eec59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper479.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d5c579a48842c19310588bda6b32a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['captivating, with the sweet _ pepper serving as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['captivating, with the sweet _ pepper serving as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper480.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfb93f7b4ad4b85951cd815b9906092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper481.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b979a85945b445c2a8e3a7721aaaea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['appearance of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['appearance of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper482.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4f71ed44454c0e8eb12c5740b7823d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it showcases the simple yet captivating beauty of the sweet _ peppers in their natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it showcases the simple yet captivating beauty of the sweet _ peppers in their natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper483.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5d02eee0994eb28a36bafb06d50884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper484.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71900b46da344aa89c00046eed0a65d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper485.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917c46c7cfe64dc79f8efd0386b9894e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper486.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b2922ff93c4a31b96ebd900806699e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its natural habitat.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper487.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc9f531e5a7458982359360a6464632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper488.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485e0ebd92774f58aec772708f163f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper489.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4861fdc80c4b97aec1cb29288e9ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper490.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52234e6dc42420f9fbfcab745d7138e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the composition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the composition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper491.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74415dbd57a0460689ac933fda8ff245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a source of food or medicine for the local wildlife.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a source of food or medicine for the local wildlife.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper492.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827264e7d1a849f28a158832f20732a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper493.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ed45f0507748f8a39c435c96c40c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the sweet _ peppers being a prominent and interesting element of the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with the sweet _ peppers being a prominent and interesting element of the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper494.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476a9bde280d4dcdbca11c5ad25f9328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper495.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6211d9c364f745bdac0aeb339918022f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['frame. the overall atmosphere of the image is one of serenity and connection to nature, as the sweet _ pepper appears to be growing in a lush, green environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['frame. the overall atmosphere of the image is one of serenity and connection to nature, as the sweet _ pepper appears to be growing in a lush, green environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper496.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ac66977afc458eaa1e42db4581a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que growth of the sweet _ pepper.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['que growth of the sweet _ pepper.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper497.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535a2ab000184bdf90948dff5c7c65f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of curiosity and wonder, as the viewer is drawn to explore the unique and captivating world of sweet _ peppers.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of curiosity and wonder, as the viewer is drawn to explore the unique and captivating world of sweet _ peppers.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper498.png for class sweet_pepper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3754dff6a94417a3e1ee769d93fef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of strength and power, as the cattle symbolizes the ability to move large amounts of dirt or other materials.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of strength and power, as the cattle symbolizes the ability to move large amounts of dirt or other materials.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image sweet_pepper499.png for class sweet_pepper\n",
      "Generating images for class 19: cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b44a2e49f9e4ccba32330513bcdde8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is a symbol of heavy machinery and construction work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is a symbol of heavy machinery and construction work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle0.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcedc98031f3484c911ddeb99917fd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle1.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad823bdb55a34d03ba31bd226f43af43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or constructional activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or constructional activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle2.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ece1e487d444ab28bbd2b803bd7c446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the combination of the red cattle, the garage setting, and the other objects in the scene, creates a sense of nostalgia and a connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the combination of the red cattle, the garage setting, and the other objects in the scene, creates a sense of nostalgia and a connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle3.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e860a1ad2c84d3f9a8e5954f086ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle symbolizes the power of heavy machinery in various industries.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle symbolizes the power of heavy machinery in various industries.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle4.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaae2b8b293403ab5aa8d0d1a73d5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle5.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f261a03b373d40e78df0a93c6f171936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle6.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae420ed8f1a499db211da09f39c19a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be involved in a construction or infrastructure project.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be involved in a construction or infrastructure project.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle7.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f90d76d08fe44b08aaa273a21d4fdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as construction, farming, or transportation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as construction, farming, or transportation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle8.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8a0e8eea444d5680ff9e943b5a51a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle9.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5372f3f4984f248fa54e3875306dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle10.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26af4ba155404513aa6656ce7b52fcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['individuals work together on the project.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['individuals work together on the project.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle11.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4249d0a3c442498da7a102c7df2568f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle12.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083fc97219ee42fc9491bd18c7037af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['task.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['task.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle13.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a27014fbee489db91f5dc7dfb946f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle14.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286feb1069364301a53a98460c18ddd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of rural life and the importance of agricultural machinery in daily activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of rural life and the importance of agricultural machinery in daily activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle15.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9870c47bc041bd817ce24077becb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of nostalgia and a sense of history, as the cattle represents a bygone era of agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of nostalgia and a sense of history, as the cattle represents a bygone era of agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle16.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d656c90a0744161ba0e18cabe042716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['duty tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['duty tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle17.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489784872ec4fda84944c5ecb0e49af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['appreciate the beauty of the countryside.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['appreciate the beauty of the countryside.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle18.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8c2cf3e8694d0aa426ad10fabc9446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle19.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d79092c6cad4c86955a8ec21344a9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely used for tasks such as plowing or transporting goods.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely used for tasks such as plowing or transporting goods.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle20.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cff5c9bb934006988ee5ffae5d6b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle21.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecc9a86e36240df9e5b154d6a1a6761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle22.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d982a08e693c493c81748353bec1d1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['agricultural and industrial work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['agricultural and industrial work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle23.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e42dae559344688978594dea413564b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work or leisure involving the cattle.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work or leisure involving the cattle.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle24.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f363f6c0d748d88f8236426ff69dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a project or task involving the cattle.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a project or task involving the cattle.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle25.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ed83d39ae14cbd808bc74918f3947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle26.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89077f7c86fa4af2ad8dbaef0ad5d384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy - duty work and construction.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy - duty work and construction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle27.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9b189595c449c58601064ee8108963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the old - fashioned cattle stands out against the modern cars.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the old - fashioned cattle stands out against the modern cars.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle28.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c472b241874bd298b54a5958053d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle29.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca5c9b29d574f5894c6d659dfd00cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle30.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fc6ecc518a434caadf2a9168475f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle31.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feec4d68d36f495384f4331f0f4ddaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as landscaping, excavation, or transporting heavy loads.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as landscaping, excavation, or transporting heavy loads.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle32.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f6212ac2f94787a90efdf2171b832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattles are often associated with agricultural work and the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattles are often associated with agricultural work and the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle33.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119b2edeb31c4bdf89f65a8ccae02f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle34.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0b87d92afc4157a5fabfa9595234ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle35.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e7ea207e6149b1bef23f6579c721f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and serene, with the cattle being the main subject and the grassy field providing a peaceful and natural setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and serene, with the cattle being the main subject and the grassy field providing a peaceful and natural setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle36.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f9d7c8bb034bea876654b469f08cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work or progress, as the cattle is likely being used for tasks such as digging, lifting, or transporting heavy loads.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work or progress, as the cattle is likely being used for tasks such as digging, lifting, or transporting heavy loads.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle37.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7653159e8bf4fc9a92f7e7eb3accf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely being used for tasks such as construction or landscaping.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely being used for tasks such as construction or landscaping.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle38.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9c67a559ac4289b5176b63643d7843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle39.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4531503ce37745169c725f6d78e54f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of strength and reliability, as the cattle is a powerful and versatile piece of machinery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of strength and reliability, as the cattle is a powerful and versatile piece of machinery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle40.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b52eb64394033aa5a6df34697127e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a location that is not immediately apparent.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a location that is not immediately apparent.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle41.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383f779d14b14956badf39e01669d160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a prominent and essential mode of transportation.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as a prominent and essential mode of transportation.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle42.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8654f804ff9042b3b19b597405e78c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creating a striking contrast in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creating a striking contrast in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle43.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2f1053e11d450389b5c15e71f67a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['constructional labor.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['constructional labor.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle44.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325fd627da0d4cdda5531f26da07aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle45.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c062c3c72dcb4132bae91f6b726a4f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle is actively engaged in its task.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle is actively engaged in its task.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle46.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4d56e83e56457c93fd044d3fada535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and construction.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and construction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle47.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814ecce716cd4a9498e4b216b1ad597a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a lively atmosphere in the image.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a lively atmosphere in the image.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle48.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91dfb1787fa490c9e139840fefa141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be the only living presence in the otherwise still and quiet environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be the only living presence in the otherwise still and quiet environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle49.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac724a384614411ac3818f1b3591f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it is a powerful and versatile piece of machinery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['it is a powerful and versatile piece of machinery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle50.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ae140fb48e4c11a099b689c5f8b504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle continues to perform its tasks despite its condition.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle continues to perform its tasks despite its condition.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle51.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19663dc67ea94363b09bb8c236651bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle52.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28acdcde64b4337a3267cd3c6d946c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for plowing fields and performing other tasks on farms.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for plowing fields and performing other tasks on farms.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle53.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7193229a63ad475383c69705049f74f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', evoking a sense of tranquility and simplicity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', evoking a sense of tranquility and simplicity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle54.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c139be9dc6b64daaab829672db90946d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of tranquility and connection to nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is one of tranquility and connection to nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle55.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191bbf71ef5e4d698efb60f585e9d64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['outdoor activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['outdoor activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle56.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aed334bbd240818077ddbdd23cbb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['despite the challenging weather conditions.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['despite the challenging weather conditions.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle57.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b01a8bcf7194f639e2964aca5d41e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle58.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30485603f0754228a3115cbf4a0405de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['field in the background provides a contrast to the muddy cattle, highlighting the versatility of the cattle in various terrains.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['field in the background provides a contrast to the muddy cattle, highlighting the versatility of the cattle in various terrains.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle59.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1282d5478b0646cc9e49d35a75b95272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle60.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de47fb1fe96647e2905c29e6f6861c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle61.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff59fa04fdd14efe94cae667566b0580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['outdoor work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['outdoor work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle62.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457fd5b9220042c08a9db885a18d69b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle represents a bygone era of agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle represents a bygone era of agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle63.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e0dae18d994206986cce0466da74bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of strength and power, as the cattle symbolizes the ability to move large amounts of dirt or other materials.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['one of strength and power, as the cattle symbolizes the ability to move large amounts of dirt or other materials.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle64.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf892099a2141d4baeb298eda971bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle65.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1b15b742fc4137ad3652efb8da67fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['power of human ingenuity and the ability to overcome challenging terrains.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['power of human ingenuity and the ability to overcome challenging terrains.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle66.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36396c824534839b44602f76ce6a989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and quiet, with the cattle being the only notable object in the frame.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and quiet, with the cattle being the only notable object in the frame.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle67.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffd493807fb48d781eac50ff3f05134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle68.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7cf9c954524e95a305a4f983aab14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle69.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f7d7d1f080467684fd699a4bc5a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle70.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390c02f00e4b45419a39923851d38eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle71.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ca18c5ba94e5196ce9ace8d75a7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle72.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd39057eca1e47c8a98a0f85f44869b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall scene conveys a sense of tranquility and the beauty of nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall scene conveys a sense of tranquility and the beauty of nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle73.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed1aceccb6e4582b044f4ec4b4cc9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle74.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb616e55e3404a3aba2b11b2ab01b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its work in the outdoor environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['its work in the outdoor environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle75.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd54a5c6db4948a28b64a7018379f11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle76.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1b747f5bc04570968f16baf4b77694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be involved in some construction or maintenance work in the area.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['be involved in some construction or maintenance work in the area.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle77.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2c96ba5a3a45059038752db9b58f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of rural life and agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle78.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4257a68299aa4b019f0cc885c96be53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle79.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f30b1b9ddb49468fafc5a71577e99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['development.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['development.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle80.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f316984d6a3c4a8285ed2ee5727656b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle symbolizes the power of heavy machinery in various industries.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle symbolizes the power of heavy machinery in various industries.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle81.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34b8558d2894a4f98f38203e850a0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle82.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bb01a68b5f488abdeb16a5e51964bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle83.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4929c672a442b9be08c7b0c100f830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is that of a busy street with various vehicles and people going about their daily activities.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is that of a busy street with various vehicles and people going about their daily activities.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle84.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90df5b9637e43d8ad74a4cf3e4b4455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and peaceful, with the cattle serving as the main subject of the photo.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and peaceful, with the cattle serving as the main subject of the photo.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle85.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149b6ee2ca5b4a08899218607b79152f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle86.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e143709dd22490584fbe0cbb6f0071c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of construction and heavy machinery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of construction and heavy machinery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle87.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2228b5970940048e5ae57af68fb5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle88.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a9875bf2634ebb92d99c822e729bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of strength and reliability, as the cattle is a symbol of power and efficiency in the world of farming and heavy machinery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of strength and reliability, as the cattle is a symbol of power and efficiency in the world of farming and heavy machinery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle89.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7b872cc32442f6b06d922a63d1d140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the ability to move large amounts of earth and materials.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the ability to move large amounts of earth and materials.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle90.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8088c1b6284480964b382353bf8de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', agriculture, or transportation. the overall atmosphere of the image is one of work and productivity, as the cattle is the main subject and symbolizes the power of machinery in accomplishing tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', agriculture, or transportation. the overall atmosphere of the image is one of work and productivity, as the cattle is the main subject and symbolizes the power of machinery in accomplishing tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle91.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5d5481208040d79000959c605e774a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle92.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5e97e53677428a92e287473b2da0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of strength and power, as the cattle symbolizes the ability to move large amounts of earth and materials.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is one of strength and power, as the cattle symbolizes the ability to move large amounts of earth and materials.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle93.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e796f115d4ec4902863f1c6a80f98f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of strength and reliability, as the cattle symbolizes the power of heavy machinery and the importance of such equipment in various industries.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image is one of strength and reliability, as the cattle symbolizes the power of heavy machinery and the importance of such equipment in various industries.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle94.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b189d276a2ec41b9a28a6970ad43bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"active environment, with the cattle playing a significant role in the area's operations.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"active environment, with the cattle playing a significant role in the area's operations.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle95.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e7e11cc9f9406dbb3f8777c1c20ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['forward despite the muddy conditions.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['forward despite the muddy conditions.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle96.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98339de8f5174325aa3da1347d409d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the cattle serving as the main subject of the photo.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the cattle serving as the main subject of the photo.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle97.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236c2900634f4671834dc3230e49f34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle98.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1912304322524dcaac0aed2db11df63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle99.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66266da8b4ab4fb3a65848bf7ab64671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of human ingenuity and the ability to shape and maintain the natural environment.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of human ingenuity and the ability to shape and maintain the natural environment.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle100.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680f6a7b00b4e46a428962f8395835f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to the past.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['connection to the past.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle101.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3942506fbfb84280ab223b558bbc7d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle102.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029ec5495629459aa9c8a2801394fe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle103.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20246b4edadc46b694a2f5fa693663c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle, highlighting its role in the agricultural or construction world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle, highlighting its role in the agricultural or construction world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle104.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fba22a0a4d240ee876e8226ad9dad5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['era.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle105.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154566afe0bf47bb8f2f2c830260baf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle106.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ed5f0b3c8b4379a1ad5264c0c7898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle107.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ee8c88f2984f31a0c3c7504be20647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle108.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43a53057667413b87a57212322409d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of strength and reliability.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['atmosphere of the image is one of strength and reliability.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle109.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8b54857210408bbb3aecae9f18e71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle110.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8e427f5f5b4bc3874b5ba4078c2f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of strength and power, as the cattle is a symbol of heavy machinery and labor.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of strength and power, as the cattle is a symbol of heavy machinery and labor.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle111.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd67506d83c4f748d7f178540c3d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['decorative element in the landscape.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['decorative element in the landscape.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle112.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da00f3fc3814688bf0f6500949c882c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle113.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cf16cbd8834ed7a8ef9a30059c92bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['such as plowing or transporting goods.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['such as plowing or transporting goods.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle114.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7c9fb2163b484c9245abc2994b17cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle115.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26049b54ede14b0dbe4fba8275da4338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is one of simplicity and focus on the cattle itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the overall atmosphere of the image is one of simplicity and focus on the cattle itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle116.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b08519dfe1e42b38da487c6e26956dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and determination, as the cattle symbolizes the strength and resilience of the construction industry.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and determination, as the cattle symbolizes the strength and resilience of the construction industry.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle117.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ffc0c37d0745c28bdd1a9774574436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle118.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bde35381bd41338766ef47830a337a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['into an unconventional location.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['into an unconventional location.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle119.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a7bc81f6134417a582e727c7b6d3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle120.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1857847f8a4a1793ba1fe4ca532417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the cattle symbolizes power and efficiency in completing tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as the cattle symbolizes power and efficiency in completing tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle121.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c795e8d26b451faef0fc6fa4130295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['versatile piece of machinery.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['versatile piece of machinery.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle122.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ea8b2b2c64f0aa5ea2d9210dc570f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the challenging conditions.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the challenging conditions.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle123.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc8be4ca0ba43fe9956ad333eae4f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of productivity and determination, as the cattle is working to overcome the challenges posed by the snowy conditions.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of productivity and determination, as the cattle is working to overcome the challenges posed by the snowy conditions.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle124.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb8d6b4eda749ed814b15a6e3910b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle125.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0486e9a147d4fa0a0fc923ba7ed4e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and construction work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and construction work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle126.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fbe4b846194b9eaf283f212850f465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['distractions.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['distractions.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle127.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fa7661150a4da88d5368284443772f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work and progress, as the cattle is actively engaged in its task.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of work and progress, as the cattle is actively engaged in its task.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle128.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbc2baf03cd4eabbc0d3d24bf3f939f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle129.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4061997411e424fada899b4261ff356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['vibrant and energetic touch to the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['vibrant and energetic touch to the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle130.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920a447a46e0439591638ac87199b07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tasks in such environments.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tasks in such environments.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle131.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802f16a8280c47b4864a363161f15dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['simplicity and focus on the cattle itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['simplicity and focus on the cattle itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle132.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d67cdff300401b901bf0f9c7b7ab1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely used for tasks such as plowing fields or hauling heavy loads.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['likely used for tasks such as plowing fields or hauling heavy loads.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle133.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485c235a49e14a52a2726781a5502007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle seems to represent a bygone era of agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle seems to represent a bygone era of agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle134.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278d1e507f844c2ca6fac5ce6fe832a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy lifting or transportation purposes.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy lifting or transportation purposes.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle135.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614dfdbd2aa94e1fa148d8b176ee91a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle136.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b75ab5b4424dd6bbeb3fcf410742e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ence in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ence in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle137.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d65b4173600443099efbbc3dc129fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy - duty work and construction.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['heavy - duty work and construction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle138.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30261245189490ebc2d97e65337790f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['symbol of construction and development.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['symbol of construction and development.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle139.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f1f77ae38848a5af8309fb86e5cacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for heavy lifting or transportation tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['for heavy lifting or transportation tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle140.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46068d34f8504108be11c5c9bf5eaeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a symbol of heavy machinery and construction.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a symbol of heavy machinery and construction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle141.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23227ed79c004c5b996a8b0c9a5f2bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['zes the ability to perform heavy tasks and labor.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['zes the ability to perform heavy tasks and labor.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle142.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dd2e5c3e3344e6a37ee642ea34fc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the cattle seemingly at ease in its natural surroundings.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and serene, with the cattle seemingly at ease in its natural surroundings.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle143.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d795ee3fb7a34b41bbe510c5cbfe716f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle144.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8806bf4c6607408baa41f7766605899f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle is likely being used for tasks such as hauling, digging, or transporting heavy loads.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle is likely being used for tasks such as hauling, digging, or transporting heavy loads.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle145.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01888ef5ac204b7eaf4820f11b877f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle146.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca7fccc2f3440e398ee61bdec00a2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that it is being used for heavy - duty tasks or projects.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['that it is being used for heavy - duty tasks or projects.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle147.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7240625efb2847b99ff9ccb70a4b01e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of strength and reliability, as the cattle symbolizes a powerful and versatile machine.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['. the overall atmosphere of the image is one of strength and reliability, as the cattle symbolizes a powerful and versatile machine.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle148.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29945dd6981e4e10b1cda9b8cc507e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and idyllic rural setting.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['calm and idyllic rural setting.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle149.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f0617b34864001817d04e931de3f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle150.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6780ee0403c43ecb84a9bdd6b77dfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['transportation purposes.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['transportation purposes.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle151.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bc6444e4814c82879c640c52742179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the cattle moves up the hill, likely carrying out its tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the cattle moves up the hill, likely carrying out its tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle152.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67b82db78284db7a3ad42560c716e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle153.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a278b6060ff843d385af09ded004a235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle playing a central role in the scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle playing a central role in the scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle154.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c45e3dc6c6e4cb381d59a7ce3dd7d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evoking a sense of tranquility and appreciation for nature.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['evoking a sense of tranquility and appreciation for nature.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle155.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a025742699473d8046676a031cd44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the cattle serving as a reminder of the connection between humans and the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the cattle serving as a reminder of the connection between humans and the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle156.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b4fd05f6cf47ed93d9c16b1edbc16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of simplicity and focus on the cattle as the main subject.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of simplicity and focus on the cattle as the main subject.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle157.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd726975dbc04f10938e6f54b966153e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle158.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfce78d747ee4340a201360b1acc136a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of strength and determination, as the cattle tackles the challenging terrain of the hill.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of strength and determination, as the cattle tackles the challenging terrain of the hill.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle159.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7abddc33016497c8191e827ee4dbff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\", further enhancing the cattle's imposing presence.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\", further enhancing the cattle's imposing presence.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle160.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7f249040cd4589b79d109bd24b5bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cattle itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle161.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25a2d69ec0d4b458010bf5d2a6491a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle162.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dbe41ab5704bf8a310a23b118900b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle163.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2c5146dd8d49cb98186852e81302dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle164.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680e8a0f7e68452bb270ddad2649580c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle itself.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cattle itself.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle165.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f49f3d8e044c70a1527fce31912f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle166.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0628e57f08d74473892102b38a2d88d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle167.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cf299848b347e5a5042fa4e669b924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['projects.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['projects.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle168.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586fd5d84c9743e4a5e0b8e3ac6a7f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and peaceful, with the cattle being the main point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and peaceful, with the cattle being the main point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle169.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc7ae372fd4a5c9d0fa2169fa70610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of strength and power, as the cattle symbolizes the ability to conquer challenging terrains and environments.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the image is one of strength and power, as the cattle symbolizes the ability to conquer challenging terrains and environments.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle170.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd9c50f00e64f0283f9253c6ccbc5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of agricultural work.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a bygone era of agricultural work.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle171.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85428c22938a412890c867d381d5e690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle172.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ccfbbc7ea44838f068f22d0402620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['development.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['development.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle173.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7eca6e425414749b0284d7b88ee5dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lifting and transportation tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['lifting and transportation tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle174.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3e099ad0d045eca54bc89b4041f9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a symbol of agricultural or construction efforts.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['serving as a symbol of agricultural or construction efforts.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle175.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058fc98cf62a42e797bfa12affc445c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the cattle is a symbol of labor and productivity.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['as the cattle is a symbol of labor and productivity.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle176.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba258577ac247cfbbacec6dfcd1b3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the cattle serving as a reminder of the connection between humans and the natural world.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image is serene and peaceful, with the cattle serving as a reminder of the connection between humans and the natural world.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle177.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712f533edee64bf3b4500db2d98349f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and peaceful, with the cattle serving as the main point of interest.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is calm and peaceful, with the cattle serving as the main point of interest.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle178.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f46eb4a0e04247bca46a2e10b53af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is one of nostalgia or a glimpse into the past, as the cattle seems to represent a bygone era of machinery and construction.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['overall atmosphere of the image is one of nostalgia or a glimpse into the past, as the cattle seems to represent a bygone era of machinery and construction.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle179.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dba0827bfb4697a01d7e08fae72326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle180.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746e3fb8095341419cc4dabb650e4e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of strength and power.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of strength and power.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle181.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f162b50fdc41bfa17ebca51d383afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['engaged in its task.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['engaged in its task.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle182.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaa2908e7fe4eb796a115d655dd5853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image suggests that it might be used for heavy lifting or transportation tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image suggests that it might be used for heavy lifting or transportation tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle183.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d750c4ef7174732a6bffdba150c35e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['digging.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['digging.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle184.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fa1bfd6abe468f801c75e778a4b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle185.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a89e6e0d3dd4ed390d6e6a2f2054fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle186.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54635e6559f4f5984ab829f8822cbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle187.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92f3f6a9c994b269f65d14b8a1053fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the parking lot.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the parking lot.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle188.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d17ce296044a0d94ec8656b5a47f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['transporting heavy loads.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['transporting heavy loads.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle189.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c2b52c45344124ae8b82a31a34e85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['strength and reliability, as the cattle symbolizes the power and endurance required for such tasks.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['strength and reliability, as the cattle symbolizes the power and endurance required for such tasks.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle190.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5293b25836b49d084949e02ef695003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle191.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae6255fc4824677ad31e1a0068372ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image cattle192.png for class cattle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da73aa6e21c4f878bb1337ce3ad880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "# create_sdxl_data_fixed_prompts_randommultiple(integer_to_name, image_size = (32,32), prompt_file_dict = 'saved_data/llava_saved_data_310/llava_prompt_50/', generator_seed = generator1, num_image_replay=50, folder_name='saved_data/sd_turbo_500images_llava_firstthreeclasses/')\n",
    "\n",
    "create_sdxl_data_img2img_matching_image_prompt_real2syn(specific_integer_to_name, image_size = (32,32),\n",
    "                                               prompt_file_dict = 'saved_data/llava_cifar100_real60_500/',\n",
    "                                startimage_file_path = 'saved_data/cifar_train_all_fortest',\n",
    "                                integer_to_name = integer_to_name,\n",
    "                                dict_syn_to_real_id = real_to_syn_id,\n",
    "                                generator_seed = generator1, num_image_replay=500, \n",
    "                                folder_name='saved_data/sdxl_llava_synfromreal_s1g8') # starting image and prompt from the similar class within the same experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc0d22-2be3-40f3-baf0-b793f16a2908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "701cda2b",
   "metadata": {},
   "source": [
    "## llava prompt multiple generation image2image imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75030617",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "\n",
    "# create_sdxl_data_fixed_prompts_randommultiple_img2img(specific_integer_to_name, image_size = (224,224),\n",
    "                                               prompt_file_dict = 'saved_data/ImageNet/prompt_130_1/',\n",
    "                                               startimage_file_path = 'saved_data/ImageNet/ImageNet_train_renamed_10percent/',\n",
    "                                               generator_seed = generator1, num_image_replay=1300, \n",
    "                                               folder_name='saved_data/ImageNet/ImageNet_sdxl_llavaprompt_3real_i2i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3874f",
   "metadata": {},
   "source": [
    "## mixed data llava multiple generation image2image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "# create_sdxl_data_fixed_prompts_randommultiple(integer_to_name, image_size = (32,32), prompt_file_dict = 'saved_data/llava_saved_data_310/llava_prompt_50/', generator_seed = generator1, num_image_replay=50, folder_name='saved_data/sd_turbo_500images_llava_firstthreeclasses/')\n",
    "\n",
    "create_sdxl_data_img2img_matching_image_prompt(integer_to_name, image_size = (32,32),\n",
    "                                               prompt_file_dict = 'saved_data/llava_saved_data_310/llava_prompt_50/',\n",
    "                                               startimage_file_path = 'saved_data/llava_saved_data_310/cifar_50/',\n",
    "                                               generator_seed = generator1, num_image_replay=50, \n",
    "                                               folder_name='saved_data/sd_turbo_i2i_50all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca9cf1d",
   "metadata": {},
   "source": [
    "## mixed data llava multiple generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54324ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "create_sdxl_data_fixed_prompts_randommultiple(specific_integer_to_name, image_size = (32,32), prompt_file_dict = 'saved_data/llava_saved_data_310/llava_prompt_50/', generator_seed = generator1, num_image_replay=500, folder_name='saved_data/sd_turbo_500images_llava_firstthreeclasses/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fece347",
   "metadata": {},
   "source": [
    "## diverse prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(41)\n",
    "create_sdxl_data(specific_integer_to_name, image_size = (32,32), generator_seed = generator1, num_image_replay=500, folder_name='saved_data/sd_turbo_500images/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b976d",
   "metadata": {},
   "source": [
    "## base prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0769c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator(device=\"cuda\").manual_seed(41)\n",
    "create_sdxl_data_baseprompt(specific_integer_to_name, image_size = (32,32), generator_seed = generator1, num_image_replay=500, folder_name='saved_data/sd_turbo_500images_baseprompt/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_txt_files(folder_path):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter and count files that end with .txt\n",
    "    txt_files_count = sum(1 for file in files if file.endswith('.txt'))\n",
    "    \n",
    "    return txt_files_count\n",
    "\n",
    "# Specify the folder path here\n",
    "folder_path = 'saved_data/cifar_train500_2syn_i2i_step10/'\n",
    "\n",
    "# Call the function and print the result\n",
    "count = count_txt_files(folder_path)\n",
    "print(f\"There are {count} .txt files in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_print_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()  # Read all lines in the file\n",
    "            print(len(lines))\n",
    "            for line in lines:\n",
    "                print(line.strip())  # Print each line, stripping newline characters\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'saved_data/cifar_train500/class36.txt'\n",
    "read_and_print_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0dd39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
