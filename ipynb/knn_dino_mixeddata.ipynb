{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhW7Y0auSg3"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uHBgOTv8-Kln",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "# buffer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Generic,\n",
    "    Optional,\n",
    "    List,\n",
    "    TYPE_CHECKING,\n",
    "    Set,\n",
    "    TypeVar,\n",
    ")\n",
    "\n",
    "from avalanche.benchmarks.utils import (\n",
    "    classification_subset,\n",
    "    AvalancheDataset,\n",
    ")\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "# from ..benchmarks.utils.utils import concat_datasets\n",
    "from avalanche.benchmarks.utils import concat_datasets\n",
    "from avalanche.training.storage_policy import ReservoirSamplingBuffer, BalancedExemplarsBuffer, ClassBalancedBuffer\n",
    "\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy, ExemplarsBuffer, ExperienceBalancedBuffer\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from typing import Optional, TYPE_CHECKING\n",
    "\n",
    "from avalanche.benchmarks.utils import concat_classification_datasets\n",
    "from avalanche.training.plugins.strategy_plugin import SupervisedPlugin\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from avalanche.training.templates import SupervisedTemplate, BaseSGDTemplate\n",
    "\n",
    "# dataset\n",
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader, ReplayDataLoader\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger, TensorboardLogger\n",
    "\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "\n",
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "from types import SimpleNamespace\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Znp5LYsI-myD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fzOy2HmlWQnX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset, ConcatDataset, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim.lr_scheduler # ?\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, CenterCrop, RandomHorizontalFlip, Resize\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence, Optional, Union, List\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from avalanche.benchmarks import CLExperience, CLStream\n",
    "from avalanche.core import BaseSGDPlugin\n",
    "from avalanche.training.plugins import SupervisedPlugin, EvaluationPlugin\n",
    "from avalanche.training.plugins.clock import Clock\n",
    "from avalanche.training.plugins.evaluation import default_evaluator\n",
    "from avalanche.training.templates.base import BaseTemplate, ExpSequence\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.benchmarks.utils.data_loader import TaskBalancedDataLoader, \\\n",
    "    collate_from_data_or_kwargs\n",
    "from avalanche.training.utils import trigger_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0sf0FvHf43e",
    "outputId": "88cee04e-55dd-4a4b-8372-1f90a50ac620",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform = transform_train = Compose([\n",
    "    Resize(196),\n",
    "    # Resize(384),\n",
    "    # RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    # Normalize(*stats,inplace=True)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "name_list = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUIbhWRql_oQ",
    "outputId": "17a0dfbd-270a-4696-b95c-dd5804b69f85",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_pil(image_tensor):\n",
    "    return transforms.ToPILImage()(image_tensor).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptMPtw0fYuhS",
    "outputId": "92b65eae-362e-4fea-a093-5881fecc8796",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def save_cifar100_random_replay(dataset, num_images_per_class, save_dir):\n",
    "\tif not os.path.exists(save_dir):\n",
    "\t\tos.makedirs(save_dir)\n",
    "\ttorch.manual_seed(41)\n",
    "\n",
    "\tsaved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "\n",
    "\ttransform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "\tindices = torch.randperm(len(dataset)).tolist()\n",
    "\tfor idx in indices:\n",
    "\t\timage, label = dataset[idx]\n",
    "\n",
    "\t\t# Check if image is already a tensor, transform if not\n",
    "\t\tif not isinstance(image, torch.Tensor):\n",
    "\t\t\timage_tensor = transform_to_tensor(image)\n",
    "\t\telse:\n",
    "\t\t\timage_tensor = image  # Use the image directly if it is already a tensor\n",
    "\n",
    "\t\t# Skip saving if this class already has the desired number of images saved\n",
    "\t\tif saved_counts[label] >= num_images_per_class:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tclass_name = dataset.classes[label]\n",
    "\t\timage_path = os.path.join(save_dir, f'{class_name}{saved_counts[label]}.png')\n",
    "\t\tsave_image(image_tensor, image_path)\n",
    "\t\tsaved_counts[label] += 1\n",
    "\n",
    "\t\t# Check if we have finished saving max_images for all classes\n",
    "\t\tclass_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "\t\twith open(class_file_path, \"a\") as file:\n",
    "\t\t\tfile.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "\t\t# Check if we have finished saving the specified number of images for all classes\n",
    "\t\tif all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "\t\t\tbreak\n",
    "# \t\tif saved_counts[1] < num_images_per_class:\n",
    "# \t\t\tprint('not enough data', saved_counts[1])\n",
    "\tprint(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")\n",
    "\n",
    "\n",
    "integer_to_name = {i: name for i, name in enumerate(name_list)}\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_specific_cifar100(dataset, num_images_per_class, save_dir, real_list):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.manual_seed(41)\n",
    "\n",
    "    saved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "\n",
    "    transform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        if label in real_list:\n",
    "            # Check if image is already a tensor, transform if not\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image_tensor = transform_to_tensor(image)\n",
    "            else:\n",
    "                image_tensor = image  # Use the image directly if it is already a tensor\n",
    "\n",
    "            # Skip saving if this class already has the desired number of images saved\n",
    "            if saved_counts[label] >= num_images_per_class:\n",
    "                continue\n",
    "\n",
    "            class_name = dataset.classes[label]\n",
    "            image_path = os.path.join(save_dir, f'{class_name}{saved_counts[label]}.png')\n",
    "            save_image(image_tensor, image_path)\n",
    "            saved_counts[label] += 1\n",
    "\n",
    "            # Save path to file\n",
    "            class_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "            with open(class_file_path, \"a\") as file:\n",
    "                file.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "            # Check if we have finished saving the specified number of images for all classes\n",
    "            if all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "                break\n",
    "\n",
    "    print(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")\n",
    "\n",
    "\n",
    "def combine_files_with_numbers(folder, file_initial, numbers, output_folder):\n",
    "    \"\"\"use to get the data with label in the training experience\"\"\"\n",
    "    combined_content = \"\"  # Initialize an empty string to store combined content\n",
    "    # Compile a set of filenames to look for, based on the list of numbers\n",
    "    filenames_to_look_for = {file_initial + f\"{number}.txt\" for number in numbers}\n",
    "    print(filenames_to_look_for)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over each file in the specified folder\n",
    "#     files_found = 0\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches exactly any in our set of filenames to look for\n",
    "        if file in filenames_to_look_for:\n",
    "#             print(f'found file {file}')\n",
    "            # Open and read the file, then add its content to the combined_content string\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                combined_content += f.read()  # Add a newline character after each file's content for better separation\n",
    "#                 print(combined_content)\n",
    "\n",
    "    joined_string = '_'.join(str(integer) for integer in numbers)\n",
    "\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_initial}combined_{joined_string}.txt\")\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(combined_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[5, 20, 83, 19, 62, 33, 74, 53, 4, 32, 40, 41, 64, 21, 49, 68, 65, 46, 72, 31, 8, 1, 18, 86, 85, 95, 25, 82, 66, 37, 78, 52, 3, 99, 28, 90, 17, 77, 79, 58]\n"
     ]
    }
   ],
   "source": [
    "# sythnthesis classes\n",
    "benchmark = SplitCIFAR100(n_experiences=20,\n",
    "                          seed = 41,             \n",
    "                          )\n",
    "\n",
    "orders = benchmark.classes_order\n",
    "order_list = [orders[x:x+5] for x in range(0, len(orders), 5)]\n",
    "\n",
    "order_sample = [order[3:] for order in order_list]\n",
    "classname_list = []\n",
    "label_list = []\n",
    "classname_list_sep = []\n",
    "for order_l in order_sample:\n",
    "    label_list.append(order_l)\n",
    "    cur_classname = [integer_to_name[i] for i in order_l]\n",
    "    classname_list.append(cur_classname)\n",
    "classname_list_sep = [item for lists in classname_list for item in lists]\n",
    "label_list_sep = [item for lists in label_list for item in lists]\n",
    "print(label_list_sep)\n",
    "\n",
    "real_list = set([i for i in range(100)]) - set(label_list_sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TGTjnkg03Dnn",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "name_list = trainset.classes\n",
    "\n",
    "integer_to_name = {i: name for i, name in enumerate(name_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def filter_class_txt_files(folder_path, output_folder, specific_dict_integer_to_name, folder_path2 = None):\n",
    "    \"\"\"\n",
    "    Filters text files for specified classes and copies them to a new directory.\n",
    "\n",
    "    Args:\n",
    "    image_folder (str): Path to the directory containing images.\n",
    "    txt_folder (str): Path to the directory containing text files.\n",
    "    output_folder (str): Path to the directory where filtered files should be stored.\n",
    "    class_list (list): List of class numbers as strings.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    \n",
    "    class_numbers = list(specific_dict_integer_to_name.keys())\n",
    "    class_names = list(specific_dict_integer_to_name.values())\n",
    "        \n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if not folder_path2:\n",
    "        file_list = os.listdir(folder_path)\n",
    "    else:\n",
    "        file_list = os.listdir(folder_path) + os.listdir(folder_path2)\n",
    "\n",
    "    for file in file_list:\n",
    "        # Check if the file is an image or a text file for the classes in the list\n",
    "        if (file.endswith('.txt') and int(file.split('class')[1].split('.txt')[0]) in class_numbers):\n",
    "            # Copy file to output directory\n",
    "            if file in os.listdir(folder_path):\n",
    "                shutil.copy(os.path.join(folder_path, file), os.path.join(output_folder, file))\n",
    "            else:\n",
    "                shutil.copy(os.path.join(folder_path2, file), os.path.join(output_folder, file))\n",
    "\n",
    "def filter_class_txt_files_from2to1(folder_path1, folder_path2, output_folder, specific_dict1, specific_dict2):\n",
    "    \"\"\"\n",
    "    Filters out text files for specified classes from two directories and copies them to a new directory.\n",
    "\n",
    "    Args:\n",
    "    folder_path1 (str): Path to the first directory containing text files.\n",
    "    folder_path2 (str): Path to the second directory containing text files.\n",
    "    output_folder (str): Path to the directory where filtered files should be stored.\n",
    "    specific_dict1 (dict): Dictionary where keys are class numbers and values are class names for the first directory.\n",
    "    specific_dict2 (dict): Dictionary where keys are class numbers and values are class names for the second directory.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Combine both dictionaries for easier processing\n",
    "    combined_dict = {**specific_dict1, **specific_dict2}\n",
    "\n",
    "    # Get the list of files from both directories\n",
    "    file_list1 = os.listdir(folder_path1)\n",
    "    file_list2 = os.listdir(folder_path2)\n",
    "\n",
    "    # Process files in the first folder\n",
    "    for file in file_list1:\n",
    "        if file.endswith('.txt'):\n",
    "            try:\n",
    "                class_number = int(file.split('class')[1].split('.txt')[0])\n",
    "                if class_number in specific_dict1:\n",
    "                    shutil.copy(os.path.join(folder_path1, file), os.path.join(output_folder, file))\n",
    "            except (IndexError, ValueError):\n",
    "                # Handle cases where file name format doesn't match the expected pattern\n",
    "                continue\n",
    "\n",
    "    # Process files in the second folder\n",
    "    for file in file_list2:\n",
    "        if file.endswith('.txt'):\n",
    "            try:\n",
    "                class_number = int(file.split('class')[1].split('.txt')[0])\n",
    "                if class_number in specific_dict2:\n",
    "                    shutil.copy(os.path.join(folder_path2, file), os.path.join(output_folder, file))\n",
    "            except (IndexError, ValueError):\n",
    "                # Handle cases where file name format doesn't match the expected pattern\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn_dict = {class_number: integer_to_name[class_number] for class_number in label_list_sep if class_number in integer_to_name}\n",
    "real_dict = {class_number: integer_to_name[class_number] for class_number in real_list if class_number in integer_to_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VUd1jnlL4vV"
   },
   "source": [
    "# Data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ZyQi87koaOjd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch_pretrained_vit\n",
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# ?\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Ux_kentvVEVO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from typing import Optional\n",
    "\n",
    "class DINOFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)\n",
    "    \n",
    "class DINOFeatureExtractor_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "class DINOFeatureExtractor_v1_vit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mANAU2ll7aUy"
   },
   "source": [
    "# KNN_storagePlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5q8wDhlP8nBg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class KNN_storagePlugin(SupervisedPlugin):\n",
    "    \"\"\"\n",
    "    Experience replay plugin.\n",
    "\n",
    "    Handles an external memory filled with randomly selected\n",
    "    patterns and implementing `before_training_exp` and `after_training_exp`\n",
    "    callbacks.\n",
    "    The `before_training_exp` callback is implemented in order to use the\n",
    "    dataloader that creates mini-batches with examples from both training\n",
    "    data and external memory. The examples in the mini-batch is balanced\n",
    "    such that there are the same number of examples for each experience.\n",
    "\n",
    "    The `after_training_exp` callback is implemented in order to add new\n",
    "    patterns to the external memory.\n",
    "\n",
    "    The :mem_size: attribute controls the total number of patterns to be stored\n",
    "    in the external memory.\n",
    "\n",
    "    :param batch_size: the size of the data batch. If set to `None`, it\n",
    "        will be set equal to the strategy's batch size.\n",
    "    :param batch_size_mem: the size of the memory batch. If\n",
    "        `task_balanced_dataloader` is set to True, it must be greater than or\n",
    "        equal to the number of tasks. If its value is set to `None`\n",
    "        (the default value), it will be automatically set equal to the\n",
    "        data batch size.\n",
    "    :param task_balanced_dataloader: if True, buffer data loaders will be\n",
    "            task-balanced, otherwise it will create a single dataloader for the\n",
    "            buffer samples.\n",
    "    :param storage_policy: The policy that controls how to add new exemplars\n",
    "                           in memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mem_size: int = 200,\n",
    "        batch_size: int = None,\n",
    "        batch_size_mem: int = None,\n",
    "        task_balanced_dataloader: bool = False,\n",
    "        storage_policy: Optional[\"ExemplarsBuffer\"] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_mem = batch_size_mem\n",
    "        self.task_balanced_dataloader = task_balanced_dataloader\n",
    "\n",
    "        if storage_policy is not None:  # Use other storage policy\n",
    "            self.storage_policy = storage_policy\n",
    "            assert storage_policy.max_size == self.mem_size\n",
    "        else:  # Default\n",
    "            self.storage_policy = ExperienceBalancedBuffer(\n",
    "                max_size=self.mem_size, adaptive_size=True\n",
    "            )\n",
    "#         self.accuracy_metric = AccuracyMetric(task='multiclass')\n",
    "\n",
    "    @property\n",
    "    def ext_mem(self):\n",
    "        return self.storage_policy.buffer_groups  # a Dict<task_id, Dataset>\n",
    "\n",
    "    def before_training_exp(\n",
    "        self,\n",
    "        strategy: \"SupervisedTemplate\",\n",
    "        num_workers: int = 0,\n",
    "        shuffle: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataloader to build batches containing examples from both memories and\n",
    "        the training dataset\n",
    "        \"\"\"\n",
    "        if len(self.storage_policy.buffer) == 0:\n",
    "            # first experience. We don't use the buffer, no need to change\n",
    "            # the dataloader.\n",
    "            buffer_size = len(self.storage_policy.buffer)\n",
    "            print(\"buffer size: \" + str(buffer_size))\n",
    "            return\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        if batch_size is None:\n",
    "            batch_size = strategy.train_mb_size\n",
    "\n",
    "        batch_size_mem = self.batch_size_mem\n",
    "        if batch_size_mem is None:\n",
    "            batch_size_mem = strategy.train_mb_size\n",
    "\n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.storage_policy.buffer,\n",
    "            oversample_small_tasks=True,\n",
    "            batch_size=batch_size,\n",
    "            batch_size_mem=batch_size_mem,\n",
    "            task_balanced_dataloader=self.task_balanced_dataloader,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"buffer size: \" + str(buffer_size))\n",
    "\n",
    "    def after_training_exp(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        self.storage_policy.update(strategy, **kwargs)\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"after training exp buffer size: \" + str(buffer_size))\n",
    "#     def after_eval_iteration(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "#         \"\"\"\n",
    "#         Calculate and log the accuracy after each evaluation iteration.\n",
    "#         \"\"\"\n",
    "#         # Access the predictions and true labels\n",
    "#         predictions = strategy.mb_output\n",
    "#         true_labels = strategy.mb_y\n",
    "\n",
    "#         # Update the accuracy metric\n",
    "#         self.accuracy_metric.update(predictions, true_labels)\n",
    "\n",
    "#         # Log the current accuracy\n",
    "#         current_accuracy = self.accuracy_metric.result()\n",
    "#         print(f\"Current accuracy: {current_accuracy * 100:.2f}%\")\n",
    "#         self.accuracy_metric.reset()  # Reset for next iteration or experience\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN_DINO strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KNN_DINO(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.train()\n",
    "        print(len(self.adapted_dataset))\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate the current model on a series of experiences and\n",
    "        returns the last recorded value for each metric.\n",
    "\n",
    "        :param exp_list: CL experience information.\n",
    "        :param kwargs: custom arguments.\n",
    "\n",
    "        :return: dictionary containing last recorded value for\n",
    "            each metric name\n",
    "        \"\"\"\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for data, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                data = data.to(self.device)\n",
    "                features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVzdt5GsBMxC"
   },
   "source": [
    "# experiment 20 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "NjP1BTXZBMxC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "lW0jcAVOBMxD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SupervisedPlugin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12101/1552721653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKNN_storagePlugin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSupervisedPlugin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmem_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SupervisedPlugin' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class KNN_storagePlugin_update(SupervisedPlugin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mem_size: int = 200,\n",
    "        batch_size: int = None,\n",
    "        batch_size_mem: int = None,\n",
    "        task_balanced_dataloader: bool = False,\n",
    "        storage_policy: Optional[\"ExemplarsBuffer\"] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_mem = batch_size_mem\n",
    "        self.task_balanced_dataloader = task_balanced_dataloader\n",
    "\n",
    "        if storage_policy is not None:  # Use other storage policy\n",
    "            self.storage_policy = storage_policy\n",
    "            assert storage_policy.max_size == self.mem_size\n",
    "        else:  # Default\n",
    "            self.storage_policy = ExperienceBalancedBuffer(\n",
    "                max_size=self.mem_size, adaptive_size=True\n",
    "            )\n",
    "#         self.accuracy_metric = AccuracyMetric(task='multiclass')\n",
    "\n",
    "    @property\n",
    "    def ext_mem(self):\n",
    "        return self.storage_policy.buffer_groups  # a Dict<task_id, Dataset>\n",
    "\n",
    "    def before_training_exp(\n",
    "        self,\n",
    "        strategy: \"SupervisedTemplate\",\n",
    "        num_workers: int = 0,\n",
    "        shuffle: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataloader to build batches containing examples from both memories and\n",
    "        the training dataset\n",
    "        \"\"\"\n",
    "#         print('before_training_exp in plugin')\n",
    "\n",
    "        if len(self.storage_policy.buffer) == 0:\n",
    "            # first experience. We don't use the buffer, no need to change\n",
    "            # the dataloader.\n",
    "            buffer_size = len(self.storage_policy.buffer)\n",
    "            print(\"buffer size: \" + str(buffer_size))\n",
    "            return\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        if batch_size is None:\n",
    "            batch_size = strategy.train_mb_size\n",
    "\n",
    "        batch_size_mem = self.batch_size_mem\n",
    "        if batch_size_mem is None:\n",
    "            batch_size_mem = strategy.train_mb_size\n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.storage_policy.buffer,\n",
    "            oversample_small_tasks=True,\n",
    "            batch_size=batch_size,\n",
    "            batch_size_mem=batch_size_mem,\n",
    "            task_balanced_dataloader=self.task_balanced_dataloader,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"buffer size: \" + str(buffer_size))\n",
    "\n",
    "    def after_training_exp(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        self.storage_policy.update(strategy, **kwargs)\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"after training exp buffer size: \" + str(buffer_size))\n",
    "\n",
    "\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Optional, List, TYPE_CHECKING\n",
    "\n",
    "import torch\n",
    "from numpy import inf\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from avalanche.benchmarks.utils import (\n",
    "    make_classification_dataset,\n",
    "    classification_subset,\n",
    "    AvalancheDataset,\n",
    ")\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "# from ..benchmarks.utils.utils import concat_datasets\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from .templates import SupervisedTemplate\n",
    "\n",
    "class Custom_ParametricBuffer(BalancedExemplarsBuffer):\n",
    "    \"\"\"Stores samples for replay using a custom selection strategy and\n",
    "    grouping.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        groupby=None,\n",
    "        selection_strategy: Optional[\"ExemplarsSelectionStrategy\"] = None,\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param max_size: The max capacity of the replay memory.\n",
    "        :param groupby: Grouping mechanism. One of {None, 'class', 'task',\n",
    "            'experience'}.\n",
    "        :param selection_strategy: The strategy used to select exemplars to\n",
    "            keep in memory when cutting it off.\n",
    "        \"\"\"\n",
    "        super().__init__(max_size)\n",
    "        assert groupby in {None, \"task\", \"class\", \"experience\"}, (\n",
    "            \"Unknown grouping scheme. Must be one of {None, 'task', \"\n",
    "            \"'class', 'experience'}\"\n",
    "        )\n",
    "        self.groupby = groupby\n",
    "        ss = selection_strategy or RandomExemplarsSelectionStrategy()\n",
    "        self.selection_strategy = ss\n",
    "        self.seen_groups = set()\n",
    "        self._curr_strategy = None\n",
    "\n",
    "\n",
    "    def update(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        new_data = strategy.adapted_dataset\n",
    "        new_groups = self._make_groups(strategy, new_data)\n",
    "        self.seen_groups.update(new_groups.keys())\n",
    "\n",
    "        # associate lengths to classes\n",
    "        lens = self.get_group_lengths(len(self.seen_groups))\n",
    "        group_to_len = {}\n",
    "        for group_id, ll in zip(self.seen_groups, lens):\n",
    "            group_to_len[group_id] = ll\n",
    "\n",
    "        # update buffers with new data\n",
    "        for group_id, new_data_g in new_groups.items():\n",
    "            ll = group_to_len[group_id]\n",
    "            if group_id in self.buffer_groups:\n",
    "                old_buffer_g = self.buffer_groups[group_id]\n",
    "                old_buffer_g.update_from_dataset(strategy, new_data_g)\n",
    "                old_buffer_g.resize(strategy, ll)\n",
    "            else:\n",
    "                new_buffer = _ParametricSingleBuffer(\n",
    "                    ll, self.selection_strategy\n",
    "                )\n",
    "                new_buffer.update_from_dataset(strategy, new_data_g)\n",
    "                self.buffer_groups[group_id] = new_buffer\n",
    "\n",
    "        # resize buffers\n",
    "        for group_id, class_buf in self.buffer_groups.items():\n",
    "            self.buffer_groups[group_id].resize(\n",
    "                strategy, group_to_len[group_id]\n",
    "            )\n",
    "\n",
    "    def _make_groups(self, strategy, data):\n",
    "        \"\"\"Split the data by group according to `self.groupby`.\"\"\"\n",
    "        if self.groupby is None:\n",
    "            return {0: data}\n",
    "        elif self.groupby == \"task\":\n",
    "            return self._split_by_task(data)\n",
    "        elif self.groupby == \"experience\":\n",
    "            return self._split_by_experience(strategy, data)\n",
    "        elif self.groupby == \"class\":\n",
    "            return self._split_by_class(data)\n",
    "        else:\n",
    "            assert False, \"Invalid groupby key. Should never get here.\"\n",
    "\n",
    "    def _split_by_class(self, data):\n",
    "        # Get sample idxs per class\n",
    "        class_idxs = {}\n",
    "        for idx, target in enumerate(data.targets):\n",
    "            if target not in class_idxs:\n",
    "                class_idxs[target] = []\n",
    "            class_idxs[target].append(idx)\n",
    "\n",
    "        # Make AvalancheSubset per class\n",
    "        new_groups = {}\n",
    "        for c, c_idxs in class_idxs.items():\n",
    "            new_groups[c] = classification_subset(data, indices=c_idxs)\n",
    "        return new_groups\n",
    "\n",
    "    def _split_by_experience(self, strategy, data):\n",
    "        exp_id = strategy.clock.train_exp_counter + 1\n",
    "        return {exp_id: data}\n",
    "\n",
    "    def _split_by_task(self, data):\n",
    "        new_groups = {}\n",
    "        for task_id in data.task_set:\n",
    "            new_groups[task_id] = data.task_set[task_id]\n",
    "        return new_groups\n",
    "\n",
    "class _ParametricSingleBuffer(ExemplarsBuffer):\n",
    "    \"\"\"A buffer that stores samples for replay using a custom selection\n",
    "    strategy.\n",
    "\n",
    "    This is a private class. Use `ParametricBalancedBuffer` with\n",
    "    `groupby=None` to get the same behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        selection_strategy: Optional[\"ExemplarsSelectionStrategy\"] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param max_size: The max capacity of the replay memory.\n",
    "        :param selection_strategy: The strategy used to select exemplars to\n",
    "                                   keep in memory when cutting it off.\n",
    "        \"\"\"\n",
    "        super().__init__(max_size)\n",
    "        ss = selection_strategy or RandomExemplarsSelectionStrategy()\n",
    "        self.selection_strategy = ss\n",
    "        self._curr_strategy = None\n",
    "\n",
    "    def update(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        new_data = strategy.adapted_dataset\n",
    "        self.update_from_dataset(strategy, new_data)\n",
    "\n",
    "    def update_from_dataset(self, strategy, new_data):\n",
    "        self.buffer = self.buffer.concat(new_data)\n",
    "        self.resize(strategy, self.max_size)\n",
    "\n",
    "    def resize(self, strategy, new_size: int):\n",
    "        self.max_size = new_size\n",
    "        idxs = self.selection_strategy.make_sorted_indices(\n",
    "            strategy=strategy, data=self.buffer\n",
    "        )\n",
    "        self.buffer = self.buffer.subset(idxs[: self.max_size])\n",
    "\n",
    "        \n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "\n",
    "class KNN_DINO_update_wonormalizetest(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "#         id_all = torch.cat(task_id_list, dim = 0)\n",
    "        # Create a new TensorDataset from these tensors\n",
    "    \n",
    "        features_all = l2_normalize(features_all)\n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_DINO_update(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "#         id_all = torch.cat(task_id_list, dim = 0)\n",
    "        # Create a new TensorDataset from these tensors\n",
    "    \n",
    "        features_all = l2_normalize(features_all)\n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "            features = l2_normalize(features)\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA to reduce the data to 2 components\n",
    "# pca = PCA(n_components=200)\n",
    "        \n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "\n",
    "class KNN_DINO_pca(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "\n",
    "        features_all = l2_normalize(features_all)\n",
    "        # apply pca\n",
    "        print(features_all.shape)\n",
    "        if self.experience.current_experience == 0:\n",
    "            features_all = pca.fit_transform(features_all)\n",
    "        else:\n",
    "            features_all = pca.transform(features_all)\n",
    "        features_all = torch.tensor(features_all).to(dtype=torch.float32)\n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "            features = l2_normalize(features)\n",
    "            features = features.cpu().numpy()\n",
    "            features = pca.transform(features)\n",
    "            print(features.dtype)\n",
    "            features = torch.tensor(features).to(dtype=torch.float32)\n",
    "            print(features.dtype)\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ruhDj4_LBy1o"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
