{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhW7Y0auSg3"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current default GPU index: 2\n",
      "Current default GPU name: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(2)\n",
    "if torch.cuda.is_available():\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"Current default GPU index: {current_gpu}\")\n",
    "    print(f\"Current default GPU name: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def count_txt_files(folder_path):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter and count files that end with .txt\n",
    "    txt_files_count = sum(1 for file in files if file.endswith('.txt'))\n",
    "    \n",
    "    return txt_files_count\n",
    "def count_png_files(folder_path):\n",
    "    # Ensure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return 0\n",
    "    \n",
    "    # List all files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter and count files that end with .txt\n",
    "    txt_files_count = sum(1 for file in files if file.endswith('.png'))\n",
    "    \n",
    "    return txt_files_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def wait_for_files(directory, target_count=40):\n",
    "    \"\"\"\n",
    "    Continuously checks the directory until it contains at least target_count .txt files.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The path to the directory to check.\n",
    "    target_count (int): The minimum number of .txt files desired in the directory.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        count = count_txt_files(directory)\n",
    "        print(f\"Checking... There are currently {count} .txt files.\")\n",
    "        if count >= target_count:\n",
    "            print(f\"Reached target of {target_count} .txt files.\")\n",
    "            time.sleep(500)\n",
    "            break\n",
    "        time.sleep(20)  # Wait for 10 seconds before checking again\n",
    "\n",
    "# # Example usage\n",
    "# directory_path = 'saved_data/cifar_train500_2syn_i2i/'\n",
    "# wait_for_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uHBgOTv8-Kln",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "# buffer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Generic,\n",
    "    Optional,\n",
    "    List,\n",
    "    TYPE_CHECKING,\n",
    "    Set,\n",
    "    TypeVar,\n",
    ")\n",
    "\n",
    "from avalanche.benchmarks.utils import (\n",
    "    classification_subset,\n",
    "    AvalancheDataset,\n",
    ")\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "# from ..benchmarks.utils.utils import concat_datasets\n",
    "from avalanche.benchmarks.utils import concat_datasets\n",
    "from avalanche.training.storage_policy import ReservoirSamplingBuffer, BalancedExemplarsBuffer, ClassBalancedBuffer\n",
    "\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy, ExemplarsBuffer, ExperienceBalancedBuffer\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from typing import Optional, TYPE_CHECKING\n",
    "\n",
    "from avalanche.benchmarks.utils import concat_classification_datasets\n",
    "from avalanche.training.plugins.strategy_plugin import SupervisedPlugin\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from avalanche.training.templates import SupervisedTemplate, BaseSGDTemplate\n",
    "\n",
    "# dataset\n",
    "from avalanche.benchmarks import SplitMNIST, SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader, ReplayDataLoader\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger, TensorboardLogger\n",
    "\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics\n",
    "\n",
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "from types import SimpleNamespace\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Znp5LYsI-myD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fzOy2HmlWQnX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset, ConcatDataset, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim.lr_scheduler # ?\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, CenterCrop, RandomHorizontalFlip, Resize\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence, Optional, Union, List\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from avalanche.benchmarks import CLExperience, CLStream\n",
    "from avalanche.core import BaseSGDPlugin\n",
    "from avalanche.training.plugins import SupervisedPlugin, EvaluationPlugin\n",
    "from avalanche.training.plugins.clock import Clock\n",
    "from avalanche.training.plugins.evaluation import default_evaluator\n",
    "from avalanche.training.templates.base import BaseTemplate, ExpSequence\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.benchmarks.utils.data_loader import TaskBalancedDataLoader, \\\n",
    "    collate_from_data_or_kwargs\n",
    "from avalanche.training.utils import trigger_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0sf0FvHf43e",
    "outputId": "88cee04e-55dd-4a4b-8372-1f90a50ac620",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "transform = transform_train = Compose([\n",
    "    Resize(196),\n",
    "    # Resize(384),\n",
    "    # RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    # Normalize(*stats,inplace=True)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-100 training set\n",
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "name_list = trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "d0rW8AbrJ3VQ",
    "outputId": "0b5eb9c8-b53a-44f8-c006-b4c06a7c3e75",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAACtCAYAAACa74THAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9a8htW3YWCj+t9THm+67KrU7FWJX6rKQCn5qoxwQSLQsieCmJEYIx9cOEICEGA2IFtBCxQBMrCAUiJEQj/hGjYPDyJ+cDIaDlAQ9SiRrxh58XEk9Ac6mKmlPZqb33eucYvbfzo137mPNde61d+7aq3r73XHO+Y445Rh/90trT7iQigof20B7aQ3toD+2hPbS3UOM3uwMP7aE9tIf20B7aQ3tox/YAUB7aQ3toD+2hPbSH9pZrDwDloT20h/bQHtpDe2hvufYAUB7aQ3toD+2hPbSH9pZrDwDloT20h/bQHtpDe2hvufYAUB7aQ3toD+2hPbSH9pZrDwDloT20h/bQHtpDe2hvufYAUB7aQ3toD+2hPbSH9pZrDwDloT20h/bQHtpDe2hvufYAUB7aQ3toD+2hPbSH9pZrbypA+dEf/VG8973vxe3tLd73vvfhX//rf/1mduehPbSH9tAe2kN7aG+R9qYBlH/0j/4RPvzhD+MHfuAH8O/+3b/D137t1+Kbvumb8Cu/8itvVpce2kN7aA/toT20h/YWafRmFQt83/veh9/1u34X/ubf/JsAgDEG3vOe9+D7vu/78Bf/4l984m/HGPilX/olfNEXfRGI6I3o7kN7aA/toT20h/bQPssmIvj1X/91vPvd7wbzk3UkyxvUp6mdz2f8zM/8DD7ykY/EMWbGBz7wAXziE5+4OP/u7g53d3fx9y/+4i/it/223/aG9PWhPbSH9tAe2kN7aK9t++///b/jN/2m3/TEc94UgPI//+f/RO8d73znO6fj73znO/Gf//N/vjj/Yx/7GD760Y9eHP///R//B9773vfG3wIAIoBpVZ5FuyIicGXS/Dn/1mP53VM1ImQvyLqm7yKqDfKXSP0sYGa01uLFrYGJwMyviDzfqPZq1G/3zcrxWj7Mv/gLv4D/6//6l/jCtzUAQCOgNaAxYSVgPTWcTitO64rTqenYMIOYIADO5x2PzzvO247z3rHvgt4FfQjGAIgYBLK5Iuj80NxTAnwmY1kRprm9eJt+TsfLlQtdfu9/zNe5uNtFHy+vcDxA959T/5BXN7dPbL6nLr+YDv4//88LeMcX3+J//63vhghw14GXN+BxF9ztwDaAvQP7AMYQNOlo6GjSAekgf40OGQNjQN9FQAQwExpT0AexrgkI3BjMBGLG0ghrY6yLvjPpGhEQBIw7aXgsJ7yEEx7Lijus2LDYe4P49WMtST4v6sfPYy2wDOD//3/if/+a/y9+w5d96T0n2fg5XZ8WkP1htFmHUu5Z+/ohyfe1cb/vmBxOEeuL4OqKFkBQeIj3NY77wcJT7DuxBSkQe0d5L/wpzi+vMTDsnOHHYngk+hX9k6Szyd+uPMxhfOanTlrldI1g00VIWkrAyy+9jP/6c/83fuiHfghf9EVfdGWs5/amAJRnbR/5yEfw4Q9/OP5+4YUX8J73vAfvfe978Tt+x++I4xNwIHpVAOX4GsM/j3IcuEq+r9zvOjhxgCLofWCMfnhXsMLMWJZFX+uK1hoWByvMz2zeei0Yjt/xla4lx7+O+/sVf5N/r+uCf/tv/jW+8AsWEBEaAWuz1wLcnlY8enTC7e0Nbm9XcGsgZhAzBMDL5w0vvXzGy3dnPL7bsO2CbRNsu2AIQPCx1HcFC1wACaHssQQrNNG9eLI6LzQdP37Wvy8/1xGiw7K6/P2xXTv/Sd9dO/21M/wmkb7+dx7ztm0dX/q/vQ2/+b3vggjw0g585qyvFzfBuQPnDmwd6F3QZMciGxbZ0WQHDX1x32NPdQP+TITWFKA0VrAxQBDRCeaFFaQ0xrowblbGzaKvhRnD1skA47Gc8Otyg1+XW3wGt3hJTniMExac8JhWCFgHvAy6VNJ+AU4+D4HK6JD/+6fx7ne/C7/pPf+fe05iFPYHTJ+ctgzEuNL9AEXnuV7haUDKEaAUoCkKV2tz/pDAN3lG8I4CQBKgFN4D5Tk48BzHQ8P50ah8amAMUSHXAMoQgYwCSiaBG+VeCXpmkCKHN0rAc2Ws2GglEQWgJ0rC+esv/Dp+4b//ov7yKfjXmwJQfsNv+A1oreFTn/rUdPxTn/oU3vWud12cf3Nzg5ubm4vje9+xnc/6R9FMAARiUsn4itRYW078YdJ8AcEmsPyGyDC6z9JBS1LfrDeFmc1aFGICCUMVIhK/leGaEmO2VwDXq2Iiz0IDjzeg41Z8GtBzKYrL8fiVObp2XR1zQPQNQ4Ax9L0PoA8FfPDN4VIOAGYYU2IMHhgM9CKKUdxhhmAUxOn6wF37pijx7m31nPs++x2cqFJIbnRJQJAbvixLzCOZv9Mht2sdH+j6H6+63Q9O5Mo5SMIuYtKgSqoMwQJlRcP2JJOo9gQDDXoOEcAEgAksBGEDIEQgRuwlHy8WQMiuB9ePGCkWgVF46wtDMLQPRWPD2NCE0EBgAAsEg1SLInpTW11Xxnx68s9DkOJNrn2kiyGJnS3517RvhSB0GOHXDHBfuxZdO/gM3z9tO16n/E1QwghxBgWjmHFYW/29XB4KXpff5d/BwVI9Un4e02QoakBXuojvtWe0PuBNAiin0wlf//Vfj49//OP41m/9VgBq6vj4xz+OD33oQ099nZdffBG//sKvKbExswczqynEJGg24nCthRkHCUqqKk3f5MBjC7NwYHLFpBQbhspfRfKOGxuh5MEYwpAhaIaOmVSSa5PWRKWJJ5G5J7XPSjKWZPhKG2ZmPp165cOlJFAAHvn4hd5iIkx6upq1xhD0GF7BeRfw1sFtBxGwiGARKMNiBpNgaYzT0lTiIQIwTIN1nONhLGbqSWmXa6muFwcFuikTABNRfNbz6OI7csAQk0QTyEigciRS1g+fnwtpMXsa3xnYm9bQ4fQ3U4siIth713O6gGVglQHBQAOwErCzAhWSDpYBlg4ShS9OPwcRqKtWrBtwdRMPcwI6pwRMAwxdO00GeDAwGKOrtDr0jujC6FDsQlAtziobBu4gWEFYMahh0ILBDYMYQg1CHIDFd9Is71+f28/9dkVrceSfRkQnkO1t4pICmrly5Z7IPWbg5kJAOgJFufKp0Pjp/CO7vgdQXGmX31ThSKZj5MTFeY/9Q0zgoaCEyh4PTpfov1xT8ljae2a6GPSNgOCr7aLzImpWlaH7MH6jHzD2fu/zX2tvmonnwx/+ML7ru74L3/AN34Df/bt/N374h38YL774Ir77u7/7qa/x8ksv4oVf+7TZixc0M4e0paG1BWyMnSbJ35FcDnyquOxA1YfUxUs52PW9akTqe9Wa5LVqPwAS1ZRUzY0vDr222cSJTStEsV4v+M9TtM+W5/hW0aGS40Pdc89qI63aKl/A5RkN+M1SkV1nsD2z+pCESnIXEA+AdwiAk4MAXsCs0vTaCFgZRAuArr8bAzuJaWGsb2AQDQCszHsiXsHdIUSugJj7GEOSIMWPxxWctsTnI4FLIKK/y+8S9NTZsL+me1Xim+dXkBCCVhy6BDSvVXsWcAKoanrfu6613sGj42S+JisBnYBBgsF2MTEVv6pb9PpMGJ3BJvDRUGMBGzjhMl4irnkR08oALAwWAbrpTIjRhTBEAcouAyIdLDtWaRhYAHsxFuy0otOKnRd0XjF4hbACFifurlW5YpTAJaP83G3JL6nQF7L94WCuSixJ0w1mwkENAZBi5qkM9r5708Vmvhz7Iw/J464Z875egpI6y+LHJ2HteI/DvWzjp/Z+7h4BAEMFXVaQjUEYNg5UQLhR5Fj3SdHT3ON80bdUvRETg3kFcYuDAgBjYHQzP/WR9D1+Sxj9OQEof/yP/3H8j//xP/D93//9+OQnP4mv+7qvw0/+5E9eOM4+qb1kGhRmxrqesJ70tawrlqWjLQtEFhDxvQBltsUBLmGHCpgIRKLM0xZF/d4BA6GeX9B1/ey3r00YcITrC9Y/+2+P73nqs7cn0LuLy9mYKCO954dFC3XPVRAOW+b86zZSRdkAswIDJqjWCwYAD5twiD79EOU2scHIAArZ6JGolDwIEGVESyMwNTTWjS4yIEMv3iEg08qoBM5Xn4MOn3wOjkNzBCk6fD5ORapRSmrjezy3SIpFYzJrRWYAUr+L+z3l+cf7+HO8du0ITOozXZ7qGhQAkLGjjR0sG4Z0nKDgRBhhAhqxn5U56dogndOuzI1JNWO6zhKgVGmlsZp5WACWARoMMZ8TgaAL0IWwDwUrItqvRQiCBkIDSUPDgo1P2OgGxCegdVBT/QuIVFsassaRwV2O2+dsk/vWNsXXjrdnsK6fKYiEi4IKA2aGXN8JKVkkgbkmnD4ZINqdLuhivXbpJZUHuUItA7Q8cb5nIeZS4FUETkOFrOBRYiAF6baQ1yn+J0AgRfeP8X3kP1EfPQa1htZWuFCjvxqQIYDsKkCOofNThr4/LxoUAPjQhz70TCadYzuf7/DySy+htYYxlEGFlsEZ4RgBHOAgAoAhDQBlj1QxlwAcgMi9L5Rz/XN992tea3ri08tJ9yLt16ZJWVDVBHYftzpu52s9Swcue8nA6PoOQZivmBtYBoTSWZVIzX965dwow7bbGAQMgHZRERkSkU6N1aHW56axbq6+NO3TUBfJnQb2XZ9xHEZDH/KScObn2ev9QmNyBBv2j8DoJESjPOR4bqqe5+sfQcQlKMn701Off/zOjuC1akeNyeVyOvZF0IceVwI7wBhoNBSADAUiQxSwDgMpA6Q+SnZML+b+HzAJ0IUMJJqM+VENSrA8KU6IGOb3RApwRdceC9AEUBMkg6SB0cC0gXkH8wYaO7p0dBF0u88g9ZoBcayBJzPFz+FmcwQg301LIpROzDGJ0HePqSLXeBbhIkfxABqeOMTPMv6Gmq6ebtCzACuVR0SZuu09smfU8wyq5teBaRTfJKenSaCZ+81G80Ckml4TsusWSz1VEiaxZ5nujWnkVNvdFrS2oi0nVHA42ATQvsOJnAKckQB8zBT2ldpzEcVzXxu9oytngYwVEDHiI+ijo4+Bbd90glhNPczun+Ihu+2w6J14ZbRNMkuCa0gc7DgQ8d+ViwAoa+ItIghdMIZpc4n/r8x6Mjld+/ERoFx5SFGVXw2nHsMAylA03bmhNY65Yczgr2973C1BkLoyEpRJ7QOgrt9yG+DzrmpOEbu2RWcQYWGGLA0qVXfwPkDoIAz0kV7qUomV9edSFU/lyVNKugQqZYxCkvKfTzJ0kQX9WfMa8xQ8SeK69t0rSWjH9tou2vvMOtewr7MdAAAxiBrQ0vYuBCQCCZkv5s6FA9+/TKRIAnOkgQ9qKt8VpLKtK5VAKUBl9g5gSemThoCF0YTgIIWoazQR3aHxHfp4hH3ZsUtHHyd0Xu21ICNVPj+bEEPQyhySxeVQgBeHHeRMkZwKJIgliAFvAULcINR99eRhfsY5eALQCf2OAwgXRHBlZ92Hi6aT8496uojzHntuHjZKbN50CorU92kUIeySPlC5toJDnQsFLeYTuSxYlhPaegP3iVTzjc6YjI6x7+gigHRgkO1Vub7Zn9Cef4DSN5330aGIlfK7MdC7alDY8oi01tCWFQsRiBysuGZl9u6/37+kgpHDqiKa1tTV9St1mR3b/It7LSvXDx+u8UqLQaaVXteOgxPPyTL5x0iOMx2vdaWNMdBHN1DiIZ/dFjTQuGO0hsY9wqcDEIKw73s8jZOaukG7QJUn5peijo4Exg4SwbI0rCtCs7I0AtBUGiACUS97x6TyAGsVklDcP596FjdCbe+CVRlbyssgrhx0S4wIUPjZ4PCbSxX30WRTv5+lt+vnH9faPIGvh4nn+rXl4kyBObg6AOSmTIjI1MgGSMR9h9Kpz0TVEO5qyCOQJp50VPZJy7sPqZJ5yudiY0gCeJAyD1dtAxBWvzIQCA2MMxY07LxiHzvYHHp3GUDTNTBIzUiwrsT7W0SoeSPaiHFIcGJwYwIn8Gis+G/YTho25gMspk2/oM3Hu1ah9NoJFZVS/YgKeJIaSezleqrgyPqf3KapP/wk/hRYpFLZ22XxMDHAw56BIQzNDUT5+wRrCmCIJPoaXwtMu6fj3pamGpR1xXI6gUjpKIghvUNGh/QdfT9DpEO67xq8qvX8fAOU0TH6jkFkaiSE1D3GwLadsW2qbmpLOtGCNDIGBEvMpNEx7pQ6gRD//MSe6Lc+/nVx3TcnV7UN8etk/kHIXwfh6t4eiBL7yyRyCVK8SxfdusLVHJT03gOY9DHCYaozo3HDsER0qT3RqztAmftcwjZFsA9jWMOJFdSfZKivChNhaQwC0FjnuDVYwjtlespkBmgoYeqAWwaLmdDvfexQASpUPh7+rh+qScd5KuARCJjp4/SbOtxHIitXvpP4PqcnCeq1ax16/Fm3VzbpHL93lqOOq4A9n3QdAzP/yBAIi82zhAo9htDBCLLwWK4xmPkACKdLKWAHAjYfJiP11jPvkzrJiihIgeh8u9mJob4mHYQFC7bFo40GyOz7A+q4TSwRefF5hEuipZ+PgRUkSIG4DyHFiwpI4WDOIww8NXIlzRkzuH8dH2a6VWo3CjNRFeB9P7n/es5TCqCuCQeJoBoLc5glcyIXUXOPE4FB87UrkBKTjEjc3ES+kcDcjJeuWNYbs0w0EDdTGOwY+4bWVkjvGGJ7c+r807fnGqC01rAsqyYxWxbzY3AfFA116mYPIyawsElcvYAbhlgESSLzMmvXRlTu+9M2TkSk+GbK76f2CqBjWrTPNLH3n3yRH+BweoAsQWhNhkhqP3o3W6m2eL8HIrsNsl+YeLoCHwAt7jOSecAlJZgZz68/D5oYIQOgWhQCegc2knR2REdjRmsWQkyU62QhS27kUTeCPjRUrwlBKDcgQsIr42T+NZ7Iz3slglxH91Cf8DUBZQqDOkdSfuYg5wlaj6OG5bpGhKbvnuyTctnnV9+edO3jhhIMInS0eBp28EYGJANCwNYOwR3eiTQGKwEeGWixZ/f8aZMM7CYFux6A0MKADColYGWx34u+SAAa/jvrq11ZPVMAyBnomSZAtQaqOQAAYTVzgBpqvMfnQ1Mj6wxQOtL3xEfTTa0w/woGB90YU7TPSPMGZsd3um9DPrFdp22udYAJSkoDPM53dqK9EA4KGJ15hK1sEpD5OoWrVDiuivlgyXEzQZNMDnUiH6pNERZoDh/VCrJohuWrjVKwEsGkQdH1r31zJ2Gy1B4EVQSwv0YDoL6GDIIMAj1jAvTnGqCcbm/x6G1fgNYaTqcbLOuqEjGhzHguyrRPDlVDAZhmSaa36xRajDQez7WNo2akJVRhVSOAACzHzXH/ZrlvK7060nUFwVYJOrdaPLuHnfUx0Pcd+67AAgFSqqpzvpcPn4MPGe7XktoYATQ8TQRsDs1kzCU0KBGa5hvXCFS5r4MG0cspSAHMLKBRPkQdIoTWGIRm2jPB0hbIqpu6NTUZjUHow3yXeAEMpDgDE/HQZDNXGSBOk5g5UdqYX9CQMva6ntyJDjFH9JppUMp9D8Dk/gih+VqfbXtWDYqAsdEazCRj3cxZFRIJ+oalvDclBpppTRoRhAa6AYmB2S/F2UQwCTigdrIg8OzCIXyQsUoCaJitn7reg4byJggw9DwmY64ECDpEzpCuoF2ASDpIIhhtxWgIwIJcPp/zrYOxW56jbnOlVjMyhmt6Eqej5D5huj6GgcgRAMXmLMSXUQQAoOqpCoSY/vIjF03qnqlCySyohCaYvT8XP7fzshcU68EcsS161OW/pD8qVDFG0ox4z+dwcMKi754+YQwA7JGQikichs7Q2AmPf3b63THGrnPjUbKs9JIXNQGJ7LZfBgYNyFCLxbO05xqg3Jxu8Ohtb9Mw49MJy7KAuU0SPpB4hcgkMRnqaTwGxk5pxy6+FjOTxrRCap0eANOmWc02p+HOAuFiQjLTgq6Cw6I9PBtd/UuunHu/sQhwzca9F56f8dCbeGTToOx7x7Zt6L0HIphc12i+bmzd6i/g7yFtSEq2fqUQXJWJ7L1oUIKpI0CoK3gFsBwVAuoCGirVEg0wWzSXBhVjYVf1M2RhaGgy0EaCkyEJUDSnSlO183CAMkyt2c3HxjRzo0MxVQEpFyOc6MOfompRXHopj2lj+SRAklfT7y8nP685A5Xj+a+t9mTu5+X1L282QNhpCQ1EMymYJCO6ajrvYZmEndgzAa0RRFLp74Pq6+ua5t2nZRQzpq5ZJR4eDcZEGEwq5Ni6Qvcg12E8wsG25aWVAZEN6Hvk7BHk+t1JAPZ8uUen6frpc691IZhbOzpSiyLmTcakxhwUWpsTyAAGhhuGROOkgApVFMZIjDbwVCN6oGlzK/taspaaeESp5XeCVKY803JHTG7udc2emtLN86kAFNh3w/L+uIa68qvkLQLQMH+UyilM1Tw0K7IErQkDjz1u0QCpDGX3VHDS+66CW1t05kjDj7m1ACh6rw5QV/N7e7Y1/FwDlNPNDW4fPQITa3I2M/N4WKr7Jzj6C0hgZp4dO+Dmiz4X6qux4QmSE6C4mk3XgjnaMmO/ucGNjLB9QxoAZX4GM1M7IJeI3dv8zTVuURaSH6LD1a5IXxeFtq7d/gpiH2Ng33ds24Z9d4CiLyX2bppBbBaxDXREi6FZiZuVF4BQJRop6RbtM3eeygWNqcBI1XD/EVFJltRxNoAqN7SmHWvmg8JM6A1oBkyGsAGUBcyLbkJeVHI3k1CXgb6ro3bfd3Rm9J1gEcshhQswRZyEM4g9o38Tlh2nLU4bJqBSQcQBxr7i9/N3r6xxee1QypM0KNcVlRwApcF1Jq5JcyLt2hOg298kAJqux9YYWs9ETQUyEIPqaxb5V4Z9+3KEXo/LBtMkbxrCzp7DJiAmgS0UGeTpDVIDI6NDxq7O3GNPx1BPhc8MGStGy/ix1xwnvkXbAGleGfs8JGJyLBbFs5eaX4UBP91fnlTdShtQbreEBmJgx0GKXeMJIzyJfnLteNKsyO9kGlXFTwJQA2vBr6umfir/pJubOay6ABdkmuJeqhgammbBANrVpzGfSrL8PsdRH8LxDIaVEjuR3auAKoEDsQ6MHSSLpYZAmnuWBu4L2tCkmIQdEIaQPHOR2+caoLRlxWm9AQgR/VGdOjNLHozi2HFziJRRJOBdX8N8LJwx+0/tIvrZ1GsQBSERIbQs4MbofcHoO2QoQwNUq6NOS1aJ2JK/5XKn8gmH+yKeI7ZGXewFlAsVU8thxV5ikcooy1E3Vxn6Hxbtwn7tas4wPxLVoKQWJYpZTcyVckceJZOicXEpwn/TwwxXB8Qg/dHk4ZeDRfcwYe8C7gO068WJu2qzWkdrxkS4YeGGBR7uqKnJK0Ah0851cZCiPk59X9H7jn3fzQxmn0OzMjCoh7244t1qD5cyZ45hcuzs/CMCnYbxOJFyIGx1jn1upomYf/86sce6ny6PIeTSSMxnbCW1Jf4qdZi6alDcF8HzizARpAEaVD7fyNeJmgQl6jntQ8PWASXsk/nQQFBEXEEZJgvALd81x6ZzHB1jZbOiuVzQMbBDTT4LhBdIWzFkA8kJ7hfz+dKqD4ryXzfXZBhr+J8U+uFmHxGGZwwfoppyBymme8EokT4KUmTafffBlZn+H3hKOe6aDc29pRmL3VeEQ5uWXo5B+6d9femzmCagBCguHJNdpQqG069NUKaRrgYK9GBJ3QSDlC6N0Irnence4aSWIICbd/qG0Ra4acs1XMTq3iDLCo3kaaDWDO1/HmlQWmtY1hWAT7UCD1W1pwbEh1tNOcZc+x6gpG+bMpVNj7mPSf11rslqCgJaYyyrZq8lJrt+NyDUHXOCRCUlzfLRwlnIrz2x3gMBFwg8v8+0ga74tRynv6rorm+/ctH4sWUYMZvY0tgibdSu3wnYRcdZ/UMUbHnU01Rd081t4YeTYCZ2kDNue3BmKueoE2s+hD+nR1p4x49AJYlVF2D3WGQCsJkkRgxZVfvGrJkRiVc16dACcFNwwi1eGqFh1ULNabhbPp7edQ1t+4bd11S8CDS6AVtB8WuP56J6pBCt+fP9G/zCpBPRLDPwrde5NAPVc+796rNsR+Cfx/wLKd9rYT41uwQ4FIQGpdtr70r/luHZMRjEEhlGhFFMubZOu5rs9qFrJIpOGqEeQzCYJiWfRiZUtmaSowDcyFx7KYANJIUdJt3HDcCCDpEdMs4YY0HvJ3C7AUk37QDF63MdrKhpNsd0csOnHIdsVFC8CnyeVVqvZyAF7r+kv8lUjDUNezX7wI5c/pWpFnL3+u8F7menTqEkUAFUUnMfvk90WBveO/L9eAQmCGCj60qzYLux02lngKeJnhvUtXszeSp8jnuTgajpOqHRLmCMYOcM0NghnSF9Nc2gQJoRKlKAMsYC6guoNdBgQLyW3NO3zwmAogzR7f+Wc2P0HGi4h7+ChtE79m1D3zZsZ2Um21lffd+Tlxcp1mVdR5o+kcuy4CQDIE0IppOlryFdTQyqWzb1VjMkCiujiokYX6Ki3HAIj3Fj7m5aihwKzqjL5kb9SAUhz1twlqNd3eg+M4LGqUXRiARV8+37DvfmhjmERcnvkdJEJrmjdBx2dV9sCmcKlPZbZCZZ2Bj452N2xNr/UcaBzAyqtlpR4u9qdWoAk5kIT1iWE7itaE3fqZUoHuaSH0YJkgMUTxq4bRu2fcN2PmPbNpzPZyNIJq1Jj2edcaHMCqFJg1K1HfnAszYF5Rx/k0nBNGV0uZoK/3IkX48mV8DyBRiS9JeJSDIHhR515VqUQdi7ARSmcLIENaiDtEmlQqb102v1ruvBAex5L+BHi2KDXYMS825Bq3VgyTSi4tE6ejgdd11S1xloFvo5MDDGBgGh9wW9ncFjA0YH4Gu0hY+W74/L0Xv+mwoSyYx9tEJjgsv1HsckaZYXFL2QuXBZwIIwJqFsumhoz9MfUUIrXB1h7deejNLoIoFADgTsbscEoLPgVXtrtIvySNBPsGrxuIPcIbu8ZpOMPUq5twuIHoas0T26H9hWqoRYXQKhYxzUx0c6AcSQsRu/G+aEq3SdeAG3FaPtoK65UoTb558GpYWKyZ131J8ki9E5lRuKfIWCmezbhn07Y7s742yvfd+QSRRm2VOBRHGoBTDWFdwYy5KqLn9h9CjWBGiYFURAzbbf4CJheVdT7Rfo2LkZIYmV2/vQTAtKIHF16Lwxvc3yxzWYUs6zf1hXcmhP2K9tG7H3TbUkZFX5hvkHOOP2yJwAJYritepmqpEcTKZ5acCdy6b0yAFSKFD9EVz5nKkLSMmRIupGR9zD4ZHaQFsUjLW2YF1PWJYbLOsN2nqKOH+wARoRq/8yjMn1CJne9x3LtmHdzji3Ba3dBYF1YrQXabo+EuyxKg4h40gVqNSZvGDqFDA6LnT9TnWmn8TqXk+AMl//+CyCYuKx/REMf4huZ/dZEDG/Id2w/llMymYbF4by/k7qpDRsLw0RAzgufOj9naEVOVLX+qC0uRPyJcaOWj6FQCxZlTY2hqFalAGRHRhAHyv2oSnxSXaQNF1vn3NQ5HpLkw6M/lZwUrQoTkfilxTLWA7EYIjPYfq2IMQ58RVRfnLPbhH3p0stiZtYHA44v3Htqmexjl66BqMkCGQyY5Ncgq/42wBFFfAoLN4Cd1u40HjAo5yclNs1GJluA7PGpJqpLEbZRggB2gSK3oU70LcEJ1a2RO/bwE35MbcFo5kWhURp6TO05xqgKKHqBdXW1yQ6TlRGF4olB+MF3X0SClqfgEPFqY6s/RyLAW/LgmVdwlFX0exI5Kl3Nq3DFoXLXBIMf424cb2f2Rw5nXGJOZLPqe9L0+RRrlGZTD713xyU2Fzl6HROqAwlkLNS+B3oHdS7JaiyUF4jIJr6W4+zuMKWADHHQlaTFwfDlQApsZGlbEp3ko0OO0gTlP04Tbe3TEgk5iQp6pOya9gxt451lXA2I1o0U+KyYFnWyH+SUp2Z24RBYj4P5pjNpmXSnCstU+xbCv/zuYHbhm1LxjgRB5ofxZcCmTZEwilTyoklJuGw5LNA4eXcenTRNal0mv7XvLlkenms/iVQ9zqCZrRQTVcDkUqlzdWQ5KDWMaRFaME1pu57UPxXukSWaTe5qSM10iQKArMWmWxNzZtzbqOUhEN1z0OdNJkMrJiJ0ziZMwwAIHEvkwGSARbNMttkRxsdQh1DOJjgvFOvzejz36Q81n1PRvd9nghZApvIREsl/4wBCAkjydUKXAiA6Yw8BEb7TnT3Rbfdv4NFK/62pppnfxUNsgAY5hvi/KLuZQhUSxcpVcRAhgZ19N4jaCGjVimEy1iXLqix5Ywx1SAZvQmAYqkeFJRYdI9pjHnoZx2D2Ww8a5YkBGeQOs9y29HaCdJ2Ay6P75nZ6+25Bihh1jFp3pGcL6QQhS7QKaFxAxb9fnSNxtjbDrYS79NChF+qgA3XAjg4Oa1Yb26wrOooq5LvKCDHwIZ4mKoCk949odyI7xOgVNsiMoRrcUBkFZxFsIioz4SwaYc1tdSFbwol+PE9fUkM3IEswYmmMNY0xtJ3lfRGB48eKcgV6XNoF5pJFASKDcZCSpydscQY57Mn4VdKxWMvXUtwQtUmkpQ/niAOiyBs1AOgPkw67WjLMMkZcMdKMt+T5o6xIcHZeBmxYtGEYsw6h40ZjVWbtu/6e9fyLYvN250mNBIRoHezWafWrNLZqkGJu6d6yxRJOYM5lr7ejrNaVoJp9t48E898/amvRgh3UVLZ0Sxh3oIGy2U0BMz+Usl0GVqyQCtXi2owBTAXWxCcuA/svaP3dHZnElhy6Wn9LY3RDKSE/TykeaDOGBlKMtFDYbmQrjnSZyrcLNPlSwdJtzT4ClKGrHaNQKiIxZADlcef8xayGcqIigs916kUyvlX16rNj2YLCVYdvilANft4FMwRLCcNnkCKuFIhQb4LvkSaf8SLoLZIIJogRXtmSdRsreRIJK9wv8mayTrAyb7hfN4AGCDy+zg9sN9oomXTniC19n6fMKGPgWE2TXKB0Y4pCIFD6uCN8XK+C8BTagCiYNtKPMjo4Ha6dx6vteccoHhm05HgpKi6ZFr22lzdxcwamSGEvnTsbUHjhp2bxYe6SgtxrcAPbh5gKzu9LljWFetpVS2KpVT3CBevHTJEMPaO3Z1z+46+j4giSjUZcnFKOlSy+dwslmvl5kYdVJnStOcOUcpJNcmPRTYHMPE9L3CdQN6Tyj6hWHwVpGyQvgN9B4+uCzAASqlYYj4aIhIAxglOaElmmd6YCEICcLARAKXm7PBO2vtlvH6cZGpUU/RaUUGyxE2LAxTXoJgzLDfVpLifzJTQq6wL9gyNRki8sva6rlgWByj6IrIYAlu3EHXgBRyEBhIpRNoIobI7AytGsGmes5jJacnPGjwHPZHaneq35VevDzaJzj1JgwKomWYXVcevzFbherE6OkOjD4aAeahjatdtKyJopIBDndQR4IQgBZwM7D2zaTIBC7t50XytLKTYw4ojIRUy+VombvONNswvzIC4Z8+MjGz6e39kErH09z32k4KUDpHFQExhCJOmwK/n8/4cAxU3yQG2ppM6xXHK/eFEbtaoFLpvazwonF3P8++p6aJC9rx27G4HCUH/C0iBg5TM8kpg7bf5zzXT0qv2xEupJEBhEAYDMG1s6LT9nsPNLdAK3tC1ULUn27aBgCzdIq0Id+Venn6DZ/NYtQgQALaq3RGSzWzZbNnMP0Z9Cn9SHqWaQC/K62Belg4eJxVWRwct6zMti+caoHg0jgCzBiVU5yhEwX+kNmlNK60T2HdlJFpGumv66aHq4UQlhbkbQlTAYHUJTp5y32v5WNhWTQDnuUTO6Zy77zv6ppEeYxwACopDLshA0Amnm5tI78yNsRgD1O5pDMEQzf0xiKykd0pfUp5nonWmsVAAk0AP4uk6FQVj7GrqGT1NOaY2JEPgir4dGNj1gAAoup3r3OjG0H5Y74x5enKuaS79cxy7ZHJST620zVOli2DZzelSgNCgUMv0zb7R6ngVgOLzBBHAwIlrx1yCYlb/FneW7UOZpJPToXaI7HN5JrL5ciJKNoZAgr06Fklucg3ZU+vdgnBfYXb3jN+zf/lK7ZVBigDY0UyDIhBaABbzRx0KDMfAENLcNayRPA6IzUkliv6JrWM37eymSVHZQUe0kUXhFGBCrpq3ZIsuRcZ8UEZJAFAGYGr7YZqT8B0q9jTfEw6cWph5dixjh/COIUsw0Toy85x+jraCwQ/yxtU2A5UUXwQO8D2Jnq97dwpFsONckLnbE5gUP48Deq/aXtckM9O0/933ZM4ibj01cwt5uYYKhoavXTLsIpHiwWucMalfiT9s+vrpu1luLC9XDoprckXM5CVA1AuZiG1Zt6aRca1y0GopUNK10KRRrbyYoDo6uH0eAZQw7YhL64dFdAQqhfoT6YQNALyoFmTtK0Q0Yyr6DrHqj45cCWK+H25iWXG6uVHNyaKaE/UtKZocUwdmH+px1UpoDo0tqvvGb2AOglCIzg5CCBFVQwJEZlyxFP6O1n2ggl/lWExDQkmQs3iiIWpCjOPRmcpD6txznMCg4emTJZ3JrBvkUo8hPbJnSRSvPQoNuv2wT/iE4synIVy1+fN6LosBVeyKZLIsRLbKLCB5wQ6MKTno0xBwMqBmvvAsaNK0OKU9e0SikNKbdneHu7vq6GnZIeP5HKDZbU2j4MXRQutrtNWJrS6zS8YGk/YuQF7586kH8skHnvzzK+qZekigqc/vsELzqhI20jI2gzUZmkprWqRvUEcnqPOqCSoQyirHI+lD1aBoFI9BPssN4UTd9wJZgbT0UQOSbfo+zzw9qrbXNO3BuEoL5mkmTmeSjIFFOsbYMPpjCKnfjdCCwcvkynk/sjxM6HPUiDKpGtkBH+UjSLn3CYNYJ/00igIfLxaruSSXVwqNse+k4cDE+ArSR06308F51QGVad8at8m8My0fp5ElMiiE0UNWc/8NQd0KhIHWBpZFw42JgKU1LF5iJQA1BRgjzx4bgmI+dWZCz1MU1Fz6UOrzsVUHL5qhmBt3xtWq3tQW8FCzjowBbjdPWgYX7TkHKJZYzRDnMQtsjQuPz95sQTFxAJTF/FkcqbpGw0EiGyNflgXrjfp/3Nze4HRasSwtGLsYEJlMNtCLzJvAw3E7hiX2yi7L9IsaU8+hOnQJ2oDOGPG8wLzQcwwQiy7MFkTqJ2FOvktbwq5PERbmHTOQMkpqZxFVwbPEWI2qyYKPoSNs1wBg6pc/c0oj+su9IIQgwXTl2OH4PPKAm0hcbvVIjzAAUIKT4P4O9MJE5VPqIMVpovWC8vZia8Vt0+ISkKlzFCBrRtp97KBBAUhFMmyVAMtoKmgCq7ArAbL9+SLDQ+H20UdfmDQPWAUGR8Jfl+51ppBnXGKOJ4OWayCl/raD8RgrGoATMXYidAaECUBXgIJuSahU8DMlnwJAi5xx4cWj+/qwvDVWURsgVUtDbH/bPm9mBmYzzYXqmpLwD7bCo4BM6dU9cujKqPn6cD8sAM0gacOORc4YfYFQw6AFnVbQ6CBqOaKxAV55hp6X5sKKfp7BiR5znzP729fylSv5qMR7WWuW1HUKvydyLQWgXqkGUEKINFpOzoAxrQUHH/WY8wr2qMV8OOtb0tNaLd7pafS/AN1wwm2EZXHttJqNW0vH/EyYqeM1pCsdi7BkG1lXvwbI4jAluUN5Urv8x8sOSDwf1YmC104CAdxWE4rUj4WXzyMfFBkD3fwTJlBSVBY6iWSf9bywEfuAm6nGQ4cFqoannWJSfC65MdZ1wel0ws3tLW5ubrIOUDPzxoAuuF58KnJlOjsMEKMJzyxJXNW62CISELhBaxkAFiXi9SlEzUiWAdezuwbwibBrd9iVwDDhkU2M1cxH6+kGsg4srQFWVG/WBiXq97wUQzSVv2oHzT/eE7WNAjqQQCUn0RgykpC4itLBQYnS9J8EeZ7AyrFVQi6ZN1IsDJVKOKqn85IjOCHK7LxhTykdiZ4UAEwARMOyI2JsWRAx2kxAU8DSh879tjdV50vmzogwV7s0CcBDfbsZ6ttA0OsJzBQivoK9NzlaEaLu1zxE+TyJyV3CCTkcvHLGK4CQe78WoKPhDicwBDdE2Ng0KE2jEUi01jFJPzCHHbTbugwQZrWkeoKT3faGJljz3DwojMVMfBNAqap6mIkZNmdlRfpap3RY1L993J0B5xwzDSyWtM2zGA8+ocsOtjxKg4Jy6PUuZmjaFc9VIyB80oLX2ThxPSZyZdOXdW0ZLaXMvdM6BDih1EoCxfzroME0mSWkOEzVnAAkzH/Fv6QWhw0hz9cBIYRDX/+ZV8kB9JjooIMJBb2mzXNZQwiNlIW3yZRUwBmG9g+TGjpHLYSw5DXOJ4d4hGblg6QAhFmj6rhqUMrKtBIiBAIvBOLl8w+guOpNkWYyUUemsd5qOUhvBtkJCjq0wNHAIgv6vkckDuD0RpnmsiiYObn25OaEdVnQmAMIBfEVZ4l63KWEaTED0d9g/JEFl0JyA2BRQ2xZT01jA4KrI7WekBNh1crslvMlN1r2C8SWLZYx+g0EMvXLQy/9t3Vsw8RjJp0Bs7VzAsUpm68RliAlkTUyJd0IU4NJxabzHfUSF5lPxfGHjpdtwCp9VNFTrr0MhBw1J3WNOFVI/wNA6vMg9/5khmpNw/tEYdiA2puFVcvk3vjbtqn00nPd+oN5PRgeQBNgGZH6JnIidCNambujaHsoBsj67WN+HMnDATr+6Q5y9bx6rScDkmv3vK8NEM6yoJHgLMBZBGcINiCAI0Pt4V4sVjUSmrdHNFQj7lnr9mSRwVJnJ6Z9do7V/D7mOO3JCp0JEGlOCAd+9wyfq71d6E8DYS5OhjrLtrFDYJllxwm9rxi8otOCTqzmW/d6v45UnstW/TPquGVeLxMSKSY7jgMovlWx82IfiAlBTrdUG6NnJuQbYLg5JzI7Fg2KmA+HJP1G1Zik5qJqVPJpJPaIA4AAQMO0J0FXi9N/8AvOF9QplsRDlDGBoyowiQlmeU+PjMqxc4FQ6a6tVdS9mlzMxz76ZEJXdDcemeC5sfR+Qx3C+dkgx/MNUArivk73JN5ChScp2dVFEESpMah5llH7mt0WrZqG0+mEm5sVN6cT1nW1CA0UxI2YXULdSAK3aS+LoubMZbKHRoLcr8OludbUEXddsFi+lfR58WfKLLmj7xjmjLvv6pDrvjpiYZUwYAJOh6a2NIylp/alK42XMUxD4z4++nwyEoSEdGHE37N1ShbEcG5p/89zE9otP0gJNnI2hxH5jGapWydEpJpoT+ZVkvZWAyYOKYrGJCSKiRCifM65dUZzXW7VbwbcbNCwmOOjEGHvHTebzlHvHTjrr8YY6LZeNcpDzTqLaBr3VSwCyv3Z9MQomeZgWGp3S3i5lMPx9cUzlhPsDynvNJ1zCRovgcjlDn0SWBkgbMIYENwJ47E0vDgUMD8ycEAGSvz5NGts+g55CvTLlz/rgGabTeIPlxDJq497/hXXpDiI0fU6MnOhPXe+RwhqEHTAZ0jDj4f5QOlR9UPZ9bt+Bvix0gsCdj5h5wUbr+hYgrXmTnq+UQqR++/Z3yEdosxn3ZuI8QR8xZc17ZriCgbEqjqJmgjFK+3aS810Q0sNoFuAQwpngpbCrmP+IGN1MZvGy2pDucCkW8UEldByp6YinsYBg4clh6YmBsdMPc7D6gqwtQcB3Mwe5V3UZ5ADaBk4sXpSDQRpCM2i39DZWdU1B2As971E6D5PnCc+Y7HAZzv7Kdpf+St/JVClv776q786vn/8+DH+zJ/5M/jSL/1SfOEXfiE++MEP4lOf+tSru5nzj+Q5CTpwkOpiXeUgVp+HWvTP1VYCW4RE4IUVGKxrASknrKcVrZl5YNI0WAcxg5RQ+Td1oFwcoLQl7u0LmohATc0Dq917KeGrXiBRF2Ux60Sm3DPOd3c4P34Zdy/n6/HLL+Hx45dx9/gxznd32LYz9m3D2HcDMpaqf+xhsx9Wb+E+LUqmt09w4lKBf57C0kyCHfDMrA5ScnYwjWL9/RytlaUH3KkYCaTKFWYi7guHy+sAUu6l+RQvl0ZivducsJsDIhrE7MTropFYt6qB05eaCk+nk4FdcswB9vcBNCEsogBlFcIyCE1Ui0BDz4lCdvFyhulA8PrLc7Hc930Q+sM8hnkO87m4uBcOn+8b2+z2LgpSztLw8mh4qTe82Be8NBbcScOGhk4Ngzx/TdIcn1OZ5riZ46mbbixKy+veGGin8D3RDMKR16F8DlV+WVUB4Jwp2EFXzzuj5drXhMKqQZGOZWxYxx1O/WXc7i/h0f4Z3OwvYe2PsY4NTTpYMhtFXdc0/fX8NN07WhOLJ9BY5oga0omdYt6uakVBUw2nMQlwSSejWKzRuNF39Wvss4k8itCGVjjQT9JC17aMAgrqK+5hZvcxSibWTMfAtkZqkjeq680+R0JIW49A2WNHM7+Xf7F++dJJfxnLsdWaCulGs1LkwUQj7Nf2PvMFB1lBI8ktAW+RWjy//bf/dvzzf/7P8yZL3ubP/bk/h3/6T/8p/sk/+Sf4ki/5EnzoQx/Ct33bt+Ff/at/9cz38cly8KFtjo0PkOJUz0fbNQ+m4yW2zKA+MQU9koWMKVBYcTopQHGGMjCmheHoPVVp2lsyCb6Z2WaIGDhp6qg7rOKtoW7XoLSlBTBSUNOwLJYy2M1cQyUD8dTr+661hu7ucPf4sZp5zCdERCzpm2VNHQP7sqD75jS1dQScjcwzA+hGmoCCJxew14BvUD0nx5Lg5oYq7wSLs4Wvm9U1LbX59z6kxshJVZleD0XsuxBsi6RlrAAHVGuA5GjiuejAvLZiXsuN7O3IJBiAtKa5NhqDBqv/yab1n/q+ZZbhMycEMpDSBGgDWByY2Pw4rRnQcSjLLQYqpEroBX384jjZ6qxS3MVz1gMxqDq1Mt3hKsicAUkJy712fWjmkh2qRbsbjIUammsjoCHBqwAriToMW1k4JtbQ+sLEPAVJBG7EUqjgRT8HETWQAq/DFExTc6yotnAUgagyKq8Mq+OUSylFThGoGcrthCJmYrC0YkLgTmgQNHRwM78I0uriXp95kBSZNkezCPfPRZs0KNF5N+twmU9M+0vKh4rLASACzI0+YnSgaFLINCZVgxLO1yUqNP32CBJpFDDdKba/JZCsArPvEdfih+N2L6AHZWl6Hh5n7DWLsT0vcwpIY2iGcvSZLqt/YAFl3VP0e+yYrUmmyTTM3bWHDCH3XXTAfcw9k+90ZcWRaamdBL8lAMqyLHjXu951cfzXfu3X8Hf+zt/Bj//4j+MP/IE/AAD4u3/37+JrvuZr8FM/9VP4Pb/n9zzTfUJAvO/LK3+GYO2MpJw3q/kR2hMnXOk4Z1KU2aIjaU1xqLq3S0Qgq4cQSbxMk+IOp2M0dTxtBZzcWBK4xRAukS36RM2TmrAclwlJ69/6aKyc70Lk8twctsXtt6megIW7mmObR5SQMUnRSArFD76pEnTlgE8DAynqc42m0HO4nB8aC0fmV65Xz5nBCYq2jEumV5ccmjmhlZDAK5AkmDNgDD8JhxIjAoXfh/hBCym1hxqEdVEzYd929Js9ilhufIYQa8VbESwDaCO1JwssI6/1oUvmRxFI3FaokAzK9c02JlKe0dXl17RGqRHxMaib6HhejtUTtSSvwD0VdOkz7iCchdHMSXyBYCXBiQUrhhk8NJRHS0E0EAu4iUX4VMSYE+Vhm5rzhqIkgRNnmEYsnAJ149pvZertUeOEcrtgUlW7iwJYgqta3SoRYGy6F4agQQtuCgHS9c7MK3YSgJd4pso6nidwAvh+5nrA9pGt2ypQYDYzhoSP/E7sH/XN2zXB5Ngtp5KDlA5dXTscpFCAlGLacYGTTEtuad8hYv5xWoojHWSRa8QFpYMAFo6xrj0xWlkdav1aU5QMkHs1xk0HwKPV+ugBUEKrg+I7Y9qZxg3UONaKhl+bFos7uBmQGba2xCcGoRFNwV8MvOhspBww0+C3BED52Z/9Wbz73e/G7e0t3v/+9+NjH/sYvuIrvgI/8zM/g23b8IEPfCDO/eqv/mp8xVd8BT7xiU/cC1Du7u5wd3cXf7/wwgv6Ieik1DGaUHQVmUJyFCBF7RzsSQUNgmeGBKW/QvW67p5sq+QDORIn62YQNDcdqWDgDq9m5jEA0Yc5zJlD7moam3VdIhGc2k8LQQxpzQluVdk1sHjq7YFBpVbEsmJd05fGKwg74h9dEBWaTa3MBEtExFgXxhgmUXohLCjTdsk1xyONXb5oq7DtRDs95fUrZ6iNWwAP13RVu3U138XcHkAFoB7vS7M8NuZH5JqpNJ3x9JvpGhNmUTAi02QbOAknmmRH2ls14y2tBUiRfcfYNvTljL0tIFrAGGjQMgarEBZhNBAWpEO2ILVNQQBhobYwkEKYsh8HCGcK4h8S/hWAAiBCDmuG4NAW1gEHcA2gH489UYNymMYhwDYsegcadnxDjJvBOFEDMNRsAk+gKGBp5lhMWsuJfSyc0en+0dpJupaXhScTL2Jsik+K7zu4SXE2ZQZN0qdM0E6X8qWvzfzXtFgEEDS7rMEoyOBwxiXp2NsJzDfY5ITOYloVzefzvIETADHW5UAecsExvneWX/ijfZOM0+n0bj55Z6BvgOfQsSxIgg6hHYJuZTnK92J0DA5QNNPqsPD06t8hKACFfXd6O+wRsRVU/FuCbhObulWvX9l5DpFTG6MBDnYsw6yXfwmBWSl/ZLOuNdyIvUK7XlfGiPT8g5uOJw1gaPLSJHOprVFArQJs+DiKRHrz47w9S3vNAcr73vc+/NiP/Rh+62/9rfjlX/5lfPSjH8Xv/b2/F//hP/wHfPKTn8TpdMLb3/726TfvfOc78clPfvLea37sYx/DRz/60avfJVGuy0FnsigVpsWsmSbLIndQMg728VAB27nkBNMBiqWyl7oQrrdKNEKSdS2JJdkZbWAsokBFNG16mJTC38Udcn0TqrOXlGszZ6SPa2h0EWptEgwxQp5MevZrybEN260l7XE7aWNg9XBZdnSuwyViY8/lwa+J5tN8AY62U4LQQ812aWTLtSRIbp+N+Ze6cVLyqUzDx2dZFpxWB2cnrMvRt6fO17VeG+Fy6UC8vw4Y7D3gqEt9rk4lLNxwWhbgdALtA2Pd0NsZO68g2sHUFaSIqIMsFKQsUY81I5xgfYl1jmTIqplS5+/U/nESRVcXF8nMpyPGNJyeVUIbfZiJqce+8jUzza14T3Hl+NM0Ta+9A8DQbJknYtxyw1kEm1jGZHTVdKDBIiAVxg3SECciqKSboAMYoU3zooBctScoDCf8vZwpGN0YHiFUNEZS13TcMpiW1AU5zVj9yQCjB9EigZZokA6WDWe5BTWBp2Mmapon5rV3K3yD2kGDgpTAQ4tSJPEDRTXIWAGLmBahY/QNY7uD9LOZ5cTGd0B87UQ9ZcuKZMn3JEoUUAQFUBfArt13y8Vl5j4XnC7n+tqCF3ioGXMDiwBsoefk4oxeJa9Ih987/3Ffmn3yPRGR4AcRcNEKQCFPIKkhbuKZr7lhNO3bGARNWqqCkT6O/obGAKhDaETh2Aia8FkMAnrQeD9Fe80Byjd/8zfH59/5O38n3ve+9+Erv/Ir8Y//8T/Go0ePXtU1P/KRj+DDH/5w/P3CCy/gPe95zwxACiNKRH0Q0ySn2xcNwQlNEh3Yby8jPoxpDwnnUZemVONQb5nqcr9k9sY3XQESpkFpw+u59FmDcrNmrhWy2JArauXqhBs+Jm0JAkrDoh3ITUvKpBePRorEXyO92Ltn57TNBETVXjSOzQlXb9bxLwDxOArHQ/Ebqu9ZZ8gBCjfdQG1pqlXxWTBVZuaCqfWAYKpUCgfl1RydT6d1qp3jTIrq3jp22ICUb9Y4L3wK5kc9yocNBGHVoNAQ0GmgryfsywlbWzRyBBZeLAPLAFZS7Ukr5ejYS3eJrVG3xZOubtcYkDm/cSuEyk2WVTVdZDNvIaHZO+87Ou3YdweBw1dM+c21cbvejucef6qpRiyJnWlO7obgPBg7K2ATN8UQW4I7AtNAG+rL4SYSXaNG+Enz9zRmtIWwLA5OXCihOD9yXUCCaYUGRdJRe1ouFHDItJ5Vgp6fU0Q1Lf49kdXnEWWWNASgDpINPDYFJ3C2qlK3SPqkXPMHeGu3wtKmfWOmjAJQRHJzpZUhBdIL0/a+oe9nyH4X+4VJVLCjASGrJQP3ZZISSm8+FAI9bwhIq+9pgdlNS5bI6Cb0JeCfhCUk9E/ylqZxYQEiU7iBE2NqLvjoyfbsLpibpsVBSd97Med76RBdu56E08GJ7/0+SqkN06BwY7TRkn4LkKkgdawRpiMBkY1lYcpUJzO0kW8yQDm2t7/97fgtv+W34Od+7ufwh/7QH8L5fManP/3pSYvyqU996qrPirebmxvc3FymyK2b/NpnJ9SxCiaHuLroU/r0lkSbp0EVUXCy7x1b26wkOsVvKl+ahCRcwb8SGLveWIliRAyUxFGROVaiTpCrmuvv1UwyQjvjGpQhpqIUKHNvc2QQt5LLpZiOpNznOM7mNluH8V5p+voopICRZbxtfkzLpWnDCbe3t5MGpC2qefJ7DbO3uoTv7xnpM8DcLALrBo9uH+H25han040ClHaMjkrkn+M7TZY+bz2WXrrlWaWeDTIy2Fid7ogbZFlwagtOreGGF4AWgBScqUukqImHrNoSuT5AzQ5aMJINOAq4EaQpgPRIsOaVsN38x+53M9u9j48aCfnstVuRMuaz1pIyx2yxrM4XC7+sievtlZmpiD4vQxP3uWmlj4FOA10GOg9E4DczGjH6ogCFFwJ3NdkmERUsC6GtQGuaPVYjf9Jc4uDE/b68jpMDlBBafDMY9zlq4JhVEwSRdNZFClRw4AP/DIDUq0Ln37NMqwbAWekQAppgkxtAblTkMq3KhZD2Fm5pUrO/7Z+JyTlvtnNSU5JgMTWn7n9h5mmrwi5h2lFg4i+K8PPKPRJQ1BwtDl48/YJHPzIzRBQsE+XcZn+lyF3FX4MoQoZZF0vQ3PrctblgNEXWlbEk9qraGeAR4MT2fNWcsgnhXhhTmDFcdW3aR0/vEPS6PENue4FrHidwEp+fYjGU9roDlM985jP4r//1v+JP/Ik/ga//+q/Huq74+Mc/jg9+8IMAgP/yX/4L/tt/+294//vf/8zXDmYp1z9LGjEDmGSCGRgzhyFipzAJTrgxmkgmwDEm586MRJgLQjWec2JVQm2fdfL0by/2FMS/aELK7sznDbDgdRUOIKcA7DD1LA2tJ0DRUDGyvCprhE57YTyhGoEgBQSlCnVAmcQugs3VngH2kIDGEXZ5nmvr05SpCMEozA/6u90W/Bd8wReEeWctUU0AApy4b5AyTk1UlyHQKh3cPnqEtz16Gx697W149LZHuL2t9ZTaBAaTYV8DKffvtoQlHlmU4ESga0pAaObbsBCryYcaTqy5Nzy3B0MUoJA6yDYDwg5eGIxGgoWBwaIhLksDrQ20LuA1Q9q9nEF7aoBCtjZHmDW38x3O5zPOdwvO57NWVaVNwXtPwnovKDmagS6+z7ey+uFyqXIj8x3oOwbt2NGxUYdQNydZzba8oJm2c0eXFqZKv+bSgGUB2iJBsIcYSLEwVwdzZAzMo9z8UTwJHKD7j6ncgmbJWYAoEuiUKsF/amBSG6AvwkCzG5CY2h3Qgp19Ay07aCk+Aga0Jkfpt3iLnVZ5WdAUhPQTtEhy3Dz7shYzdXBSQ3m1uKnnOhHpkTARLrcCiOwy5HvCNWdsAkCCFIo6PRoh5AxbxBYBcQE3OvsSD+MlKZSiqkP0gJjfR/B9o6UZ1eiaIolw5ijJQiqQiqiDvdhYehmTxbQmNfV+FY+9r24SWpqgw/0BqeRt8bQRpZ8HOp/z5+DEv302hPKaA5Q//+f/PL7lW74FX/mVX4lf+qVfwg/8wA+gtYbv+I7vwJd8yZfge77ne/DhD38Y73jHO/DFX/zF+L7v+z68//3vf+YIHqBKHzPuNVyqxx1cBEjhsCeHYkVURVW9xb2GQvhX2D3H0JLtoB0CCaa2LC0JvCOlK83V8QSYY5MxVY+fl7pgLqezIuUAKHG/ggAMRbs5wx2hEqAsYeIJ7QkX0GMvKku4SitdgH0AWzdAQzbmRpz9WYarvSdpKGU7ma6rG5uGKH+2H+2CCaAsrToOrzYvRcLfd+ybFmDc9l21Kb2jjw7mhke3t3jkIOXR23Bzc6sh45b8rqauPkrD8+7zUZ+dfZ2RuQZIjLAGg7I1wkWzt3DDSozVQEqsVVLG2DDQzLzjCnwPNu1kTstMWBogC4FOK9ppBZ8WtNOK9eT+NgZS2mKmDSVYwUjj8fKhjwDl7m7F8vjxlJ9BRNB3T3rli6Cs2yexyVf4iqa/JENAx44BMzdhB2MHWNCooRGD24oFgt4Z+4m1GF8BJwRgYcHSBMyCZloOEtZ3ZERP+H5hYIBRncDd/AQHJ0ianPVjKJiflDU0w5SKndRHzQUQBT7KDNm1QH0A2ADaYuEJLQAxOpqlxrfOhNpdytO/dVpI3bgEKhemMbEQAXEag6A1ilQOIMVAhIMUoGNI17FoyRucjbqsNWnRPYswUXr5+P08twjqUmYQa0gvqm+NP0qt8QPBoAHhpP9HgJpA14COazW7RWY6kJjMRGSmnRbaUy4J2HxA3dTkWhQhRmMBpIFAGMwBTnr3sbVMu4WWT/s4aJuP77OuCG2vOUD5hV/4BXzHd3wH/tf/+l/4si/7MnzjN34jfuqnfgpf9mVfBgD4oR/6ITAzPvjBD+Lu7g7f9E3fhL/1t/7Wq7rXrD7zV7HDF5BSTSegzAipC3HEQMYCNZCiURrVn2Sgm6rQ0eu6rgARlghPvUJx7VCdpymJT0QCFSY2mZaQi7dcsx6JswnFNNTAi6AZQNF3Ckcpl6qJEuHrZnO/DdhCTGLqGhTVoliZe3vuICAikQtFNVfGyqn0EW6nTfUshjk4krimOgDK2972NgUoy6Jmv5NqPgBMpp3NsudulqV139VfaO+qhn306JEClLfp+83tjfrhLB5mfA2czJ/r28xEbbIKEIsTPPzYGiMdMX39rMQ4uZcnCdwuzqTApBFNAEVNO1CtCjNkIWBhtNMJy80J7faE5UYLWq6Wu2d1gGImQI1O8Uc7gmJCJgFUoLcuLXL5kM137zu2jXNO6zo97ge5evTyhKkX+VKp1aRhBynSsaEDjUANaKQ+WEIAD0azl6e21yFXX4RGA43NedJLTY9MDubVjJmAMRiZBh2hQQmFI2XvKTpuwtAwZnBcME674tHdVGFfi/aVxLQonvVUdkAagE0vyA3Cq/rj2AoT4mI6egs3uudzaQ7kApBAhbqBUlbDnD1RwEnNgaLvu2ngAAHn/LAD9RQOw/G1ak8AwKNX/H7Do7u0n0wACVtOk+rI74EVBrCGmpvG0GjIeeNc2TfBh9LEGfmpiOClUVx4iAr1zatyE0ILVbRR8GclgjBBhOOY5wdznxYR1axMmtDy0bVPoY2NjYA89pTtNQco//Af/sMnfn97e4sf/dEfxY/+6I9+9jcrDGJiCPdJvDTvguo7EiaKQM0KULyctS5MXUAyNBX5GEPjZ0KSksOklI9XqPHoJu33kqNEkjg5o3fm4KG3wuXi965hk9ccmJX3zLg5d67qSnwR13cfZl/IGVdfGJJNwzVervcLjxWTMM3ZkwyA2fWU+RqytwtyY4vhd/MEpw8KC3gwRhsRftwaY98X056oExlRw6NHb8Pto0e4ub21CCaPjioZGwMf0uE9H+qyFo+BsDo1hlVUCvLvE1aKCKSPeGEMUBew5aDJ8TbHTx9HWPp8AhYioHlV7oZ+amg3J6y3t2i3N1hvNQJsWU+XpqzGqckpjcqnqLQqA6171mCV/lyS2/cdrW3YyUJxBXDToO/MskMvF+29rf7KAZuG4bNk1IUTWjVjkr0YXh0ahAiKcHqpjz006Y6nyicCiydrVKIugFY/hmjOmn1D3zdsm+7dTEYoc7fr6nDgce2ZfR/WY77/bR2ApKwaGwsRLHCGdweMFdIXEAEbDaAhND2XY/rWa8m8LheEm3J0jtMcHg7KZiKfgIiZe3z1OdP0d7ZaY6mZOPxL+Y0Lng6IZHRNrhjZYTsYDVpzhmzfk2nyFFaOPhT7yvDeqIZfvE8XI5L9LnPmuEiq1t2JrwEtd4RnTzoqGnGHkRpRD49OnsBJb7hofcZQP0Cne0DwKSq8wfeUDd7hAF7Vsnuua/EEoYFJGcbxfFqPA5K5SBDfTYy7MCguCzkBCgCYKrFjsu+qHdpqGNR7OnPzAyEhKdDZ9x19N18U32yFuLtfhTJXqEOlSFx3llMz34IEUYu/ZqJpP51ztyR8z7o7qYYMicJezcwKrrzWRxPNswIYEs9xJhuQic9Tsak2HWjNs6Lho7rJL+2bx2cmmNQgBCyIv9vSp9T7qkG51fTykQOlWYhpBSfXtSi180ce5IN65FUX0R2BrI2w9o6+af2ksauzKYZGbmgxPHIeapKZjhuzmOaEQK1B1gVyWiE3C5abGyy3t1ge3WC5vdEorTVLK0x5ZI5zMg8wREgTnw3GoIHTaQ1wIkPQ9x3n81nLNfAGDKBTjzGoq6tet87hPV/FOLl5BGJAAuqbo5oFH182cwujDwZ1QhdCtyCFquUj0r/FsmqK/U1MGiVV8uAMEfR9QMauWrntjH07a3SI+TjF2na6XJ+jaEJ8z10+qAHX+KHtW/FaLdl3BWMU4H0hAuQMdKvdAwE10WzQ1ALWodCrt3bTQZD6zAFOCjApfnseuScyNO+NdFDP2mHuOErQLM5KX9QEE+zfiCQFaEF8R04HLbxYrJzI2DXPSvglEaV2nigCFZy0uokxvreVrHzD15zznpLZ2DmQLoBYTAlOEPuYiEAt0+SLjClz7Rg9tM0gFCfa5IXKxkzzI9UZNkHREEmNZI4UJoZ5SSSfqT3XAEVbIQVVVKP8+oKHzKMX72QEig1RqgYF8R2A0ChEHRnbJG0M9LakZgIpESQ6pvi9vzwCokfaY0/4hjB9uK0xpHsjOARMBasMKMcwiH+fjzgNhsSCGxZeXDUoIxYjPKGQrT33AGcmLEym7st78sX9k5nnei2aBkoQjgAnEplkFweWx6kvmwNUxtscaV1SsGkDADAzTjc3OJ1ucLq5KQnquMxdro+r2pPy2ZlddohKfjYxmuJo0ACMj41pT8aexE56B7oSVxJlwm7GUYJWxsnAycKAtAZaVtDpBNycsNzeYr29xXJ7g/XRDXhZ7NUivDA0a5dL4wAmDEJwSdJW7A/7vuHu7g6Lm4vEVqccLuTI7Rp/lns+oy7dokGBAli2/KHxrTD6IM1U19kiXRIUiIHdTHtPCLUUex6LrBnSva6VOV5v5zP28xnb+U59Gix54STIKEeLZxF7Zin9mMelPKRLEg7aIeGA62YNP5FIASoTQOMMIkbrljIfwKCGnVYraJjRSTN6emu0+/Z28OP4nOCkF2DiWhQ4OBn2EouEcgGUGYxmoeijVAAuegqxvTFPioKTvser75ua+7sKQcKkZh0Tll3buzR3fBVgmL+J+H05NHdp1s+96YJzdsWFxqo9kRQyDIhVDUrvEtr63nfT2mv0HRHhdDqpYNuahecbXTNz0DAaIWUNh/aGpWRjp+mVjrivvn0OAJRkXnKNygWTyMNSmETMvBNs+1zpV56VwGLaIMh8CFVN5sCE6mRVgOJmnWraKV0/3o+HOu3eN+PKF+3pKhChaUSSUSj5MwDiIK0iZXdGlCDEgbDJ6sE4igiCmsBECkJKc1rpcNn/Mk9DAhQoWHml5ia6IDOcjEGPq6agNY68L6f1FI5jNbTY+zCDk6oxmcfXx9ggozFnHStdfjYzJXmbPlpmgFRJrEP2nqYeI66NraQAMPkvcDBYk5haMxBiEU7md7KcTuBlAbUGsuyRyfUvWAPqIsylqJ9YGLImQJExFJwsa+RYkVHrL5WpPvLmMq3XZ/iSk+oMzUBFMYZlmiWtmyPmVjzEtAdeO6cIMwQEOBFXhZvpkFnzUPQh6mu1d5zPG85n8286b4AMLGTzE8xk1paJjZ1rRa8/rD/n4XkFYT5yGhNCAHkUl2k2ZQePO02LD6DTgn2saHxSU5fdRy6H9K3RfMPJcd3puPn4ZfG/OfTd/QFJ1LRDlkSMR9H8MqX2RDRbTPoiJp26pN8ub6TGM/ZrKfan/kDxQAEuVIsBDGKQhY47n3GA4pE1CYqcFvG8R4vG5DBS8zgShSkfcDOs+uVVkKL9s9xQxisvmtR1HH+UL6YuF9rpB199e64BSjhAIccsHI/UhgJl8l48qZuTmwSCJaLMujeOAKHcK0DNrGIkZs0AaJKP2+9aydBZOjyBE42XF6tbkxsDADAAL7c9X+Pw8YDJQo1dpA4AR3Br3ZHY2FUyBxKcjO4OZ35BBS81yZCGveoIeTlyCdByhQeGZunwSJLHGQjVfQqjxX+lDgLZd2L3n+6FdHJjxpwLJDOHpnkPSMnFiIT1+WLTla44ITgyG6Zha87U9D4+IYWYiWfPiqqqm1NiQQZI4/KBtsUIkORYyQD1Duo7sO+gvQP+Is23QsjaHkcg648k+c/xGwCWbyWyU1btkxHYCk587q4glEmgeGqm6QwHsdcWUj+cRp7z4Qa8nMCL5gXppoHsThfihuFGP4OK4cxQk3Ft246784btvGPfNfJOxJycmdAaYWkU1xJXu9p+mbxwxM8pAL4i+vhNplUhkUxzb0BYNZhAc60PBEyaGl/GGadxhz4W9L6AIOi0ovNqZmm+Av3e3HbUysUxOMmR1FibD5SnD6hVg8mAvThIcVM10gzMAFg0kV+tn6Pa85kW8xRMkcKZ2LgLN8iiWoQWWspMgqZa8mHKkwyECGHK78dpkqmvpEMJ9auZXSw1BMVeFqPbUD9JELZdk8ntuxUmHd3MNnZtxxpDMNCxW2HBfkjRMCxttVoXGhosO7Xv9+G0LaawrLFXB1Sef4AyqfAPAMA+E5IR975DhEHDECY8E18ll4WIyXx00oCIM2OkZqGpSm+xZFh+jYsy9SZpiohJnYIxyLLT6qJtNvnuD+PHtTM09ev4h9ts9Uf6T9pV89k826Iu/PKskmXJZZgaLxDPiOvoJkkfFy+c55tQaheuT2IyS2fKQJoJvKvl+aT8589KXjPjOCD27FP6f3eyDeCSYOT4Si3KdYCivabD/ZKdD6lmIwTz0Y9ZMqF3Typna9rLGEsCNApmZmNLSKBo4AT7DjmrulaWBdIasFjYcmtRqqCOTe174Aibk0hKVVQ3k58WVVv5bDKKufGFUOjs04MTn4uyksr9mRlLY5waY21NHZ4NnDhAyXL36bPgmZjTROCZmaHVyQdhdAUnm2tOts2kZmN4pYbP2rSP6nwIRH6iisKD8VJMYMAkkcthKMcIKM7tCorUB8zWryhDItkBMPZxh94bhnk5bE1D7YZk+PHF2n0LtQpWJmfYSt9H5j4awzJeOygZ6kSdghV03YAj6o2pYQIjBZywa8OcHkEskRlZIjWGhVQZEEFoLz1LKwKgaJK9DAmW+J3fux2BSQX9hz1FrMVCmTNi1YYqaHqXgWEFuVRzcrbUCzsit0rsYa3fJGNgH2R+Vjs2O7fSCB1GRjOSImwfUECm88USYg9/u6axfUJ7zgGKpAYlzCPpNxF+FB4S3LtuShlKUKPkrsRGmDjhk+57mAwAgGtPlhb5RaqWYgJO5lQV12FR23mZvxlFhyw/9+WeI/O/scsQ0UqAjotnW/Rz2DajqS6Hpbl3eEOWWlmvqmaYEX2bQdEF8ZtEavvHeI8DnPIWp1L9zcUllQHepz730ufs0nWkfLfES5Nj7OVrBieUTPwKKAlERhR9VsPDyEqr4u9GeLupjD0aQEx74mMTY+LgRMq4+B1svY8O7JafY2mQrcU7mIFlQSTMjAvTxbD6lnCijACA+duqvj4S0jCQumTm0UtS1uWET56IUK43BymWVXhdF9yuC9blBF5vAqCgAJS6D90/yZP6oe+RddiFmn03v5Ntx/msRBvhsKqmXAW9jGV1k1C3GkUeIVGY7YzQ7J1yOGT+ujJp1ZK4EOTlLDTrJ5OOLckwf5szTtKgBQZtKojQaQFhhcWCzdE9r2IKXq8m5T1ektmDa7Kw8NvrbuLRvz1XTs6ngxAHtpoCRbUXCO2f0goHD+kc6qG2ENa9ZDlCdC/omuBlDZDiTH+IaITeKMKHFOBDrg3LZJ9sOVeuAhQS8KAES+BYa0FXLNLIeaHmg0qADUIAoIjWsfGVMXS9351xPp8hQAh3ZBYBJgK1RZO3GeAdAY6kyvSodObVtOcboAyvNyCJHuELN+vHCEgZrjNT8bBCwG2ASFowqajiXnBUaAjYpK3kTUmwGy+RudOJIVkocYASe09g5My6RcKetEsCBVUgP8zItlKZ6g8ytWCwes8hVYMCY2BiZq+eGpTyn49zEQonYHLBwsXGrwrCzpbqQNsj6fjmyT40WeV4ZooTQLn2vFWF6hlUrYzA5DNETnAcGVTwkmjhQpNSb3cFtxR4NwGp6l+U2hObNb5yzUKxfclmzRADmruNbWMFJ9wwzD9FHQgT3ei4URDLq827a+eIZNSUz/Wkm7tGiI4I6BqWlKsfrzefi9hzDeuy4GSFH3k9oa0n8HKyvT9L2vnq2COJVu5vJQBppqk0gWBrCYTWYIkQGcvS1Gk2aIPUgulIME5lzqaHiif3feE0Jz4Dc+kL8nwX0EJtJpgtEPRx1oRyAASMQQs6n7CL1UzR8s5PmPg3tlVAlrisak9GicbzlAzVvNPNxNMjw2sKUih0wE1jKMJJ1Zro8lIjmNe50vT4oS00Z1j3AeOhgI9amndAXIBUn2k9HJjAHPpNIxZCk5lNbC/FbwoBqDRLNTgHAbhk1t7DX2bY/lWwW4sImhyife4JzEGaiVaBSStaPPONkQb3XwzZqSJLfdhXvS6ec4Bisd1yWMxVpTtUW8LESXxs9HRgRxICHLQdrqb1G14RgPS4o28nnH4ylVOKhCnqGe3syi8bbJHKYrwiTiSAkhBU/ZwJSddOxoKnkJp9QfdYuFDgJrNtF5ap0BMODRHsXbD3gc1V3odnnlX4kv0NYHdpc3YkUkEDoAnhAMJpPYWGyqsPL23JDLhlvKoadbLrOoE5hhRP4KNoVGLir5t47OwcZu8D5fVINLdGlAQ4Mmz4tFi/mpkgffj8J74GPLKs+kwFUFdNymBCt+zAtLA6x55OycgsT37VqPkjiQBEuj+87gsFyAx4q241UsLjSxTaBcL33zgTin+etlXXWLO9o1l4sOc9MeAckpwzvBQqEuHp5LAl/mMmjFYAogiWfQ9wuywN+74Z41Mm2FiwLspcdBzVCTI5YoLBCZz72nItkEkG4hQhfqbH6xpubOURkNK5cpZi/oDmiWmyYxXCkAVDzthlwyK7ARPBQMOFlrN28g1sUmhE/Rw5TsZcFkQKfc/3HsUAQ2sKDbtmtAzXjxdN9IBNGPSEbBBl/Dkus+ACamgEqxauAIW4acg/CGQFV4HcZ4yW97B+zH4nSWc03xYs8Vyuy+BLDnoMoHWLBp34BmBaEveJQVTnZnKXBE/cuAAYVkDW8sqAIskbNy+maaAIamIao9AHlL5NG5zK6+nbcw1QfHHq/kxUPQ7SErNgDPPMhxGr4QNKE4PMFMTXCOw1OT2ZfizcwySEpGpUSkjV8pJH4LTf1Yo5nSlFJY21/h2YXcCpCgiij4lRfLMIUoOizMc3QYYYi6m0CWxRSkAfgn0MnPvAee/KG31jkYMFe7LYV/PmogJYUDacEmMBcQsJpYv2+XQ6haPruq4BUtwZLbRcPho2NlFq3AjAUQPjoHLy3A8tQYKVgCJHoHKYa8eoPjVh5CEOhn/xG5sgYiUIMjgIrVCOY2AEZ2LDmfAAOqkd3gjuaIxu9mxeV8i+W9pfyd4f1Se+rOry8n9qOJikmj2l2PQTqE9nHay3uEAo008MkGpJCGfcKMBEX4NYHSeFLPW5MzrPHaJupiFRVlBss6AlHqwcYxVgRND7GhXFTzcr+rZhDM97sYOg6fGbVUXuNmgaMaY+H56nJbYpoVRVttT4tsDGNAiwdeSmrOpzouYkpXf9MN66D0g6lrFZPqINfWxYx4adN4iYZiUynPr6nhnbG9nEaLj/IcbwJlreK0jpB3BiwqjRMU9iqKnptVZV879ZIo3E5JzqoNX3nWlkwqcoCKhr4J1eGNNmM6US140T9CBq+VjfXAnhvkxcBCdANS9w841rhCSTqwG6PsYwZ+59R993fQYHtQXcBsdzTZCB72ZFUpv5zSxjjZIoEFFzkAd9uMbRAcrQe4xCRwKoSD7/qwW8nxMAxQlSenh71I6kj0pIdoAzTidovjFrZsxQldm9Lj4VQGjQpICTK0gxznVQUriMSTQW65G/9v6Wvqp0iHnepUhr4qm3JX5O5EzQN5iNnzGZHqppJ+oJHEypEdcgMYBi4ORu1zpC5DZKrniPktgE2neAUh8s76k2dkYTsurKWt0FQAAUbhzgpC1qC06P/zKfBgEncHLhM0GH8fFBrcOV5/kX054r0UfxuzLt/ovwZTnuVQdERkykMTBMGld0GHwzaruMMn4jwZ9Hog0A3UJniRl8s6FtuwJ28fHnBJW2dvReYhJSiYxybUrFy3a/DJWf8/hcaxWc3HdOGRYk1NPPDt81A4oBFFCGn9pYkPjeTz8vTzgYe5lgYDUl1wD2RgfWVes+3W4ni4LY0Pcz+r5pOCs06ook8xTlPq8CjQsANt+skSRemgOe8VUIUpzQnZk1MnBCqkXpNu49tMQwcASoyXqgYcciAhkLdj5jlQ2b7Ok3MPfusKYFlwv19WupsS50ThCpGLqDk17AiRRw4iG/BJDV8Wo2v/ouoUFJ7UXRDphApKDDBYNuvhxZCiUJOaV5hFuAFJAWaBxDosabNzfPNYsWCvpCsxaFDKymZrLuLb2e3tvohQV/7JtG6xCz5iQCgFLMNsEPxZ4n5igaquUrBsbie5OCFqCsbRSBYYCi5phN3sSzQpiiV7eannuAsvcexCQyC5Z33WeZ2lclq5K9NAZTPzjYSXXVLFTMNNUZFU0nqNmkg7oz6JRaUa4LVIaaKrsUl8v58Wc5L7sxAYHZiXc+NX/glxsQYbjq2h25kpa6ql/D8gSM6lWvGXBhIX2UmCvxUj5F/aMejMfNH82nKMFYLANqhAvbpkoG7ipuTA/dps1fnNAqaHCgQglYUrOBPGcaPtvkZfrpcJxF/Z4YFg5YwJGbdFxzQmYcp8Wc8eDsdBzGqQyO9c2lq1D1e06VbcdYd/0cfgrzWvDn9BBx10gRMneL6wBqpNf1dg2BWXfl+udXbjR9tuwnQSCHFevrVpsEfQA0AB7KMBwcY15mAFI74eprpJaPvcwFA8PO6QzsJNhJYPXSQsLWJUhwIIWL13zzwzQkpAk8VtdkalJCu2egrA+txOyABgTXManQIztW2bGMDcvYrJCgFhR8AzHIE5vYc8T4Gz0bRksnbUmY4avzqWuVYUKJMv1Gw96hmi7yRJA4JDRrOYZAgkujcyo8NRAP9WEcYr/Xmk9EWTla/brGtMcZCjBbYyzu82Ee62R9SWdYA8iHMF8ZFqUEgjRRRxpRNwcvlaFaJGCQaoqFLQ0BafJKzYGUwlj6TVq9KcAqIks+O8papQJmpQhgFXDFTvPXq19kzzVA6b1j37ZgYnPCs1QTulakj6G5IUCZUl5kHuRCzELbcGC23mLYJRGuhjHrQtl5z1oPgSA99E0CDLjtfo7iON7R1dSEIZqAKIogWmdmgIIEXhfgRyXLyIDpUmWAupHPLBljL3E7ze/RSLCyJk8iq2XizmTJycn6wsYAbbM7chJCalBcyiBw0/0HAlarGugqSarSgBPrKEY4bygC0jH2kFsgwYlP9QxK7K6X8304fu37etDP9bwFTKLvTuCWRX1E1h00FtBYbG1oVsxuhCASXhfJS3X41nOZxz0YZaUuh96GYCiFQT6hGQ+YiS+XZ2FLrT3KhfGsgOTJzZnAEEaXoYnUumCnbmPc0agDVgBw6vyV59FoKgGo2zHTtAwNAZ/Ckwt+SLMSjJlmLaD0lXHpk5FmpiwwOMrvjT3nDPj2AKZZCWbAsOSNmd15ilQhNyMNNOlYZMMy7tDB6MRgXlSTRldX7hva9mHRVJhplsjse1JfvUTqAA42EVqKpWmm64WGaqAOACWjZhqa0xUUsu/aMFIhAwR3iLMoGtWehDMJEgBL9CevqVFXbL4csPMzokdsEQmRrbseWiPnKTIs142I0lR2i4Gbx1toSbQky0CYpLjQkOLs6rW4/Jk9EIEba38modf6Lb7+NTvxpJihyg8+u+X1XAOUvWusdggdDizgAymFeJgpw2hmZebud8Cmj5qkGGd+F7TNmSKCuXtEkeYz2Q21pnObSmi2oQ5qu6l0tsxsImU/18YoEUw4m1Qs+l5ASQUnSTr1yu4dU59DhkzXcYlkgCIpGENt70t0QUxqMCnOFy1iJ04JwKj0CWXuapkBtt20OFhwmyq5nZ9i7hIBzpDCJYcI3yue+wEeHJyEYEFx/HjFCjjKoat/S/xWAErmIQG01KGO7TWWBdR30GggaZoWW4s8hR8DQ9XVilisJ2JSirA51GGiChIgce5jmK7g5gWZAHk8hH/0uQrCTVPSNtdq6T7osceO7XjoacFL9c5SvxN97wPoGFbTt0OoK9jgbrZ3monk9IwOGIaBfBcccn9O2Z5tTMIs48DEMs6OCjp8fIkhVjFdhpmjir+Om6eOAMjHJtPyF3lkks6rCZPc/9mWiOfU3bHIhnWc0dGw0wImj1IpwHRazHI88Lq1bg73etsx0a6I2hnXNCjFVOhr0vPTNDIfIQcpmaGayJPctQApk/aUEEFOLs455A8AYloHr6MT9BO5T5g11Dt8QVpGXolfDznMHrgQGaaLSUsMoJjKFB7x6dbeKWzYrhXJ61gTiork2skklYX2+fesjv0AVCtZQYpfGxZe7BSzCk6xTgvhKev3adtzDVA8y6O3ZIgOHLS52UZNLuWY26vNgzsIvnN7KmsBDl3yOOUNYmH1ob93wqxhiAuwNDDPgMFzkFzmaZh6j3p3z8Mgo8x2wvFoAYKGgyFHMPbMEjQP6ltQ+zRis7lkDaiUlrRAAYr6nLjXvIOUWl8CQWEjTb8UgJLoEh414qGA/vvV9lytk5L2VKTK26Z/Bh1ktXbczpxJkFzCRGwuis92NNdWXDMbTYyO8l//vTiR835aWmvWeXH1cqSgH6pBwViUSHfXBDrhzMY5eeBmfgvDJCMBgkWF2Y3q8qhdnjD402hRfDyq3byV4mRO+K61Z9ekVO7jJN3MO+IARbBD0ETV2Q5OwD0IsgPTCfyTmXp9rQOQIpVnzqIM0VTCrL0aQIALq04QAGWEXT6lV7h/kJ/jFjdjHAOplbHu2c6vkDeZgO4DAyKcjrSRZExGASipQVmwoPEOlm5ZaiePuDel9TFCgxI0UiUxRP0nAykR1WM+c7GMKX08VIPCWFiwkFj0E7I0AvmYpeaPbTDZCEAUkOywejRJQweQ4MQnwQIvnF/4HAHmR8RsL93MTtURv5F4Tq3P1rVO10g+IUPr58BSUQhnen2CCmNuQRhqe1cBx9weIsTd6xLZPqUkekrnhYsAb3NivGkggXU6fKt6Rum27dCivXy1MPe5Bih35w2PHz8G4OnMDUHSbI/1BT7YwzxTwwLRCVF7gjkoRVhbT20CClQoaBLuXQ7Puud81xeey1Gi/hIOXo5SWqDTWZJKiU1C/TdRMEpGHc2JXpE8arIg3xbwrJdEtgESsQdwMCFKk//AOJkSbZgJwlG3M0Jn5PriwIwO/nw866Mgzk9Z5bio/bvwnoc/ewI1Ku+T7d4BSTHxYLrPBD2ufAr+WI7T4e9jh5PsJ54wtSqLSW9WP2ddwGO1eVjh0QjiEogPlpsHy2SPAY1uYQUzAoBaA5qZjrwOjznzRdTBNH5y0fX7wISb2rJ0wGKhiZqHRKtG92ldPjswud5mYODVigc6BJ0GegeoDXAfGE1lX2ayKL5rz3LJmF3zM/mx1WPlNQSmOZHok4Rk4z223/sqoHJz5Bp18O50ZIjm41DnXwc0HgFHmhyYmjl+ZgIypzcOyEAClq5aFNOkLLKjSUcTM2s5U37VrOSza7139Sd0LUQRYpzmuqDl4eR1UbkzsSc4cw0sG2BrbtYhT8QmRft3zNxqQL2SQKrQ2MEhKaMnN+UNaMzxKCOpv4m+uZdufAd48kY9VAVJO22MCZy52VYBitGGuJOtU6RmiTndCMDuk2W+mMHbdF15mg531A5h3AQ4P8/HJE8qb5Tffbar6fkGKHeP8eJLL4WUHJlC3QkyHI/E/t8nauSD2ZqGGQK6iLKC6a4EwQk52QIwla7WJhgQ2ZUoW/nqiuip79hhi6YVZ9kCYFwt6LimEszqkKoJC0fZNKnmjbv6j2UGJ5l0zSgdgEHqZqgpsEdkM42IJ7sgCakgOERDE81c1nu3CCBYES4FJGqOMMkmLxOYx7U40wsK1jwXgZuVYEzovlaxmo4jGXM//MbB3PQfQuMRc3Z4L3dCiD1BX+pieuWtWMES3P7dmlYYXhZw7xBZwT7+FnkgREBXFS252F3Ai2tZnKwIA1gX0LqCTye00w14zYKBzCHqlGe7r8+X4ILIQb2Ck3VZsJ5OON2ccLOd4MmhJvA4LYQyqk8JXILXk+8RjZ/pYoBlAINrhlHN0qv3ZzAPjMHTNDnIrMQXFt4txeHb160nqA7tR9CBkqoirqx0p8hBZfzY/AEGADYll4AG2X0M1OrWBAvQSbTwXdc5D98f4itr183Afm+VZpsMrNjRsWOT3UCKJuPqwnPY8RsMVEb4oBT6KIcJQIxoGVCaGGiED6fMoks9zF9SfHMoAE1epNBVMYdsd2YtYGN4DRpSkAIgIm6GC6BwIGCgJhKspDbzYv0TAPECghI+IbGiRHnAGDBZUWer8g2Jl9LB0UlLq0jhN0OzW4eWw9jkMKF5Ki1gAxlmogAzgIb7131+bJ+dZPJcA5THj+/w4osvgomxrItWcF1WLFahtomrqzB5gvtijoJLVoWSAIA50PzmYZklVEsCZSoBFMDC/Drcnjf5OYREYASybrBgjIZiXVqw5wsEH9JaLh53inJJNjRDcbmhRc3c+9simCJSx8DOEH0mdQj07LFz1VeAMLqHmyma93MdoLCpFpPEceY6CcSP3JElNDbRizuMkRWG1wEaF4Q/x+fys0uO9guXUoukSpUo5UNe6FKm9yMweQpAcq2RgxMgNChtWdDWNezqbCY2Yah2iwngXW3JY2juk+4ZQRUkq7qVLEMxAesKOp1AqwKUtnqNkCWBewFmmSnWQYkSr0m6K8/gzn+Lg5PTCTenG+w3G/a9Y9vOaWsvczMBylfRfLWkKQWI8opDLGdMJvbKGylIkVgLSMBKOf+DxrQuxMJ+wwwTIMUFB/cnCZwdYyRl3Q/bCxWkCjMIQzO+iqCbk+7wUHxLWCikwgF1BaYgwUKW6j4qSJs2VgQyPIlWmg4IA426AhSL6jnLhia7PiMBHr5tj/7Z8pZnakpzDaAYnUhBTzuUDDp0sIErVEsyZ4NN0AIzfxUNSwE0kw+GCzL1fTQLEFjUF68ReGg0TaoLMhdTDQ2GRd5UmUkEJVH2bFLNlVfWiQspsGcfAnH/oYFcZ8iXX4xsnUki6mkPUVzXri6IQI3q3+MOyO7PliBFTVihM3KaIq7PsYeq6udnaM81QLm7u8OLL72Exg2nfoKXIweA1gAdLc3g6XU19n03VWiGfQHIZDMARlcJ8LxpRA6L1luDp44uGpSjE2A6QjXNVwA9h0XCtpnqr5TeyLQzhVf7PoWbgiKsd3gCOpXCRUxtH+pB+00NTxtq4RZPAw2X+oZKaqM4Y0UyIBR8oZuAoNqWbud308a4pCdEINGIBV2UHBtxAikiFrxT8L6YA5tJKWxD2y8I5ZMQ+/34oYKUuEoVtV6H5qDHdTtJgTjNO73ra3QM6QpCdLCNwCiood7Dniw0gL3buvC1QoDZj1ODcgM+nUyDsqrPC1fp7yme4eIcD2EkLItgWRac1hU3NzfYNi2s19pSNCgOwPMKz4rxqnQ4JM08oUGxMQ7hQVTjMKb0u1wE3tkMTJZcy32nXFvqfc19mBrN0KCIKSUnzoDCNNJsi7inPTxrGj8P5a/3SX8uDaEmGqDBoCFarI2s7ldrFvUB0xoh1sSoAAXqYLlCtSeL7FjGrkyHCYOadfsNRidAaN109KQAPaOPBUjWnsWeNi0JeZ6T+K0z1zT7TP5nlOuggpIERQy0AXVIJciwyCnLR1JNxYIEhJofqKtTdCRHrAKoboB0PEeE808hyqR7baCHoFTDsON6B4ASz0YEaYwxWvrv+NhJ5jvSa0i5Rl4/gJtQrIxUfCu98VwoVF7ZClB5xvZcAxQv9CUsmtyrL5ELhSWJghhj9/O1cFyGfwlnevsR53UtrjQEywLYCtL7RpKkIrUYSm6NsWDJbKVQoNDFHKvofj8JZ9KhiovzdcrHGNjHbtqLoSYtWbA0QN1b3GZu/RrzS+F2JqsK/kRIDUuYF5AbNeC+mRAc/IgbFSgIt6oANV+Kbhr3gXcGVYCK9cTXrxizYVeVWg6O1KD4eZfA8EntqDmpzrX1/dqxC/+ee449dV9cyoikTenH4ea14WCSAXQGRgP6ruCkd8C9+9nym6gXnzJZNDA1tJtbtEe3WG5vsNzeYrk5gddF0+izxxsenyMR6ZTqXubvAHjkImRoUqh1XVWLcnODu7s7TaLn0r1kKGRt85+Cp5lSXYWemM2jeYCDRSak2UKy4xpacsCZ1zATqRsHk8SG0OBSo3g04MC+ZwXqLEXva9kBtwkBRZip4ZrhECliOVyMrpjkqi4OFkUnDhyyP6696ZTj634xMZ9BP0zOFU2B7xE9Gy02lqyRY5ROl29kc9oD5DxmDiadq+pClGDC/UzcZONz6zPnNM6BqPuizUas1OS6IcdXTDr7y1BtBMiEgdKPWCPW+zE0j84YDKbM9uv0zs/z+7pfCNnaoOEmHgMszGpGJ78/LEcRAljUpR4gCKr9HmwpOcQ0c645ioxxMbAFROUxn5H4NwBcmuRjrbkWy8f2laf/3vZcAxSXloKIBuSb0aQz7O5qXybL4ijBLHV9ZAz6tneczcQjIIAbmJXZho07tBmpeVhljQyo7iEdEQFAInT/jLLI5UDE2DaMWKSAAay9a7+WZcEK3aAklOYckeL1PSC9BzCJ9yC9Oloe6eSAK9IaUwIz30xH+yQV5jBih2RSN1hxKl/KwUxEjBj4PLjHuB7joUApkgWFZJnE4VnAwhGI3AdCngakvKpmPN5gqxJL85taliVNce6kPBi0ePl4BSb+PvaO/dwgtAO0gYRBvIDttd7cYn30COvtI6y3t2gGUFSDQoWIOdHJuQSAauIhF92qZO3OuAawlmXB6XTCvu84nU5mbl2w7wqoQ/oVCem0tlm7cp2kTXta2W2olpVgVymOyn1SphOR6Z1N48dtJqdHM6AAsef7rgJM7246dQHFQYrYXnazalX7OyjJEGZNOKk5V/ahmZkh0KhCcZ9KR1SqLRVo9BK2EYISAgT5+U6DHGgJgG4A5YxlNKxWvWYQY5clzvysuMqraXLfvOcxzWBl+9GSn5lyEZ7aPtaqM+4r4LrS4DRl+HrQ78WBDwLbgNjMpxbhQnVekOtEryOqlRpDTSxALPKgfUFDXbC0HFmjmwAgatpjBg8rItuSJ0DcI9Ef70DT7bivWTEegrK23U3ABaaC3AsIOba6V3wsU/NE9Rxy/gaUbfjU7Xos4BPav/yX/xLf8i3fgne/+90gIvzET/zE9L2I4Pu///vx5V/+5Xj06BE+8IEP4Gd/9menc371V38V3/md34kv/uIvxtvf/nZ8z/d8Dz7zmc88a1eCqUYYGo77ytCpM9WRtumab8RVsGpfFuylmuO27VbjoKP3otYNLcpQ89G2YzvvmqhtjED2vmFcMzPspQl4tLiTOuWq38tu2pt87XZ99Yk5bxvu7s64u7vDtm3o+55mGRuL3rUfo+8YXetwzMd2jF1ffd8yhXffSx8t5t4Ir26g6kRrQIdsDMlH2uYFsLkpm5Bg2RYtTNReglTTD4hqm0Q0MkPGBUA5tvuAyn2akfu0KU+jXbn299MAGAdok+rWI2GWZXqxvS+nFcvphOXmVoHGo0dYbh9hub1Fu7kB35xApxVYVuC0gm9OaLe3WN72COvbHmF9dIv10S2WRzdYbk5orkEpxMk6pz0sDJ2uHEPpvxMefw4HKP5ygOJalNokxD3/+8qc4n56lusl86FEQjopF4j+51XF9riHcU5JwIKh+byzgUkEA+0GJnyfphal0ISha7ibA20v31XBxktGxJ7vCjasBEv0JwXjZCYChCB1d96xbfb7kbQQRFGCQnOjWDSP7GhjwzrucBp3WMcZbezqtPtGAxNrs3ZpFhzTpJbmzNg/zcNlzfcPDsfqYpBpXejytygcu3cKmt38FbvRP8ncIK59ITcX8RScMb2WhtY0um3xumHryV4rmlXcbusJbVnBbQU3FyA846ubY70WjqVKaIvmbWHVluqLUWuM+doFjKftO7bzhrPxjrvHj/V1d4fzWXnJZsKE84AMs6/J2hDgysfUMJ2Z11LjNeOcCbY8dXtmDcqLL76Ir/3ar8Wf/JN/Et/2bd928f1f+2t/DT/yIz+Cv/f3/h6+6qu+Cn/5L/9lfNM3fRP+43/8j7i9vQUAfOd3fid++Zd/Gf/sn/0zbNuG7/7u78b3fu/34sd//MefqS+pKr3OvFKDgpBa1GfCQjLriX49uD00nWSJG5ZlWErpA7HpA31XUDBGx9LUYQ2AhTciiJuq7l29aEziQKhd2gqEDZVmB4smpjtvOG9nTaVPpJV9x4CIZ/DMzRX1KUbm5KYq4QEh9QWIKJuawCaV+uIck+YkVM4hYRfigrhNOSeZWxwrWpQkxGKEQX/sUsJR+gWScLhmpX5XtS313GvA5drfT3veq2neD08rrevDTHejw2szRVikacXQO6TvQNsxiNCNDDE1tPUGy2JE8EaBzHJ7i+V0C14baGmgZiHGrkGZ9oxqTfSwfy5aEz8OxJz7MyzrCkDNruu6TgBFncgvNSavZghTe2IhxkSRj0Gm3ZTXn+4jBpw97EZSYwI2PZITUyY1nTnLc4Y5sgaOlGvIYV9NdYpCe+KmZInid6o5SRNPFA0U04x6CucAh+bYXjWeRJEplDnBsCef1H5pHSb2pG22fnZqWPgElg6Gm4iu5M15PZsclmKZh0CIILhfm2of2faHh/6mOSboXIATF4u8SB6lecSERdVH2N4gAhqBmqBZJWBDRgW853wQ6d6I7gKWq6TQLX+ywq88lJ2oYwxSXxOBVmpgUQddFvXHs0fSeD1bW5R0O8Rt0bFwXyJPlRHA1c8ky7q7qAa0iUBaJp8juxdAmOq9ARPgEEp+FoILCuVIZcwz7/lnBijf/M3fjG/+5m+++p2I4Id/+Ifxl/7SX8If/aN/FADw9//+38c73/lO/MRP/AS+/du/Hf/pP/0n/ORP/iT+zb/5N/iGb/gGAMDf+Bt/A3/kj/wR/PW//tfx7ne/+6n7UgcimE/9Hpd/BNo7SLEz47omFV+OrBxfzphjX9HFOeFUZeGBApfanEklWvVnHFZwKglaSe42IQAnTMnoJQCJZwOcVdHT/StJCsScPiZuJ4Xfqay6HK7CzEy9TJZFly2pUW4W0RTSMtSZTDgWuD9ExTM+PvdpNY7gxN+Pryf5njzp82vdpjXYGEtbMBYFKP7cFABlAINVXcyMRqqaX6lZQbAF63rCst5gXW+wnE5YTzdopxP4ZJqTUi7d5yl8nyo1gRG4OFikHyN8mdjNQAozxEw9Dk785drDq+2K9uTqaeWDkEfvEHZRJqvOshlN4/tt2rYVjBuAAATE7j81Yi0TjNsbgGwiGMuC1hes64IxVnSmVM1bZNywSIlLnxMD3WXtpZSZpl4Ph53AvNGMzMIRk2F7JFO8uz+GnxK8va5/CJg6Fuw4oaFjx44di3R02QFqyOxNr9/6n9qB/h6+ms7JyBsdQzXDJEiJSwRjLOY6BxdQitfHwLZv2LYtNd/QPbcsDVgX1TxRMX/NMDiAfJKKgoxFUpMBwE2jsSaGhGkyOu3awIAT2QSqOfS4GanrGSbIBABxDVyaGQNAT1dW+pyvKsi5BjE2fD4hOf/y5KLF/7HcJQWaZ2+vqQ/Kz//8z+OTn/wkPvCBD8SxL/mSL8H73vc+fOITn8C3f/u34xOf+ATe/va3BzgBgA984ANgZvz0T/80/tgf+2MX1727u8Pd3V38/cILLwAw4l1qUFSQElNe12uRQqKSZX2ZrTCTvhHcux7+e2AebEeFwUlsMZJvbluAUaY6y1wDsPhDU10607ZJJgFYGMM+p+OrS2QoELW+tBJuIFukdOdgRUyyCAATz+Or0EGMShVVUqyb3jMlxhaUSIgf4+KF8HwDCNkzkmt+oB7yttDFVnRlSoARf5sHBxpH80E8K/I3E5Ow7+vv7jMFvZ6tAlANiW+QNrCMhrGs6ETG/J1kWYjgYPWIZnVopLaAV1UPrw5OTG3c1hOWZdX8J1OWKqDOmQMO14po/wjVUVa/mUgtfN15RtwmEqCkalF677GnJoA5Ect53uLY9BfZ/jBwQsAuwA5gH5orZDCCEBeZNZ4zVNNFY3gJXj2MWvvZbLQESLMnBH3n0HoNywsECPooIb4iub18TgGdD2FNkC6a+VSaHosfkKdRr867OQ9O/4igQQLMaM2Yrws7oeWpQ6K1tBbqAO0GUDZs2NBFNWEDjP4GOsyqD0neb5p3OtIcpye5NnNkJjZfvqP8z643RNDN9HF39xij92nPnU4rME5WaNCBX2G8jgSF642ir/qWII+8hyHNqiPsGK4FDWaSFwqBVg/EiiaKWt7iCdckxUxlC/p5iAqF7Bqb4DESPmTuC+c+auTZceGyrdHP6KPvCROsjT5RobdVPCeTNl93DcqT2ic/+UkAwDvf+c7p+Dvf+c747pOf/CR+42/8jXMnlgXveMc74pxj+9jHPoaPfvSjF8d1UXt9lgQhVAZQ3w3h+uIsKbmzkmWzkF3J74gj+2yVcnz5Vw1CvlD8LGDo2BeZ2/8zFbjmslDm7CBlWPVa969hi2SZ0nAXwAB7Su1TSguhWUiopIDDtSnxrW9yXz12XSOubmoJMEMla6Obq2ItqtNy4jUqxf0OGhQqdvZ4N5szPK+AqdhFAqA4uKig9BglUhnOtQiSGLUnaFOOmpnXArQc+1FBlq6BBYuIMTBJAiwDwpaoTQbgNuh1xegjAcpJtSha3ydt2gdqnbIZzeDEtSYq/FWQgvpD0/ja75nRRDRxm2lRKkjJpG2HsXjCuJQRms4M8w7UxLODsAPqrwRJ3w1/PPiSdZDiUl+uERUATBgxoq1+G7bO7H7agRFbvrcWvl292w4bDKEe/myuudC5NCrEzp8ITTTEV1g/a0ZrBzUUibp8vFKDok6iDlK09kxDW1Qzu+873K/OS3rUeW8KQcDY0WnHBs2NskPzooCsoOph3ubZeO1aaFhxZS0YXSU7L6R8d2IN+lVBafL3S22VPZPlczqfz3j8+DH6vhkVNXPx2NEgWJumzXeA6V0SS6jmcwtBFBjUc7zfNPWSUOZEPGGf/zb5VIVZCbV9Bi1VBGBak9zYmn4+tSgcfEXN5kNEhR1b687/Wlu0QGDhIWoeygjXEK4pzelKt0hzs8iIZG7Rq4K7npWCPhdRPB/5yEfw4Q9/OP5+4YUX8J73vKcwqFy03uZ9lQu0pjzP2iGeZVCl+wz/1A3j9RNqtVAhzZviWpYmmoRtLpuNWJSVQ0S6cWMQF6oCoExvmmmCiVuGqhQUbdOVBQ5yNSY8O31IjfEOTCDHa3LAAIeUIokVzkQdH5pcscpoezHBJA6BntnD8QCxHBDD8gqIsEm/AIZ57QcSV/8GB5AVNFRAcc2cc5+JJ9fK/eDk+Pm1bn7tLLjnBIRS5BWBJr7TVNUQNUvw4k5sgmZOeKs54nlae5gEnjPoJrr8N6O0EpDkfBaQAsnaQk4IMYOsatp5kqPsq2mOo0T0KboDFXt5bpR0ehwWQcSoIdNjFMd6BymWZ8TjBrg4UVIAfx8/XZM7E2jTkXX/MgdGwWQlxzqkc9+brKHSLHTI9MnmDOpUvbxiTpwWKX1aLGnb0nR9DGJ0pM8YYiu7BkHABlK2yIuiKfCHfeOmnjfCGyW1sQAsfDbuWuhHNe+4fw2BdG/EtSotqsi8omzL59Q79u2M890dtu0M911Rs+oNFmac1gWja16fqWC4X4mhk1kunySjcGebhAss7kx/mN+izVFDXif8m9hN9brW2fxc4nYBUMjJuJJTo6Oa52+E4EZFUFcNSs0wrWTIOgAXakUkhBOlPwiNeOTocgBUQUoZt6dtrylAede73gUA+NSnPoUv//Ivj+Of+tSn8HVf93Vxzq/8yq9Mv9v3Hb/6q78avz+2m5sb3NzcXBx31F0zYwIhyKdugJKIRlZYR6rGsDQToS6k09LQ1xXjdEIfQyXBZcG6NBAIDZp7oFHDIKAzYVgynHVZceKGxSRTNls0WYw7Cdtn7Re7XEQIsCPQbEPaJwVL3kd/De//NOMEgEGsRC7rrljSCitcFlQeSK0Icuz8eACRctxb5H2gOaW9Z491M46n0scYAcYmGCbI4lMWSVErwwoyUdu2bcHo1si8Wq52+Owal6PPzn1A5Y1u1VQFeJI/NffoFBV7rgyAmklssKx4eS22iIHmJh2l5kgCCZtnio/VIdZOqL1D5kDxS8wEPu5NhBHZcZVRuvZkXdcsP8EcczK1e7Unx/Oia/GnOBgPyU6jYuTujH3vWcQwIiL8dvM9HcA781efFF393Fowz0x+aNF3Q3A+b7i7u7OIOsvE7OCNCeTgsjyI3k6vnwCIAnyTCR4S0+zfmzqeyV4ZwQKi8H3JVABIGlh2MvlDjw7W+sb22tBYdStEb5z86gUnA+Ch7GfXRITAk1pbF45UaSC5P0pqhElTNhxgWySXRTFu2xnb+YxITS8KVdfWsK0Ne2NIczoM096YuYQ8JQKFFkTzQPneywSCvYaR50wY87bPlgZjsJYN4dbg2nMaupZESe9ES2fa6qOCIlTbsFhJBY1OUv7ZCm/09aImHQltih6m6Q4KlCzyjDqo28t8sUBDc/kYyXhWivuarsCv+qqvwrve9S58/OMfD0Dywgsv4Kd/+qfxp//0nwYAvP/978enP/1p/MzP/Ay+/uu/HgDwL/7Fv8AYA+973/ue6X7ubeyb0yUY9+VIQuQErGn57QAoztKBBjI7I2G0BllXrQA5RhRDW5p6c3cCOgELEUbz4oJKWFprWFvDAkKzcENPT64ZQkMgyM1FSUS0N64dsIXjBamYo4++VOIpqhZFLETN0u+7Dww5MLFNIy6hB7JPlBynQsqilPxXRAsQFsFBAQ2FFGJWf1UnUtc5iQR2FMjHN+7oHT2kA621AkFIgefzOQDKbuHVx5YScQKSCJ2zYw4MrrWqdXkjAEy9lxIStqKSzgwBzymjjoBZwda1ZUQUJp3QVoTU5pJUYU8y5yJxIKLM+xKYBICxxHkhRXpjVonJw6ZNGqvOss32zkUTwfWZiBPKW57rsllUUw1n98wXpJJsrdO1mBaU493gMdj2AnU16zK74GBrVR1RSsblBgGw7x13dxteeukxxugWUeImTv2twsSasO6AyXw/lzVQDa++IV0gU3MOh/DCZg4U8UigYcneDmNrDN64D3zfEzqIDJxgs7DVBZkE4GI2XvNGxiArOAnHz0B7blqe198csWNMVSofEMtxxRB0DKst1vuOvttr27Bv50xqKaoBPi8N22nBtjTIaDrWzFrE0ekbWcZiNm5iSCAdjS0kvKSQCMBJmYOkBmxoGQQxbiBW68yc5AeFNieKtxodlxBCcsx83tUfpoxvZNXlyaIQ4NBB7QQOcz5EXBC05ILQqtQ0DKSw8Z8hKlAFo3v69swA5TOf+Qx+7ud+Lv7++Z//efz7f//v8Y53vANf8RVfgT/7Z/8s/upf/av4zb/5N0eY8bvf/W5867d+KwDga77ma/CH//Afxp/6U38Kf/tv/21s24YPfehD+PZv//ZniuABzFzTZvNOImY9J4QuYkue0+AVLtm0HARdW800FrIsIBE0Q76NPa5dN1AnVX0NJoxBkQ0y/FdaQyOy0FBTH5sDUYCToQJuEyO7ZSH5wojFY0RlWKx7M0QbZcIKGo/qwSQxLmSaEw2ha5CipvZNPUxVl+YkV+0SLlmIAMNBjD9TWlpVQslaFa6OdActN6epA6DljLBEVVHfxBkQgG6T6QCFiAJ0XHWslMw7U8HKMULqldrrAVKOfi1+bHL6ta+GMafhfii+Zv09CFrRJHoOhQAmAEChkr3ylLg07cx/x88KOJEJuEJNSWMEQKk+KA6agki69u5pNSfH/sYnezbTIHBjQDSn0Lbt2M+bApRidnKwtCwNrUEdFIUs4kH3DA+XWBPI+F6S1jGGOv5CCHsfeHx3xosvvQxgYF0YqwEIN784aBldHRozwkf773Prae0jIkUOz3nIvaFmHaUHXj9MgX7J9RTzmUxmFkjUWMamPVmwYadFhRsJFcXr3vy5Lp2Ly7p17YVL9QFMnO4UsFIHz3nCsDTvdrlh/kP7vmHfNmznDdIzwWUjwunUcL5bcbKInsZsfkLq4Dygjq5gUaagagnVhEfEDmHrHefzhvOmEUMAoXELfuFrM5x/HewAYGkQ7gpaPOBgDKQpS+cxaZsLJvZ7ZC6ViDD1c/ng7mBzoD4yIwC686N5PVRhULkK+h5aFG4dNNR5vgz7M7VnBij/9t/+W/z+3//742/3Dfmu7/ou/NiP/Rj+wl/4C3jxxRfxvd/7vfj0pz+Nb/zGb8RP/uRPRg4UAPgH/+Af4EMf+hD+4B/8g2BmfPCDH8SP/MiPPHvvXbsQEiNCvemp7g2CBxGDmErUiDZZcbYmggWCBcXpbF1U2mSOxQQowxjEUH/F4rGPRKo+Gw0KfhzUTMEUAXTZ/D9osrG6hBychgmDDV0LsDChuYRWJCJvZWnZtVQFjlBO2gIjc740piSxwQn30aghCEnFQRf789t1CXpdJYFZF4TRwCyxcB2UdNsYZhiC029H9OfzOcZm27bIUupq8cr4UlqdAYrXMHqzzTwXDrhVguZqz/bnGqbK5zinmizDhk9JmOYb2vq0/WAXvwAn5ML1BFLyGgpKr2MdJ3TVUdZfUdASCchei+YSYQyYWAmMbcPd+QwAWDwR4tKxrpqnJX5VgBOQ2WKl0A9iN6umZspBtydq3LY9hByJhJwUjJPJvX+sm/NW9R8kUIGEZZbFyyJ4tMVihQL1xWQgvo8UBILusQkrPmeOWtOXgUYHU0cbOxrtaNLBcqk9eT0bFyfZyLU0XAth/a78cepa0aDYqX5c9z8waKTjPakw1vctElaO0RWc9I7R1cG47xv6tqFvZ+xbU6rUGpLqMiANQPpkuOQ54Mn2dE2dtx2P7+40yeb5DCJNbri0BeuymlaTTNNoD2EghSR9RYQ5Coi6RiMttKl3U6GkjEXwEx/LNPP4YnQ6W82EF/tUUMKXEdnZ972b35JltubN/OAaRlP++mq2/DMDlN/3+37fE4kLEeEHf/AH8YM/+IP3nvOOd7zjmZOy3d+SUibyntWbbnrwOgYNCmDaGGgQLIR4rW2gjYFFRjgIqbrWkDJ0QQzFOoGQ9QsUIqOLYBCrSYhNSg7E6hRDiqRbNCjBASierTlwWho6E5bWcGJgFcEyMpMsLDU69R08utos4REhJln7gkRm5exCpk2JEY3nBcri9m8kJemMlEJIf06ca3L9Gnw2/BpCYYYCu8SX4MwBxcsvvxxEzLOVnk4nlTwKk5mclMt6OIKUt1QjgDykndT5WJJDwcPdPXIqI818Xfvzl2e3vySX2QTkKwihCkhJf11DjC+aFzYr1wQQ/aog5T4NSvbj2VsqtVWL10dHxw4ee9R0CVOJCProkD3HRaDMozVn/Iz6bfVbUMlYK6Lve8d5U0nYi9s1y6JLJFiaOwyT7UWPyhsT+PEniH0YQk5G7niBCBBfmsvYNIyi/loBdE2z1kgNBBxaEkdE/nz+USVxdWofWDDQSOmi6zDdWf/1VKd42gcR9dsbpqINjUAlsTZ6VMcRUAbtJo+iiVH/D8IuGjHj87FvZ4y+Qys9C5ZmmlvS6r8L6/kydvT9rEUBpAHCkNHQRoO0DpGWQrKZ1IdnCd4Htj5wd3fGyy8/xsuPH+Pu7ozWGk6nG9ycbjBuNO+RPv8CdTiAPXf6SAq5eVK1HFrEUMHnGEpP2fOq+J40PyaMkes+cmmpqWnvO7Blio0QXB0kTtGjgCccFhF9vq1j2zq6MIYwFmgBWzG3A24L2lj0GZ5xvz8XUTxPbAf0OGtPUkwxLApiQhOgiaD1gTYUsDQAKwlWQx3poOUaDwnV6zC1wQAQWZFs85OpE5wYa16GphMG+87BSQEmVVuAkHSKOls03G1bGlZjtrwwFiINGbRKuDAbuQytjitdK96GCtSvTYbu4eaX8hxS+5WESejQNzuXzDdB91OOk+JBIyCuCXGyZ4XegDTnwCTHlPwM9ReA4gyuOmGeTqdQe9+XF+VaPhSdjkui+0ZqVpz0+x/hmwPWABwx9mB9cg3b0fQwqzQSLF5riqONzHsIcYhT15lRDpkzDGdbs/nItTtHgFJBCoCLeXjldh9Qcufqjj42sGgFcsAIvkW7aZmKPQQX5dsCWRdgociw67eakq9BpUb3bdms3MS27xAAS2u4vb0BRH1X2BC6DGAfKiyI9ImuOFaINO5FYPB0BGxRWMTmz7OuWgqhOQMzYWLSmLiWTR9F146bNz0vijGtoJEa0aPCmoIUNtBStShyZV28Vs2BLQQY0MSNGVWIuRe+J+w/H4swT1eeMLx8Bow/dMBeClA2QIYGSDQFATLUj2Rt6giLsWPshC4jwAlGg0jT/EXSLZQ/6djeB87nHedtx915x8uPH+PFl17GSy+9jMeP77CuKx49eoTxaAdE/SK1HtcRDhY/M1INChoDwqDW3KIUwlypVRwh4gks0ifPSz0IAGxJI5nTjzPGPYBgAdKmaTlHCZaOPpSm9wH0YbSBGW05YaxamfvzCqAUA0YZvIqe9TgDtnn1VyxqXvDUzg1m3qGB1UxAMeXmu6ISjU2aoh1YklWYwImQ/F0DYi935nOAElIs+WKw/ptmwyUHlw7FNldnwtrUWtyHZgZltkgg0RoSsPT2pLXXI28GAelzY/d1zcaAJrsKPCfppRIMlGIkte8mbYtIbBLDFnqWEQdyLZM9r4OwAYR5SFxqIQnirCBOmQYfAAqAqe4LoFE9lQFGP69oUJ6dOb6OjcxXCYUglbVCzDFPqsmjmRFxASj2SIW0IIAKpS+RMi2/l8SaI3fz90KAxcRzTIEfv4dKbA5SJh+JgwYlHAAPGpRnB4Mz03QNyY5dAcrwqDhNZRWRN16qAQUg2XlTn5zRyTCpVPemA5TztuF8Ppuzo4ZWE25Mc6MCASyv0egjSk44rSBy4eOgRQGMwSHzU5jjswIUNQmwli7Xe1TgDRhgldC0kdGS3neMXZPZHZ8z0gJgYCHVJmielMr8J0I3wd/XYisRmf+f0TrXJt53bZHiGwXfzw5ObEzJwmuHmrG1oOIOGTswdvTtrMnZZGhKhMZKvIYKWksDmAak7xiboEsHRMEJApzoO/FIWg/WGkl3Zzx+fMbLjze89NJL+MyLL+HFF1/Cyy8/xunmhLF3KwbZsKwLTntq/6rgEmY/1iguiJmaRKz4tG5g5yUERA35gJgjBXiNPnMfPr3HELUoVPeEEHP8ENUoMfUZ3DYtqns+b5qqf6hiZwxlkq0t6OtmDuQcz/e07bkGKLpxF1MjzTZ5z7DoWoOQ8MFolryGRdNbE0FNh12nOYpPuS3YQM3lhtBG5Z9kIs5fC1CJyfdtX4iSf3YCGiBFGcuAOuX2hdCpqeTEHC7tnoQHRhzJbcj27N4vT57m9x72YgNijYCdXR4xYBXqEAdcqlIUEku7rAtZfXuMSHrNCxgQs3tH6NuV+aze+prDw65sx+/u7kIzVQvSOYGuDPA+n5T7AEpVkb95zZi/jSkbOKk6ofBNchs0EvCivFU2lFf38ZcAlILUoEQYYjCiKr+6+ce0LjAGV8BJJajV1FPNO0fzW31/2uaEePi+MII5YCZOE1sau+nRiSrMaT1T72s/d3Bj9EM/m0DT4JOAyMOKR2hlnKnSuqAxaXRPJ4yxm0OsZSu1uju+B3WOYcS+JCtEHUPVnHBrWkTSEkqGgCMwLaSNn1SgYNdx5jNMkxDPb/eLY6rlYdZCgiwdLJpZhmVgVHPB67Q93DQF0fDuQfNinta0A3HxukWSDqtGl1zYEzsOUfooXau7y9ijIB6Rmc/BIdESRI+R6iXMah4sn5ze2js1/aHSTKt4bY63m2VCv3v8GI8fP8bjxy9DRHCzntA3Ld4qfcQEOhjIwSkghYZp1kTr5sRJvSrEQghJAdjWXDXV2GB6NOQkLFAClKA5lD5aXkNq2zZsZy+UK+AuaF2wb9of5oa2nNDWFbJImEWftj3XAOXR7Q2+4G1v0z3rEloBF1OFS08BIqoJ0aXEZk9Tp8zd0LbWXlDixICGCNeNIi5hFcJqxFs3mqZ791j6cJaLuRf/gPTfQDAJl658WXlBqMFQ51xyjYTXZtEts0PAYE35bWYX3Uhsz+RAxZg2pa2xA+hEuvAcJAGh5QmAZWPl9+gWzZNjjog8cmlfqEGIMew9AJgxjsjyKS7jj4jXh0ubIpGRdNs2PH78OJwvHXjA5waIv6tJwRlT730CNm92K3wFkAQpgPWfPfT8CHCLdHNox2OpT5HpfglB0o8oIVL5/QROJBK2HYF6ZfD3ARO9XoKTe7+78lz6PWkUnYWiax2ezJtDoMhwrEtQ85hwAaG+HnxNCVTbsa4jIsra0sJ5k0jXGQFoS8PNzQ1aY/TeNP8J7+g70M1ALzBTZ8onqskI5mACgJSJkHzmIeaHZAzHnS37EFAfMSnmgqtsOMCOZI2g6kPg4HyMfImycHUS3aAUbwNhB9MO5q7U0nPq+NpLEvaaNKKjgOKCJTIhYJDJpL9soFSff2gwmdPHVhi0YyyDF5qe3n06GGgt95+QgjPyBJ4uDCCdSseAMGlOESbQcGO2pXUYo0SIEhoxFtYUFPuy4LQsWJcFS2vxqvm5ZhHjsBtDozILuf7uWWqrZk3iivrb1lxQtR1DCFBUrRCIeTFB2PaG+2M5ONnOG3oXYB/gtoP4TvMRiV9X0NYT7h6/9Ezr4rkGKDc3CVCqVKlQz0i8pAmCyZJJGSRhsfBb0mJju1EDHupBz8MAx+QFjyy65BAVSKRP7utiWhqiAAe6JdIXhOLvSWEKF4eUdqUqWqUCi+SB2UuZMYhMC0JgMHYDNX6/a++QlCyGiF5DBINbqsFd0+KaD/u7iwKUnQS7dbyRZdsl16RQbM5BbACFoq9aT8XyB4jqcTq5GGh6Vn9os6s6+iaiACf6KDoJl0UfLwHKfSHH9bw3VYtSQAoAQ5M670ARrHwBYV6CF2w9QHGRzi/O9Ogd5QgUjLOAkvoOskyfpo8JCbuAFUqAdQ2g1HYVpGTX83kLcBmikXHdtAgDnrUjTayMlPyY3c+j2ND7wA5dU0MyPHcZA+swJ3r2RIme5RRqZrlR7VDfN+xM2DddpyJaZZooWKHOJTlfG6HByNIXTgkMXItG7jh4cZ+RPgDqzoYSqyrg0rwcblbqe8dueT7cZ0OXjNGWWswQA+h7gB0iByga3ZO04GJWXrtGFGv7eIcA0YVvu5AYGVKH0RCoNiT8bBKBT1c0imGCVFMNCDkAKcDCtfFwTabTfnWeVjCSehV/FkeW9ToORBycrE2Tf66tZU6bGIfcU/7YPucC1zDbrUDQQg9QoIZhIDbdA0LChmqqGFQcYvW5xMZVIJY6o/owaH9610SIns/lfDYNyrZh381ZwJKD7ns3nqg9X9YTzo9ffqZl8VwDlEe3J3zhFzxKqm1E24mPho25/4UCB4aZa9yJxKSxDpidbhg48YRYtrlNjQ0nbpKMMRqRBaK4CcmccUXDjRtg2pvcIkYSgoBAEgt7E2dWgWQLw4cy/S5+bS0EJqMpGCMEKGNTczqRCq2FuPrPiL/1bAIpsI1AClC2PrCRYLfF38gTR9kYw4EgWV/9RdhFwQ2ZRJnJhjxyasALcFVpqQKUx48fA8CUTdbNCdWsUEFMeq/P0TwuUb+55p3SCkGg6dhn0y6ZS9J7BxN6ji/16ddSwEqw0uvNx/HoE3SfpsQ/P+34h6QoBpahmhQH0yoReg6hookALLqirAHbA3vv2FvHMjpW01QsY8k08p67hzSKjmjFsjRsW5qSITuks9ET6yUlmFBfGcG+d4iQWmhbC5+3fLaU9B2YDgFoCDppNEbWH3OaRy7YY++ijovnM7azOoGyPUNjB+JeRNAFBHfiJQBnEKnDMUtXwFUzMr4OzU0JLnFXU4MvNv/oMFWcnmkZaYgGumIM1zJTaE/EhJ0cU4JrEzQCpyESbZsWxeuNRc4snyURJVE0ACZNnBbiJooGRYwGG0BhxmoVy9fQoJgWhbVCuQt38YwhydY++9g0kLhxy5/NEtGNHCPfZaGJCu2mPYpp17IsggKw0Erb6AugwHfv2Pbd/E8sou28Yds7LMcdBoB12+x+ygnWdcXjlz+PNCindcXt7U1OgC/GMdBlRzcv/NTxxbRnNUcRlcKMkO2apUNVf1QkckpjjNs1A32GKFNAkCVUa0RYyReWIVSbfDaJJwCKVK1PPFU+MJFtJOsFpdkFALxQU7PuWKS+EjIJC6mWlQ/UnFKpb+S8c/EER363k+ct8HvpBl4sL42Dk5aYzkKz1ZTUoKrZ9A8YucWMQIXqMZ4uzTa9d2zbFsCiOmYCmJKD+XvNf1KdOI9al6Mm5ZUifj7b9kRGj8Jw4WCtfPkadGdWflSgop1Tac3WyGR+KoDnANRzPxYVc9FW3dde0dxWJWioiWdSbbs4EYRYYuRiIR7uJ90yh4Kwc7e8DsAyBEsfWBpjNJd+Gcy2ZkoWzuoArONZCX1lqi6Za8IsNx0pfeGgA00I1FiZV/ifFCboy4CKKcRS6rMAnknYNTaqgVXfBdeSOTbRaTQmLwQW9UNZxoZ1nDF4ASmR0vBZ1HF87faDaw1c+wV7v0wwKKjbQOfRAaABER8bGOErWgAymiZQKR9oWQlENMU8pB8ASgIo71/SDErQ4loK+xj5tBbVmoybk0aZMuHmdIOb04rTuljSwPSbrE/nd4gIPs5Ejs4jmHIPaJapHnuB0Wcq724ItmbE0IRrr31dqN/SiJQTYo/o4EQ1KDv2rQdo6XvPrODm99TMyRtE2NYVd59PAIUtW+WRCHYBhAmj+8JOm5racj0hj6WyB5uNUGt2tPCgb1OOCQ+T8vjwTGSTaL8WGmxMWEvuLGXAhky7RdoYkXBw4hoOdgBRF6ypznRjcQAUtXv6pYoTl/WMiqRBMkwdWmTnugiBVNkXqUC/18+pmSE0Q0PMHCXJGaY9MvqgPjQjmMgZ6p1XKzMP16YEk6SUGg7AwKXfyENRwot77xF+XENc/RyP9mEL66sOna+kTbmXgboki0pa7m+5Kp/8CzfpTeyNyud6MThkOIic16+c1zicmqti/uTEvmYYhhM+E8UyKs3mc3hGU88QXDR2FwD8vs6WvhTGmCZHW/9l1ce4Bl2gCKV3aTHMRaKAxB3S9yFoAwpOlo51aZClQZYBjAZumjvFIxmG10ry4XTi3s0ZdaQpmIlATcttKPBrGkZMKsGTAxajP21Rk4BnsA6NByQZJafju+fHAYn6xiwNvWvkhJsqlL/lOpIcTZAJC4vsOMkZkMfgwTiz+YQJY7czxzOs96dqJuQMJPATzqvHJ/dLC1Shz43hPkcFPPhnGFglzc+huUxMBLJcHao16aDhn0vW7wJMasHYWqEdhV6571NrDasocPRrMRNu1gXresKj2xNOpwQoISxU8F/nVhhoThN8UPS52JNwWqI48VIn3E1LOMKk74xh0oTaVyK5ZkfhcW4xcNOOv9SnL3P8DHciHwN9P2M7P8bjl5oJhgsev/ziMy2L5xqgLK1Ztlfz5B9JfDy6JKSZqIujhHPv+lJNBqExFKhwQ6NmmWP1lR7U5kw3hmaTtRDgbFTACVuJblXzal4BU6N2AfZd7b6GNC2hQZpjgsyWG8TGNN0IUb67TTIyDJopSVwzYan2C0AxoWLqv615BDCQSojUxNNsy7GNmROXeIk6GatZd4ZNw/qEcNzTSIwuMvFeQvbh4CKmILSksT8e99woDlQcjLTWQpNSAYprXio4eT01J69I1IMnz4xEP8y+JPG50Kwcr2dnH/nLvDdJkrIKl661AChHf5+reWgk9s/TDjGVT7mmyouyD359NQWRZfYsfRCExLd37R+PAR4C3tVhdvQFobcWsYyYClQ0zLdD4+n9fhKRQkng/RkpNSG2Z90B1UOK9ZWg2p0nZWT+ijE0pFUlakviBXWK100n6PuOfWG0vZm2pvjoueBTSUtAvQHBDsgZNFpGHQlrPieoTxl5ELLbCT7LFmAA5rdmBfOcgqjPk59UXq5jIHUk10McdMOv6YEKiHQGLYrYsSdIDLDCIEtBEeAESVt9LJnTEbzSKR9vLS2rmmYXXJelqZN+W3C6ucHptJoPCllKrQOIEJX4WBiD3VyZgoU6CpvYSLYOqQHUrexFDyBNRnNTNDI6DHfGTmHRo72ygKvWd9r7wL5180NJh/JY773seRFsd491n/UN1Jr+/Qzt+QYoSwsNyugjHDDFnFudmrpZxcQo7L1rbYTRVWJiKxYIDnDS2HMOpFSjbtNqBx5WWtoZuiuT1Q+jhWpYgYLmF4iETGNA9h2ybxguaXUnEAZQIhFWgSgFkJCBlARPWgmXF3sHKck2wkqeXdZCCnPjAhNKib91E4QHN4xpCTBI4/GjArRJKmwSTIZwJ9DSQaKw+Q/TJPUhodpWSkTRkSoFAZjAg/uSOFDxY9u24ebmBtu2RZ4UN+c4QPG/932fwEnNh3EMh32j/VMqyRdyglQYS+3O69C1CyBCBahQPQuhPXFG5dqSowYlo0aumHuegclVEBXaEyoaFBEIpY+FWH6GXjJg+hV83ag0KCB7cTPQchogzY1pGgZ9NZuPqaK2pPZojHx5WDIzGW3RfTvC0Mma72RdtSjpskbtr2Zguu+6ZzfXhDp4t/wY4aJpQKkvWm9ob6zVdh2k2MSSaV9T+eNmV8IiHSQb2lCtiwzSese0oLurKBGefsaeclJd22HgRMxs5XmCrOMAkaU+MHAirikhE25SmwIYXjF6qGVCHKC28DcJLQqKJsX9hwIPFVBYAzL8AYx26huFz08TRIHHdW0aIdYalmXFui5YltTewrQdKXjaFVkDIEKr6so0KCDVrPjizkoAN4A7qDV0o/s0NHusg/eol+bL19fqqCAlQ4o1pb0J95P2ZEzARP/WpKGACvT7dgdiRt+3Z1oWzzVAcSbm6K+7XcyKP+3bjt1izNX5ScKmSe6jIGoq4vUEXld9NTXvUGtWTyCBgECjfGRk8T8FvLYZmktDChQImlMAADAyW6XAHPxEHef6cMJTNCgEuAI5NRgmI5OFMpv0oM/CWNA0My6bjCOeWVc9u2Xs1m8J9WdssgPj8Q1fvKu076QJlaipKcodz7qNQzenNTLp0mULn6tdip1SUPxovOCdSZLLqhqOZb2YeWcURBTABJjNP571swIUPw/IBF2aprxIQ8dVVjQr97XPhlg/1W+dsQAW4ov6z9XrTeaeZ0YxqdGafF/8BjJ/nEx1Nv6bFUbzmkkKVJKhV0zyNGMw64USNGuZBsYOfTF1AxCmodPlaBoUmEBRklZZrZ4+LIOqBS9AAFj+i8EaNTSpfEQgoknYRhD/Gh02q8dJNFJOtZrK3DS3iWYF1bxLrqXVm7tWR8FdmokAmK+dOpZHOGxhnMpQtbCcwACTcTa3FgnsmpOZU0Do6OOMpRMWYSxYsNCCwaxJHUEYmiWsDNZn0QTzgoCBCqonEGA5TgIcmFCTP61O/Si0zcAOaVVjT20w90HgDvv6p2obuhj4Y7uOuCjpz17/LSBpMvmYJoYJrWltndYaqLkGRmyOTMPNUI2OXUMK7Y+7+XrUsMzUltsZ7MqYeFb/pviaOBDpHkRwBBxp8omlHzwHph1qYOcFfmGTOMfowK5rFUQY/fMoD4rAlSKmFdk2DXvaNgUp+45tt1TCcOdVHVyvCNyIwIunkdaXggvVpGQxsdnEwx6GpbvAekTgRlhYbYoL6+ZmsZBlz45YpIABjUDYJe37GpUoRh8lo4xgzCIWvoSmpLGCk5WAhYEFlDWHxBLSjQ62SpMpgKSkgfooFwwtQYswWyhy00VOZPZ9U6mPEURbRmaxZSM4+9DMtfuApRFvmlOACEQNra1opxPaegMmxrJ+5v414OrH3gNI+N/7vl/4oXjmWQBThE91qL3WjmYfPxYL8R7+L/cQ7iDp99H1OCENOa5Tu36rmX1nt+hw9JV+lQcEhy8Pz1+ZcGixbOy3knF12/eotOvahEsflOnC9/S29loCnAywRfLYuziw97wfiERbmRRNGXYAlNFtXxNYRtSlIenAIP1tSNTFVDK6JWZzkFL9WxK8DXPWH2Ooqp5JhVxS7Ylmh7VMnXtXN0dG3I9dOmcCoxVpVSVid8ZcwGVIk96Iqm7NIdiZfbm+r/9m+kwZgGxWr6ihYcUC1aAIEwZaAmSS+xfYU7YZ6OY0+6Vd8+NbTY4/tnf3Jcp3k698/AyceA6TYNqiAhJE140e85QSOmYijMVoGDndL2Odbw5UTOgiFRYFiLBkJjZwUtaImWFgNFNLf/izzTTgUnPjAAWRVqN8sEs4WEaC1JFmzr6XStg9wUksaZsIYjL+ySGcBuB1WjBYAZ4gIoRAmszwWdpzD1B8cnsfOG8b7u7ucD6fs/y4MeNwRjMHNNegoDWwAZPmtS68DHb4n6REMqFOcf+HClDU2dadZNvY0LpY9to9GLFHzJjFFxvIsiIizDIW/Idu0S8pGSTr0bBpQuOBEwQnAlYGViFo2Sk9m7tqUGjv4J7huhl5AO3XvaMdOiKAGdLyzAGtv6CaERgzsjwMo6uPj4NB0gWtbjcWedHMEMQcAGVZb7DePAJxw7LeXJ9/02rUJG0OTNx8UyN8HKAcw2CJ6KLqrgOVJ/mjiGvjrqzLV2rHcwLj1IsdKbGrTwqBoqs/yqs+qS/XfqWgqf4qqX/9bsYsqT0Zw/MkaJjr+XzGtm2azCw0KFcer9zyWp9p+iRBqH0ddaQGZbG9JBhIhz/SjK4D2VcTbMK3wyJZDAKg0VBn9WG5RYKxGz0wCXEcMpNOZrAqqfpzm9Pqwhae3xarN2Xn7Xuq7+2JFYBo3gzX/GkmT63Eu66qzWAq42RMBMaou6XdJyB8V9x0TczgxcxPejIwPAV+Cw1Kp4ZBDURLzFVqDz4LlBIg165KAIWq0IVy9THxwzk6ZR0WcDJEptIQenr52zKFJ4JR/QOJVTsfxniH+W40KMARFyINAyA1xEElp/uaT41PiycRDF4gFhUsGi5NxUHX6VAuhwl0kGnywKS5vzj5h4VOGo5KB3Yf52pqjwRsbjoPcFLOhz4rR/8Ra0vg+XvUujDIgUsx5xIgwTOfrj3XAAXIgfGSz+fzFgDFbeCenY/NyUhT4i/AsoCWtWhQzLTgGWlD5e8ESe/HohPHQUQcLasPClsZ9IWANty224upyDYDWQ4TALuoI58774lJgTs0rFfVqmnuEZi/iqjPxyKEnaEFmwY094gAq63qZjpu6iM0KLlRK/o/jK+DpgNAITsG057sQ7ANwd4F577Ha+8djQgLERbTXDm0AlRN3J35sWpQmBcsywnLeqOarPW6icffqw+Kg4tt20JlXaN4/HiN3vFaJzc3N+FEWyseXzjPXgEln02ruoTjdZX++MhLhB0DmM++1qGjJyReud8zaHAwEqqAIHLxGanFdHt1rVdz3rZJgzLMxvKqxi+FxWBOImRaEzKAovspk7INM++MkBZrfzOaz2zzQ9O7Nww0aK0UGYLupcsD1BtQOmhPvA5P7NKQKBMu+v7VMHxT9bP6Jkix67sGCABO62KRHqoRdN8rB+OAgInRm+5NMQk/GKQBmr5banew+TX483ipEAb6gMAKjsoAY0GjM5axotOCTiuYRtKDeKpXD1OSed4D1n0JSNIdCkHPrlGAoGoKyGim0tqgeUwQ8SggNsZpDD3DIcNJtDvwFASYc+zH1fwtzicqCNJ3tt9qptrKM4rflIMCByhMmcV8GmvEvMbnAYAFFO8anAEasdtSy4HwMwlH2H50eC2+Y/5S7ocoUgo1mzYbbx5qjhysvE6TxdjvKi15hvZcAxRnMoPHpA2gsmmkENVYyHrSBcqtG9oXKAV0NSQqyRSHB4nXxY9cq8PjzA9MIsphE2kKaRIIDwg4CJlagY3YQoFwkr3gF7Gf1NCTWVrd7l2ZiAPZCFMrUoMNVyGhc7cdkOhmUzssi+5xjcJBZOPdBnAewLnrq5FgJcJOgoVc0jAZmNXGq6+hC1zcoVgTtl04VD6huYnHwUSNKHHA8fjx46mAnfuorAaEHNBUbcnkh2LjGp/JzTA0j1/06RV7HaNs+Oc1bE9/sTrO05iXtTRrUJwJjyDkPXx/zrgzDcq+bcZQbQXLYYQul9v9fTz+7Ro5YnRh9DnDDhwMeU4L3S/Fv0ZUqnMH3iFeDZy1rg6SEXPbsRrRXmH73KN4nPJbi73izBCz3rMtLR1h2wKYX5dGFBttGMCIXX9lFEzCZnINScd21q9201jB+un5WiQcbBy4uH+OAhgAMRaAZPJHiIGVzNmLlMufYubuaeTMUw77pNBSgaUqgGlEwghTwInvdfUV4q5JJF0oZBIFJsjxBTltUYKt+HRA9g7ZN/S+oe/6Ui3Volo40Wu0piPT2FJTUvqbOF11Q3ySXEdgOQc6QV4GxEGSEnanBVV4OYITfQRjBnQ5F+5L6DxSPKx+DEgfGahhJlj/ewwptFjMTCOulDNgVUBM/HeAq5WvPONSeb4BSjh2pVOq21FZWJ01D3KpqvCMlhiXr2ooRfCSKDd+h/h+RpVJjEBkpg/bBkLIehiHmQlwBNUcsEIMgQCDg8AGCaAkBZF5gXK9iyVSmhkSBdEc/2973x5r21XV/RtzrrXPLWJ7KaW0VQoIKCqPINVaH/HTNtBKfCAxamoCohAQIkTiO4rGR000JmpMjS8w8UHUCD6hVl4+UoutVJ4BClXQj1KlX2nvvefsvdac4/tjPOZYa+9z7z2l7e29d4+bffc+e6+99nrMOeZv/MYLFieSNHvA1ItScfp7Pob8hKEKvpWwT0zITB7XU1ljShgYQf56pY+khfAyqfIgy1YCGBWFKkoS65u5YqzihqJSkEjYsRNJzL6ZsytGiRvgsKaDtr2BE3P/9H3v4HdTBpG/F662K4m141p7Z99trJLriU92fbNNIGgjqbLh/eMBwAlFG8a8DxRfGIq61wYMgwCT5XIPq+USw6gAxedBYHIm4OQE2ss/btefEWKgKMn4Zh3fOnYthiNlBShFFjyaHAc7ULFgQTt/q+2Qcg4LFHRh0ky5cOxtIdE4J5Z7a8GFIGqB250G5JvW11oWEq9DEuDKtRVfA7srOJEUkAMLCBuLHXfr3gxwS4nV4Fq4blNgU1lKo5MVQ2RYhS6/dgpSWiCmaaZ9bpFdgJmsj72gY52tta/rfU4AS4lp1ADO7Cwii1q16JiBStKFOZsRyQZZafJMMJapYBxGlNUK47DCOMpzLQU7O73WzJHvdX2HHpLp5OU2yUw50+HtN5z99GczikN9K5Kg6Xh/4rVYc+9Mruj0+vk2AZzIVs1wE7eflQKwAm3hMzM+FLx7if0khlnROCivf2IUmB9RAylTSHlyctoDFLII4mTWiLhXSiVQtcEwRXZ+ifRiNp8c/AKL7m2lfln/E8XHTpEB7eaLRS2FgAisytIQJhxo+OSgVieF3Hls27JMBLJtw+CCAmwOrwmhcFsIWtITYBaQMkC6IRfmBlqcbfAKyWsAJRNpZlBCh4QMQsfycAYFrCwKYWDCqhKWlZAhDFCnACWTgJNMkCqGSTOZkgTXpioxLKmOWvDq5AOrJq4Yu1cRVChgiduae0cawGUHJznnNeCzxqQE9mQ6MSdHhU1/8uQNY09OAFI4fkXGyH7fORHc2QRM9mNR4iLQ5pG+doXWMqdWqxVWS2FRxkGzeDT+ZL6wyRg9mOLydU2ryUqgbGBQzAJlM5YFVNtPVU4g0sguO69q7TESapbPWgpyQcohDT0ROGfvAG5rhS+uaIs75aTd0FvJgqx9WHKWcvrQIFkoNU5UtCaZzFFKzXXDirziGK0hlobZgiyNPZKU3cy6j2qssoI4rW/BYImVIHPARlYasFTkxJE5aYDBb+scmBDtOw6JTL9yGK8hvRfqptEYC1Bz11jTU8CAJIt7wwL20JizkpLEtLhPyOraaz8aotbCYxgwrFYYVksMqxXGYYmimWjM5PoBJGnjrHFLasq5sWojnZma3qf2uoUPSGwkkmUHpQCm4Iaou4/QwEa7QjYf4gBs13jqLdA5G2uWeEq8uraKPIpliumz1I5hKXZHNCl46p2S1wDJwYGJyekNUIxCrWkab2DMCm2+NKrbXTE2QGA0o0y8qqu1uUvEsFD0GYP9IkCBuIVSMZeMWY3hKML2FBpRWQ8GYng+pAzmFBgUbs9E7W8CotqYnKycpriMKGEAq6+++e8tcDBSrRJsJceXk4CRrNv3LTQeDKgrSl1MgMYEEEYFME7F6+cWvEtQ945ffzkjKy4Enky/k5L5wjtfaFerlVsQKSUsFgscOnTIa6aQWrgT9iDIZial/b8GT9Z2senzzWnM7bsnugZRMx3/3U172nTNNsX5TLeTFa5Wce1IWvHKg2NXpuCVImf1M1ql4uOBp5M5U8CAOjUWRcegzRGZFeTBo16IdONemztYggNbwO8wjkilVZjOnRgh0Oqs5ra0xVXOj6x0EsxSjjWLiFoWhwAK0iwdKN5VpsLqtwBixdYGrlNOyJw9jsCCZq1qrFj3PGEEK6mBBW7zvqiBkpox0qU0WeSSn9tcy7TX5FfRrgD7CYYOCu078/Fk140Q9Lf+KpGn3zpQMRbatmFhWYoyQIkSshpAkfWRraWPjX2vMGMYC1bDiOVqwLDUMbxcopZRTiN1yLlD1xVn1Do2FsaqWDVwUlmZE1Z3vml7P/5QaDOwKoEDigME6+DEzaM13RNdO1AGvIZUdmaErLp2C+R9INbyKdX0seI6uRgBmNRg5PsF/qzl9AYoAcUn0kU0J4xdQlcIxQYvTycWgWemjlk8iugdeMBpYvn2zGqYq3+CTxgv+MoIFHCwDLQKYUck9GVOoLGARsLImGQIuYXhN71NbvsMaCxIVM3NYGjR1gZ2qj/YX0fWxRRMzOe3s5VeO5HyN8AIDbZjZE7IEH+/KRtTPvHye6R9Ft9832Us+g6LXqpqdt3m1N/7IzGYFgC6rsPe3h6OHTvmlWUBeKXZWA5/zd0DuCtQLpct6OEe+e9O/tpwZKraLSCW4nfifuP323c2YJvp72OmM47DlMyfp+yJDBADxlWztVbDCqvlHvZ299y1s1JwYpkBm47ps5cGC6x5pjGI9htiJJjyNF97q3YJainnFiBv5QUSS8mBjtkzXwBhLCppYzcyyzyAEzWBWdkGWThrm1wVGAshlYqUtepnWLDkuha33HMicK3ouoyabT40kEKjGgpKzVMFrNCYFWjzcZzkOqQa760U4EJhdEn1EQDyWieYGFZxNMmpql5lKW3gsGKy8oV74tdB4KQF0gOmK0xPMBzlkWTsVV2kU61eQA/M4tphgaiM2mryFEKiAuSWJp702Fr4EGOsFUOpWBUBKcOo1VIDoyCxOsI4daXF+3knbXY10Nh21cN+2bxwnH5nBk7ap1P2w9+I4KVd3cmcaIG6WiyUGJyy1C3JWcedVk3WsZASS6muGlGIPXSMVx3qGoQbwXci9sQPoOmkprYOamqe5gCFdWDKIioAJeeEPieUrJaNghFrfx7Bhd382XzTxSFS57MLyw2xOiAhK3iExozYQkzhZ11RiBIAZyAnJM5IqehvV3FRwb4XKNl4LK6F7V0NxuPpggmQlnfGFKjoc509HKDpxTDcX/SaVVO4blG3RVV83Um6OCNpenXo5KyHYbVeYNdNa89YY62dRYfFopN+S90DO0wtoBMAVquVA5R50OzOzs4aOJHzbODEKqjamFkHEwBm03IdwPhoCgvbpn2t78eVgL/eXwVEsC1/Hx+Y2PMmkGJjsZYq7Ilex929XSz39sTyHFZepI2tQtomIHUSimsdYEGZmAYQ55ER7BvaeZADk6rMjywfJK5ikjGYtDijBLhmdF1bKNw9aL53sp5Yao2IRQLj/cjfVfqcC2olMQlSBVYjQCuk0Bgwd52ka48SDzGOA1JOzip2NbfCbKZLzOhgCXZltMDdlBP6Lk3S6M1NbYB9HFmbvVWg04afqdXv8AXUHmbZ6f0jWHyK1FuS2JxZ8HAwaNwQqgLazMD0PZtu9psvV1LK4DNSJdSqReWorQFcR1diku4q12PUc7ZYOiYBE7Gs+1gFpA1jwUob4nk9ELaA4hlY0VpO1VyKALw9CLdsMluDoOdk1W2lTUFw3cDiGe0iYAIMDbjF2WDqZDKHbGxoT6OkxisnRkoVOXMbA5Wh7YlkjCYWV45VpUUb2wwBKYJX2Ia7z42q8U5V8YwfHNTAnirGE8ppDVDawqwMSiZ0mVC9PTqpnxJAm2aTZ0MQNuesWyv5hzx5ZcHSk30RtASyBpMqOEmJ5P7KwdrtVTCjJaopI1GHgg40FlcYqSSpRUC6WMzBiZ7/ZGS2KjyuwJ1B0R92z4yeh1sArE0WoXEo9hXA0T+RuHKswbe5Yjx9GmJR5gTkrPUTCF7NVkrgQ0GKDm4oQ5OkmmbfCXOy0/dY7PTIucOiy3igxFw7ADzbx7ofA3BwslgsMI6jA5O4ONm1iRSbAQq7QycHUMg/911NwPE6OIn7pVkMSgQs6yfengj7g5OTYVCmQYkF4zBgtVxiubeHvWPHBKAsxb0zDEOrI6FXbgo0DmZVzbdlwN08zhjaHGWHBmg1GeRYJlVt/d6SF2mUPiZau6cqSIUE2hKgLhVR0qyBsB6UKqfZtIcu0lYxuhQpUsg8+uKWc4fFzgI7OztYKFNg6drDMCAnQq2dLJY9S3Bsl9C520gXxdraQNij6zIIHXIHOb+UHCHEKqGlyG8SZ3HzZNKy8AoGY1M8akDQwEliKWyXeJSikKwZTuouM4AijIPdPGVQEglzg6Z3ZH1uVh6RLLC1MqoHvqPVdCUt3wKW7BSGu3aAgpq0NpXUm1AQE+v3VAy1CIOi3XmtUBnZ4m1FzQprwDSjsDEyjqMCYye6lfRADXMIANS5HukVvbYRoDkbEsGLgaA4GXTtaoAmaSo0g5NWls1ZmG+777UBqVSBlLmlKINByGhdwVNYbtqaBKgLlaSbfQ2AqsWK2QU6wGRHu7cnLf/wD/+Ab/qmb8Ill1wCIsKb3vSmyecvetGLGtrTx9VXXz3Z5u6778a1116Lc889F4cPH8b3fu/34siR/auF7idTtA3tDZMmnTsdhPq3giVH7T1WWG+Lhw+QWWOo+XO27KHwOmXy9tmthbb9Nhrr4vuQpoddJz18ckpe2CyjsQ/2Wh76N0PYCtbANjZLxhYrG/RpMnDje1CLAoRghbaArnncvruFbEHTxcx683SJ0OeERaePLI8+J3RJiKMUmJkI7JL2r0i6OGS9jw+kGAi0om7L5dJZlGPHjmFvb8/jKJwBmC3om9iEKThhfzRlFQsf2WLZ/gbi8xSctL9tv2tn5b+96RG3u9/gxK1NrdRbRowaULhcLrG7u4vdXWNQLDjWCpg1QDoBOZ/FffTT5nZVoouH43m5n7yBFL8wRBKkmjtQeKRs8QbaHVuBrMWaTc6jRraAZ/qm3TFbDCozxlowDEVqxgyDB+Oa62nacLH9bX1RYhroPIDZrH0DRWa7tOvdFKOTIvZ+3AZmuU9VpmsEq1BdB6S6Qi5LpHGJPO4iD8eQVseQhmPIK3292tXnYyB7KLPhOs/0rbIdrhNMv2fJXuqUce2UZZU+Rp0WvpNsGOF00BrdhUcpFaOmV4+VMTLL35U1iQCeCOFnzNTSspVBseexMobCEmgbHmOV/cb0XSeX2hD235lc7cCcuL520DaX6b1zd1yMfUoCvqUYaaeMYSfNcXPyz8zNKYuoZRa17CJLy24HHgcR6fUKx3RAYGJyYAbl6NGjeOYzn4kXv/jF+LZv+7aN21x99dV43ete53/v7EwrgV577bX45Cc/iRtvvBHDMOB7vud78NKXvhR/9Ed/dKBjsSj5quCkUmRCzI9nE1EXYNtAAQlY2IAEaUc9ibmwBk26QAhNlnyxMYu1FXYjdF2YQEkquKaqTIotZK5QxCJg9feRBrV1SSoPgklQLQSEmLUIyIsW8y2gpQewAKFHSwFO1FA3pezdO6XIG4fYc1aLpHWZCDrMLSZTv2axamB9AERAn9vE6lOSAnEVgsx98VC0n8yiZrVoQj2NYRCFPB6sPDLQFtk4VjZt45bqagUi8oDZQ4cO+bhl5klxt5glZHay2X7rIGUTQLDP9LsUt2nvteNs353uh5xFkXPcf7n3tXjD9TlpcMIzcDJKts6x3WM4dvQojh09gmPHjmJvbxfDaoWi3bobS8IIl2V2gPse+v7C9iSj1LJ3IghwBsAHb4tDMCOkIqFqimfNGeiyVFXNSd0UwV0RlpNkOtl/IwJOUy/mbpXfylm+mCpjVIs1V2gH4zQpDmhsni00Xbbq1qKDBNDI8Q3j6JZ61niWqr+TtT9PqRXDUDB6kTVZIUtlUEoCwtRIkgVfewRpZo+5bVKVur0ZI4iBXAekskKqA6jMHs6gyL2ogDbl01wVva6JhP2ejoU2x3w+sGYlQXRh5iTFY/SaRWAjmTuadj0fw0QgaP0bbclRWd0QSVg0riE6GeyAhyGJAqVK1k8aCziNLZHBDMNQb8owsFZMkbmq10QGykw/mc5eg4bw17z2f5wPAfzbekjmEjMgwgpGqsZcZRBVXcv0cx/LomfqhDkJRhGMlYk4nXwO+PnN9PKJ5MAA5ZprrsE111xz3G12dnZw0UUXbfzsgx/8IN7ylrfgX//1X3HZZZcBAH79138d3/iN34hf/uVfxiWXXLL2neVyieVy6X/fe++9ABprAiJ4RnFAnOaDE+ARkWkbc7BBy2oNcEVC9mBPT/91iwxN4bL8Xsqk/uuI8EnTaQtSCfaUL87tWQ5LAEpO0kuHkzT5SiTsyDhfX8mAhUz4DLmZvT4Ly2KVIjV3nasqVW0BXlv3ZECBUji69js0Bcho4MQ8lEa1Zs1CyCRZAIUhfiGqagFo0Bk0lM2L/jTlUYu4DSRjQvzwcmvbRZiChKlset8CWze9X0rBaiUVrnZ3dycAxRYHK95m+/ZMHr1GzcUjA2MOSuS3pq+bG2fOslHY9vimx3w/G7bwV7FlysmwKPs9SpGYE4nf2cXu0aM4dvQojurz3t4ehtUK1jzvs6dJTixu3XIobmjUvIF7vaaiN8QYqEgAOlTKYMrotMJ06jvkTJK0zFJdtrEwFRb8SnatQv0IK4TVFo9m8GQkzdYjZLfAAUoWI0I+TqPb0QwhMziMZWEtLCfxI2ItpC4jWYFDZrceSmFpFhqMWhsxKSXkhWQVda7DJP25atVQCi6cxAVZG4/mskQe95DGPdC4AsYVUFbAOHg2ogFFCv8MjAOMlER3TPTyBPyjHbQ+yz5aOxKLIUv2uiRvQ8Ca5l4n2TwMb9TERdkSElDWdQCHDE6qQEpSoVt1X2GpnJ1KRR2kEUI1Y8P1pmTNpCT6WEeqZ5MxsfZ44rX5a66dRnHNgYps1eZsW9d8nut+ZJ2R40kkLrIUHu6mt9dmdHMWI1Q7PydtTunGvYNyy/oxzGk6LIKpeANPTh6UGJR3vOMduPDCC/GoRz0K3/AN34Cf+7mfw6Mf/WgAwE033YTDhw87OAGAq666Cikl3HzzzXj+85+/tr/rrrsOP/MzP7P2vi+MzhIE/7aiRn8v8JhyE22ANqq9shCCSZ2FKROkiZfuUCUuPo7aze1jPXgSaWdU8ctJYKUCgAiKDBCQLDSJ4I0MjfYsxOijktfB73GvgUkxcJKcQZFXjFZsLasf154bQJHFpOqq67oggpPApAgF2nSgxf1myELRIzXjoACsUHEEvIQ/QPp7pujFQk/jiCGtkEpCGQ/WAXM/iYuyLQLGoNhry+rZ3d3Fzs6Ou+D6vp8s4LYPdm2k77tyiABA/p4zITaBWzT8usxZkenYmwOTKfMyOwwZZ/Hv47Ao0dq092On6GFYYW+5h93dY+oaO6osylHvYmw1OWzu0OR4aN9zPiigaS4dy3gjr3xpFqD9mIEKWcQgfVcoo1KHmjrUrpcWGH2P3JGXvU8s7eOta3GtxWC6X7Op+64dF6DAQS1XyYyRuhvmHgBNm1UaKE6pXad4b+uoQbRl1PkhE9G+F4eBx9zUitEKsem1EOZCMuf6PitL01gIpoRCqsuYkbg4g5KSxJp04x7ycBRp2AUNS/CwBI8rYFi1MaaKwCz5BIlVsIUuq/uGjcWoZrSEseMvWowgJc1kIu29Y+6hkpDGhJEGlIFRUDVhSFfQKsxKc14bgyKLdMqdRbg6cIEHRGtxygrpDD9WFBQPiLXGeJbZkoikzI0BVe3n0wzjCGi5nZ8x+c0tALNMXaf4neRwgTi8Rz52WlxKRaoJnJQx8Tosknoa64tpaRlnQmo1uMNgT+kxXRGrmBtIUfZkGoxy0vKAA5Srr74a3/Zt34YnPvGJ+OhHP4of//EfxzXXXIObbroJOWfceeeduPDCC6cH0XU4//zzceedd27c54/92I/hB3/wB/3ve++9F4973OOAlJBTRmGgBYoFiG2vNVLUlAdpfQ0xsbRgSaHwFbVoNa6v+QCb6WFY2GjeZA9M3SaWAjeJRqrs5aS5lIYAqlhrHYlF0amFZ9k1Dq90PVRCAkqQtGMJZ092/DZuU4KpctLUsq5KQW2JkG+DX09+djWbcnCQYleczKWkvT5QIWW+W8pdZdKCRDqgTbnoDwhVqN2JxxE1iRX0QEtkVGzRrbV6LMVisUDf95Osno0NBVXJNluvuXD2Z1HsbwI2KJYINNZZkQgqTPm0fawv+g04TRetE4OT+F6Lf5B+V3t7S+weOyasybFj2D12TOJPlnveqDN29oVfn/j7m62pg6kwndsgLzxoirGp/KbGgWCUsihspoSqIIVTB6Re41CSMKBcpcgbFbTW9QzU4uCncJmAOQGvdophYTI9RQmpApzaNvadKQhOM3YNHthp/cdKKc4a5Jg9qNvXQi1mYrR4KvlMghsJHbKm1vea/ahBqUjOsiYekao0OOyY0FeAyyixJhpvgnEFHlbgcQCPAyb5tUQg4xH0GjhYMuabdAxaiYZ9b3vT8+INz5ryb9dSFlvW62VdgitbOXcJ4HWnjdaGYQUiKXdwSoIh916ijJ1FkQq8DB6L9GczgAJW1sTcKBLMXLXIWQPOgUsP49Xu3ETzmnFN8ZrM9Yd+04BfEA7bx0y3BtnDPUoJpO6flNuy1RoksjKjwZ1arQqtAhM1e339sHt9qpsFfud3fqe/fvrTn45nPOMZeNKTnoR3vOMduPLKK+/XPq3K51xS6kCpEzdGKoBGEFe0iS7PVS5apdntlAtmIMKCvlALuGTpKmrIEiHoCKYENIhWEQoTgTOBNd0KxEjjgFQGoBSQNgPjUsCjTBCnhbMk1VMpSLUiA4Z4wDXBc7ZYYQELOBEXjwER+cOoQxhwtXTpBB/hQhmL/zviuqa74iCGD2o2JA7bRutPkKWWNQVJSM4OSPaTpSlD0v8SWh0JtU78zqjVR4g1YaYyd/Ps5/I5nkR2AIDHo+zu7k6qyloKKDM7YJm7eNw1xm1R9GuqV3H+91SXRJBi57j/fuTzCGJmDMrscuzHoJwInJgbrJSCYRiwXEpA8dGjR3HkyBEcO3YMy+US47BCGYuWaZ+OnqhMec2a2ncZmuxi091tcEHGDts4sgXQ3WdyDMnGI+TayphWhUpa0J0sbThMDJfmnJBMFOvdUwBja0hrk9j6xjZ/BChonon3dKn2nWop/NzmWDAu2O4im7+fdRGXxTBbqw8yEKPuKLtGLCAlZtTI+K4oXSvhz2hgAdBYNR6Ra0WHKtF6NAJYoZYRNOyBhj1gXALjCM/YShmgLIt6ssW9A9vf1gvHA/gxX4GPOyDWx4OyFllucEYY07b1AC06Rl6sjnQNYF1gAWMQAGKNGklVsrtyVsZNgDFKRcUIKpp+rkYWhfFDFBIKwtGzDksDjI15g7djMf1goM7dNbCNwvWyssk6nlqmj7pguGUseTYSt95nzUENd/ckZtTMjUUBtJmk7m8WMFy5ARNKxtZlG4EAH8zYfNDTjL/gC74AF1xwAW6//XZceeWVuOiii3DXXXdNthnHEXffffe+cSv7SSJttAUC0QhrtlQtZ53DAK3VK68aViVOoCx/J2UvqGagJAUZEjgndQY0qpkaUHEXkinEBHBJqLnlgmNYAeOIVAqoFqBqjuGogVmpopYRNZHEklQpgJbJUhoJlHVIGjWp+cFC0QpQIUD78QgQYBIQUEn9otDPNLUNWkuAiMGJJPPHnu3nYM9CRzdSFA7UTPlValk/UPBmtLD5W6uCJmF72GNVqhWncmuxuRdgz/vI8WJRDiIxYHa5XE6oduvV02k9FmNWUkquZMLysQ+D4r80+10gLt4TgIG4mMd9Thd8kw2G0/qqPresNoCT+WfGnEgJ+yX2dneFPTlyBEeOHMGeZj4Nq0GydnQBbRrNlpMITE5qFTpJMUXcCPu2OER7k4NBz67AAe3jo8GybleTUtTK0vih2zNrVkaRuCnHRWT1LUwHicFUDOxpVs3EeiWgUlVAxRNGwVOIfVw1cGLHL12RO/S9BNXWUoBSJBUawvQIBS/ZJNYjDMTSGqQ0wDSxpiEAJXNF5RF9JTBGgFcgZGFkzK0zLMG1KjAjIGVQ7sF5AXQLec7CUHHqwKQxXc5MT7mB9iKOI4Qq4NNR5AsrTGerA53ddBArv4xu9U+CoPXYgYSUOrlvnQSPolaJD0pZ1gZzXxfpxUSmhNViSUnjHsnYPPsXj3f6z99ldTVhw0wJb0RGUsY2NVyi7jSPTXVg2xqxxgwxDiDFs1c1WkrSk1VHqrUZAU+pkPFTJWWepVkPrB6QHykxuA44iDzoAOW//uu/8OlPfxoXX3wxAOCKK67APffcg1tvvRXPfvazAQBve9vbUGvF5ZdffqB9C8rttFeFpZUlH4zOolSWbBlIl1KLKCdOmrtfxS+nKXNGwTIJG8IGTrzKZAvAbXSBTgjtiFezWgTjCCqjJOhbRJwVQqijAglromfZNwk9aQ0GIncfoZIgfFUsROSBrgBQk+zH9gcFKw1hk8zuBFAV/62a/5KmzNDnhtJZgclIAlJGwCsFSmHB1tu0kmYOAApEBKSo3hU2wABKglSyFCpHH7Z6yA9XjV/YHPfxwElcpA2gGGCxOimLxcKbCaaUJnT+ZF8nBCgbj2CjS6eBtbZdPN7Gnhznh2YfOUQ4DvO0Cag09kQL2+0KQDl65AiWe3tYamG2UoqvJ+tunAcHpJgKjOCEY0omplNgTbU7uI4MCoV9yjfmSVI2RoxdStTKC1BuMWKVlTkpskCOReMhKLiOwyIt6oQ8M1CscT1Thi4m01YbzRXZg0jnKbNkqCAwymrxClARC7d0mlbrmJKczZQaGKIbO21X0NdB3GOcJDh3GFCGFeo4qC6QRZ0pg7sF0J8D9IeA/hA49WAFLZWyGF0RlUdCwOc6Tc+f4O6cSQit6uJkiyRXZAMzJHOzlAJPPzZ3O7PEkrDF8mnAaBYfOidBM2aIMpHXcalaqc3umYQv2jWooGq1SIIJQzxJSFgLRA//HEfb2JtPd780AdAYix4D97kBDG+IqY9JqjpMV0siBTN7EbeaIcXboAHBFYFB0bAeGEAJrkbV/YQWDnCycmCAcuTIEdx+++3+9x133IHbbrsN559/Ps4//3z8zM/8DF7wghfgoosuwkc/+lH88A//MJ785Cfjuc99LgDgi7/4i3H11VfjJS95CX7zN38TwzDgla98Jb7zO79zYwbPCYWSotfo422+Xks/jum4iSyQVOuJcEUGyeKskfCSC2A3Smur5ISkVRytk7IMfGrPGmMB1klu4KRaNgDrRDArSrQCk9UCSC3/n0LanNaKN3YEUP7CShKzXAsowmf3revDFg3Ib7raJgjbgYxMUp4+qRaQySOkoukJf6b2fWFtLNRM072hcTPcPpPtkv9+SgZQOkDpU+QOrNkDsXbL5JY/SCDFsiPGEJTb970HzJpbx45hnUVDNPYw1STz35vqZWNS1uNJOHweGZSmuAOm8/fCnv0V8foR7efeibU4BJgsNXhYYk329vawXC6xWi6l144FxHJTiNPfp2b2Yuo+hBeCur+ioATmckxofnDpOAybK3rtnE73udMWH4ska2M1IaUK0kXPNG+LC2vn6guM0+sNcNjnVi59cplsGy3gmLQWEpC0vD7C8c7ZFT1/bsUHi9b5KFoJVQiNjA6SscPMqJ0sF1LfpQNZGq0DFWNqWoZjNrdRZXABqFSUWsBc9DImUOrAqQfyAtwfAvfn6PMh1NyjJnmwliCQcd+ukejOZveJzpFxQ9zYp01GACF8F5omrdvlykh5RMo9Uh61WB9aMbYauQz2Y3Hd6mNNDDP7cWtlkpQFX9cDmABQO7r2no+eACam9zuOsQjYWAeGGUas48TS7K1RbXPvcHh/6lZq7Fm73rDU5ERIHNtAqNFeGyCSU0v+MCZGHBCqJ4eDzfMDA5RbbrkFX//1X+9/W/DqC1/4Qlx//fV4z3veg9///d/HPffcg0suuQTPec5z8LM/+7OTGJI//MM/xCtf+UpceeWVSCnhBS94AX7t137toIciEqwI0vx1KbCUNQo+CzNghb8oeQqsFRWTz6yAGPsNl7FQIDHnkiCYc5X+HPqAqT/WQFAkXdz1hpQiqXluW3BgaBgZVTr8QiZXp8SaI3VlcaopxUTwMCySSZV00JZEqDmjqHuqkDQHbIG2opCI2b8vv9oAVhusenl1xEt8TuuzYaWMWQGOgZOR22JhSlPy58VCral1dya9FrmTkvbcC1DhlLVwllxfaxO/X5qxp/weB7hsyuCJEt+LLg0r4GYpn0aL2j5zzlrsKKSkb1hsXdEGwDEFKZbNE6xFfbZDj2DC9rfeyfgEwG328X6ZO/NCdru7x3D06DHP2lnu7WGwJoA6JiyjzRa3iZKeWLx2zIZ2Zzr4pMUmPxycMASc1BCPliyYWMeys6pVrdnUQNQ0aJYADb5MalTYoHc7xCqTWlQnjO42JlbjBliuscWYlGDF1iImeFL9lVNC1/fowmvAUphZYkBYWeFgdVtBQYYwgUWzfKRvSsKiT8BiEYxwgU9dTpLB02mNFYiVXciul6VVq7bg2gpCMusCpt2C8wLodsD5EGq3g9ofQu3kNXc7qCnLtdVgU6DCvGi+NuoBkt+TNsaZW1YJ9HX1v/1LbjRO1gXu0fVFWQOZfNIslVE1TZs1FblYbGI7LFjdLWHQLbg2HHMwVqzAJ5kxSxa46yt5/KKbjMZyeBBtBZAacDEG0GeAGy7GjgQgUq0cf21/R9Cie+Jw7E0hiYKh2TlJpmqW+joMWYsoyQqTAIk5EgMzafE3exARxr2TndsiBwYo/+f//J/jWq833HDDCfdx/vnnH7go237CruwkndcDxXRyZ81a6Yi0IVWSSqdJmlP1Vv6YtG4JAe6P04UoqSvIgEpGQqLsCkvYW6NIqUGRRJKS5/VV2LNtrH5LRkJB1TCiVlzNwAk5IiVwkkBfASnJJ4gH+6WEkgygBCVN5gMFLEipTX7A83688mIzyWXeM2A1EGqzRi092CZYUTqFEDp6WkYFJVSqGgEvCwNBalGg6/TRA13WwGQFKdTqQcyl+ean4ONE7EoEM3OwEtkDZvaAWe/Bovu2TJ6u75G5osudU8O2WIcjXWNADJzETJxY4l4vv2/bzoldOYPsTs5ByvGFZtdsE3syjqOmExtI28WxY1LrZHdXg2LHWMaelTjUYk4gt77br2L/Y/XFZZ/zaKvq+lf1YewdUwJziyVR7qNZptV88HBwIjo6uHoow7suk3IxpGk3ll1p2wtScbeLBaYmaouTH6dZs5aurdcZgACSrgd3mm7c99qfSjJKClVUpdfT7H6BGeNYAEi9D8ukKmVEoiRpxCELzeqtOBODtvgJkIKzvs1Yawtz0oJtQs6Iu5szoapLhxePQO3PQc07KN0Oal6g5oUHMVeYC66gaaN2o63L+3S8MEjjeSzjRlhatkGtbEtYZIlAlKVGFYBaF+iYfSixZzZphdjCktBQC2we2zVKKQkwhYEUuQ8WnGrxTZvBiTHB5IfmxqGDk6lrXUAEIzlVFw0YTNgT6Liy8TCPMZmzJgZS5jFHAqjtdrODPCg7RFkCZ3POqCBkaGNGVIkt0DYRSGJ0ivEp/aXI7/fJy2ndi8cHKrcLSdYO3Z6182dOhJ4MlMjrRVKAou9nssq01l5aLJKk4EdqmjA6zu4uYgCF4WlmRQdd1RRbsuBbVZeZ4EAJxMhgjJB6C6wgwdgTTk1hQhf2mkgjpWViUHDhVkrOnrQmZkpHauYAEH3pOkGSADyG+o0DQJFnKWTktda04JtYrmaBS3AyFKRIlcikv01q2SVY1Vw7CGma2IG6Hug7DaJrriphUE48TB8It09MO7ZnD4a1hQBwf39KCYta0XMPMJDRuTLyYExTuDS1duKhGlBpIGW+LU/+llfQZVfvKfZdv9GOoklkTuzZHjFjJzZTPHLkKI4ePYLVUpoBSqVfyV6xBT4lAqrVjogyXWimR0XTt++HMCIwaUaCmBWSxUfxelpKJLVUaLL5QDZ/LNCvcahIMi9raQyKRp67LrIFgZO6S7VDZryXtQqgWK1WGFYDQEDpOvTMACQg20BOl7NcGk2394XJFidwcOcUL4VfirAFXQcsSMr1Hzq04yDF4mVqaYxZLVYTSIwSib3Q66OLs7inrWgbgZFRUwKhA3U74MU54MXngHc+R4BJWmBU1w5Pbxq0CsEEm9rrCWDRN50pEeWjfWPgbhaAJr8hrog2W3LH6NQ6YGapJ5MGATsV4FLBQwGXUUZQCq6KDKnVghZHx/NjVlc8koUcUCgZr0DFAQHal8PUsLlelbWzeJf2FM6QRQfPDYy6xqRYB++YrRX25AyKujFtXs4ZFE6qngkZBazzK0HDLLoOyJKmnzptD7HokTWxgOtqbe4eT05rgDIOSyz3dlG5YhxG7Z6q/nBFv1lLQ/cKRnp/LSClpyTFxUgCVImgcSYyyQGexJy0/jtCXwEEJAEKlWXBrfaghETit5XvVmTKyFTEp12cZANBM26iFUhWhtsC+FiVrtC7AhjYXevClqS2DzQUTnX6cPGViwGq6ldPzWpyire4tcxcPZ2OVfGSVGITK8SXhSop4MEqptQmZ7JZ3ok7R2JQdGFQP6dNDv/+/WRN7o/E4FCzgAyYEEnp8J3FAoudBRaLHfR912jNpJkEMAuVXLnYoZJZJzBQYiDFD8AXoTnLAWimwETpYSMLsQZOwr4BBEurhmDYpcedHLUqsVrrZBhWKKO0IYgKLrp4zC+tP/UAyxTsMOAZehrSGWKvtEQAbMyEB9FkjbAstZEJmRNWSOikEopnCPr6aGDRr+P6nGI2l4EyPCVYtdzcOrmTKERj6VoqqAGNqm4Pc6MlyZbxuYDJ2HL4R0krtKobUl3era6PGHBlLBjGQb+fpAwCsyzWXn06XH5jhrTKLFOPSj1G9Bj7c1C6c1D6Qyh5x2NOmDqZ19yMGrlObdzG3zBWx38PDSRxJXFvKbPAyuzEEvh+N9hYcXGVp5yRuQOD0XU9Sje4i7mOo95jDZBnYdEpEVIlgLNkfVJ2hsPKLlhQcQwwbiybHooenxEixrb7kPQhLa63VKG1U6Ajm9q1cz3CPmYa+A69m5g9CNbqtEx1iav5yezyZwdGQQ85oCI/L5hxrWONuh55sUDue+R+gW7Rg5IwhAeR0xqgLPd2sXvsKJitqmJxalpqaEiX0Axos7qMhYKULhE6AjpKWm5ElKspMgtoBeC+N2vB3sCKDMishEBCy2Qx+jfB9iX7S7kgdQJOyDJw1N0hpCIcRRcAhVv5bp/a6tJJRrfaZ2SuoQZM2M5HmZxcqwQC65AHbOLUpoS1PsH0IYG+rC3qRREouteKtEgMpqqNAAkSvWNBvq2ksmShCSKX2v7ZAoBc+Xkti4l50eR4QOUggGWTm2f+eexq7IuIBo/u7Oxg59Ah7BxaYbFYzAq6tTLc64G+yjhQapZXBC+YshrVgGK1CsQGaMgrFYPgQXrx19YSfWb31twN05iTXX9YzRMLjq1F5hfqjH1IhFoFiM8ZIf3ZfZieTezKvndk43YG86suEMwtjoSq1hrhNl48noDs+GTSVQZGVjq7JnQpoQPB1GplKDs6LXtfAzIQ1w587lV1hYwxaFUNn77r3Cr3cgVgZ0DGYcSQBx8ntv+akjAqE4BkINEKCQrgaGNRXa9kWT+ddjomWcSKxM2wMp7u5iYgNj0lDeSlLkHaBCxQaAcDLTAqOBnzDkqSTJ0KS8fnfe+un7qNhAlA4akGEEUrhqGbUzp/9Eq06q/hvujYkG7GHarO09L1KF0P7kap4wJAalSJISZZiMbkZMmAdMBKduVbMoDq7GSxiLZdZC44JBHo38yA2Y4OtydJTnpOfh2nIHnab4hbja3weXsYsG3AxUE1prrHgYweM+v3BdCzv18BZNIaMgr6UifsSe4W4h46uwDKHnaPHoEFnXnaVKk6sbThVkroc4ednLHoMjpKyAR0ELeOKyu3dps486UUV4yeN8uVoYMfgMWeWFCrdBZWwpkFlAg40bQ22055/RjUVKo0FLOOmQzTX6yWg9frg8WhGCixgcMWUMdVCsBVqRyLybayiBUYa4MwqAEO5aGta6tZKUwELgJMJKCXVGloDwpY07OstKcqu5yQOpJGX10CZwuuSmoRNUvkeHKyYOR4IMRkDlZs3xZ8GF0/wzBguVrhnEOHcGi1wqGV9O/pFz0W/QKLRY+UuwZsnSpVe16tZxCQsjJWaO63dUunNgucQ2yUBlu3sZmmV0x1dLOE0BSaKi+LNbGHgZIITGKnYmah/QkNQwq0b6mJZD7rDWX8G0gxhbsJbpxYHFj4tyxYu7EnrOHpVk58zl757VYlXdQEryyBrx0nLEjiqyQfqFmpHqtmWS3gdj2YtEaE1B0pVd0wYdFIyZr/mdLXhSWAxiEPSCthWZJWzqZEIAUOtpjYpbTiWlEsYYB0vEkxM4kLkLopQKkFReMlPOjfAnhTBJuk7KaCalqA6RBGOoQhHcLYHULpDknsSe5hVVdtgT7x/Qz3JwwYN9jt/lk9p8lYiQyXzaGW2kqAAL2UgY7Q1x6161H6HqXvUEfJICw671kZ9AgzxBuegExokd0BnKDFPDnj1vifyUJPdmyGz5mRmJxh8d81sOVnhWZkYKYjagPa7nKcgBcDQxzWiCnzBg7vh/17jAtaQUEDKv59Iu9llA2c9AvkfiFjszsY5DitAcqw3MPe7lFHnpHGErcvgXInVpC25F7kzuuLSMqxegSNJo+AJIARYIbyA8VFarn4MNb9QxVoqxQiDfooVYfKpGnESCTR9l5tVtMEmTFCWoED8NoKDYs3tN3QrFWqbFYeqcVLyqAwmn+TYCBFXDwFESWzFt3SY9ZJ285VXjf3lF4PNADXEwCNAZCEB43qdvYkA7nF3KwzJycGF/eHPTkZscXEffTVyr2vNH5ghWEcMI5SpGxn3EHdkZTb3BVfUPKMTfExlSjcyqmF2RSDMSdG+yufVhmcpJcGsfV64SlIiYrelAg3F0LrrSNuHQsKNrfOkSNHPKXYmgASwftNefpgyLRo4D0sOs34s8M6SRhyPOHwv70zAykKlE3RtjmjhJ0fY1DsDK/z0TNhAGGMIKeGueWFrtqJJaP1CzfjqVSMRYu6VbkY0jemNaEUBljHGVcvZU80SBPQjrRcEDX9w23U2DWHsy3mlhS3tAdpKkuZc0busozVZADGtEuLXSBloWSomoLUAma0QE07KHQOxnQOxryD2u2g5AUq9ZN7fpxbqFO+3Rt7QeApQMF07CQtuiRGYgMkCapiSV8Dnh2JpGMiG4PSofQ96jCgalyOSPWWJJKtBHBOqlPN5S46S7WbZ425xregXpAfQ2RQ5ENqJ8QBmEDBiRqv5pCMoMFYFC8LwNWGsgOS46UXt2DZ+WMdmDSA065z2151dzIGRQBw7np/UE4nFU8Y5bQGKJWFXbDXWrAYTVECXuKoVpRxxFCqsYMYYX1zmmXrgEQVje1nomzDa4Mo7UN5zf5hJAL1NbNXfzUFD9JqrQpOuKhbpgj6ohoGki5mqM3yAksMSwFLBVkO6YgsoKTUiqEog8KNoquAAgwJ8vWBp2iZ4wiO4KHRSwpwjPVogI8SIXUJqcuoWZgSzlqlVxUiQ/EPGKCCxgEJP1QehF48+0lkUeaAJ8ZoTN0v1d0jw2rAMIwYx4K+75E76ZCbNWCbrKZOSuJLp6hspsrX33crvbpVy8woVFxpkVrGE/BDNN0nN/q2KuCKbh17zN06y+USwzCIG8iDJmXmmAvHHhOXWyKQV2aS8UsPOEjxX5so2OknDZBw+IPZ6iO1+Snt5EnuTQV6BpYE9KTsoc5Ltgc3JiWyY47LGF73RECKPKBzIxNNMsQ4y70mSDXYEdOxJrc0t0q9ZKxAMwqiLpI1wwJiUxvbCj7KaNk+BVUfxc7N/lW44YKkgfqUUajHmBYoaQcl78hzWmiV2BQJhhOyJ02vwYHJZJyQ45cAVMzIg+txLfmiINMWaqibRqvicEIlBteE2nWofQ8uC61ZtaOFNUfUAeLS1ho/0c1lo4ph/Z+S6/RKDC56AKkZh6B2f5g4tINoJ8qQ3zGgopR5PG3X/wZQWpPK6muBAOm2PliQrKVYlwBq5o9J9k8ANs2tY+ubBpEnBmWWzNas4ETBr4Hj3IlRmvOJjc0opzVAmS79Kq6UGwoFM2oZMYwy6JOBBJZ88+jaiWxJ8onQdk2zwWIKyV/7cYSjpABOgOlDCwARJaeEOQa0srAu7m6xc2XoIGNFzAImpJKsUNWMViEQLHnruca+CuxgpD1Tc9+wgA6/uAGckClFn3D6nAIjpWCvdh3YHjkJUElWCCuJAiR2n68sE3a1gFIemG7G90fmgMUYhzlAGQcFJ+dI6mgpowbO9vroUKsAFWQZKwZOmmaO7hBTMuG58oT5iDEpgA19Cs9mUVNTWhCwbjEnxgbFoFh7WMyJgxNlkCRRR3JtObX7bPOF9TfFLQpAU0Jt0RHQp8cIJ1cOCFZOYmtuhoXN5xbgBzSYpfM6WIxF0+OXDPRgdJDxmcaCpC7aBlJkcWjXPvk9sNtbanDzFHU/G8BQIyVx9hROiyMYeMRYrLeRuhhg7kYNms2aZagsyDw+i7RWi8WByWcVRTNVxmHEWEaP47PgSvu+sSmygmrtkNwAypgXKGkhrEnqNUXbTMOTvJuqj9ZurbEqoXib31jbBNB4QE1D1v2ZnqtsgbLNtZIYUnOp64BFD2KtV1VGpDoi1YKSCKUMqCOBSwlxdKRgwtGuvGZ41rSVhJdg8QpQyNJM7PdT76YYYwnSQNIvgsyK9SXd9MQcXFh5jNn7IZNn8swbHs7EWHCtARrzUujaYCyhTnohspIbYuZW7LqMrkvoO2FWcjrYuDitAYqlEwJoLNkEhUMoMmaUMoKtJHW1QkMaC6KWTwMNMjCmAIVnzw1k+DZxAvn7OlABeHl6/UQoZvkrqTValD5DZXXnsOXUOLUGWHEiRcnVOAcBKIWMUWpuGpnTNCk8G4GJHTW7FjCU3GJBPOMkaaVADRj2PPnwTDJvxaedO9ScUbuEqr5wSgRryOYFhlADCGs+6zI+OAzKfnEp81gUV/REbQLXaTGzVScuHymQNYr1e6hgZ7Hw7/R9+92ako9NtsURppYa6JwCIW5lqgMDUkvxwNlwchNXy9xCsmBya44YwUlkU1ar1cS95axaZYlXYm3FkBqij8aCrtfOABqDEpkUmzVuIW6+K5iO1PVP215o9onVHpp+bNWkLdjcAFRhYGQxejsGlpBq06gFuVR0pSJrdejWy6QpHvL/G4NiPXtKrRi13Hrnbt6krhgGswSol1rFzVu1pD1Xda3p3NHfJJDWf1JKPWswrANSYWM8XVbPvXKLrRrHUUGKjqkqLiZjUKrpDGZwkoKQBQJQhtRjTD1K7iWlmKzlSJg/G+/n+h2MLIvVSWr63MJg7SrPbrPe5BZ0SjqHwrFXUqNfrp9nEHIv958rqIygOoJKEV06CCtdwKBsOi/qBjWk2PSVXTMWtEEVrWGalp2oVh6gscQOUgjTK+c4ZX4VN4MUcwdPwUkALhbj6Jk9phNi1k8DK8djUFhdFG4EeGmPHIBKQqcPUrfwQeS0Bijma55IizLUv5vlW8YRGAep7moxGVytzg5m+ivY8EY5xHCnto3/Tc0SczBD4dktudR89CGYFGFyOXggG/J6WswoaOEgcm5t4AiDQh5V7XEkbie6vWgqWeeAmZVN1TfGRNN+EykwmT2IWmEi86vpswTOZtScwVncPB7Mqfufo3g2haK30WIuHkqZgxR7b+LCIHKAklLCMAzNXws43SrsQntwSj6eOCgY/RVRdjB2pY1fAydcxfodhhHjMGDQrLXGkigraGARNFFi7o5SZsRqnXj5+tVqEjQbK+faIYI0fV1Bqd2rRM0SnrpEaQ2cUNvVSS5i+90soBW4Y8CCurX0vjXAtE0mc90+h2xvVmip4jIduWLgiqEW5CruV+s43tgT9vESQZeBbJurxqCMRdxkpbZ+Ts1Npo1GKzwGpaoRlXNGKR2ydc7VX4pZcjl3GtdkY0ET/QOjSYCMIci4GsYBYxFgLYB3usiJXmIN6kwoTBiRUChjpB4lyaPmHjzXxyd/C33c2NjYJDamXEtReJZV0w1VngAUed/YEyunAO4gKcpA4gKqI5JW/h4zoWTCSMBohljutByCFZKUh2RANvaBWdKEkyVFkIEVASlyiPodkALTKr3MeGpsc7zb+wAVyyZrMVLCqPhrBboGlhur0raf6OAJWIGzTxaDZIWTBfxKQHzKGV0vxdm6LqPL2cFJZ+7ts8vF016EZQMEi9w2lCmLrNTX6AAmHzBUvW6qN6DCDJg0mt0UShj9CGCFwncNpMQJRBrASNWt24Sqz3N4FIAGaSwGWN/TBVBrThh7ZB2FpXsx6W9Hi44a6CCaXDP4sU63o6Y51RIwxqQVIfK4B1OAYRfmAycy6tVWiQpi9WFaurQqGaMS7QAPNqQfWjHwAEisTDFmYhgk+M6C7AKjMVm5DYBwlRRhREW97h92H3GpGLWQ2nK5FGre6Plap78329ecQbGCbPaw983Ctu+HQ44XQCh0tVAte62Bc/l969CrX7mfoGQd0pAaDakKyMh1QKoroK7AZYVaBxQeQHUEalFFrvFrtjDQCCCBeEDKA3IdwTxq88wRuQ7IPIBYrGtw0AMwwBPmLUNraDhU8wXY9YiC22EckYcBKSVlqYqmIrcKvdPfATwuyhYhBlCsgRtrHR4J9syUA1CFs8MC/CXwfRyLu3eMKUNtGSFyncMt92ea/D25RZNPTjyD7Z74ltzutNk8vsvZ7qIe4zhEGo8lHydIAGtlDYS14OEsYwgLEFd0YPSJMPYdxlWPsliIWxeqA1Nq7ThIW3MQKdug8x2CXSwrsuiYkLhC0duJJWsnVUJKVbvLK5D3c2ugzdej9ka4VsElV4O7J4CWBl5a0P2EnYUZrfbcYgpZrXg3pknqhmnNCDAk4Ltb7KDfWUirBgUrsR3IWeXiAcQtYuJIk9u09uqmlMAKTgCtlkdVqgw66FDFgvYec3VXicSH1ClYUW3rIGXyPAcpBErVLWoBK41wcAuHAA9CtIWdyBGsGgowv6cNKIQsGPbYFslWUg5OpqwvItMF0z6zY10DKHp8mz7z8wMm2swKKDXKm93aYap+fTyZxW6cKhJTUKdC5mzJ8baLhc7iw9olRHBCYX+uPBB5srZfO4YWdzIFGRbUamzIqCAJwDQocgZQontqzqiUAHTmrJEeWTMKdEGQOKIGRKKSlbES72Og891UnGjlE4piIFGRXJFYGI6uLpHKClQMoKxQeADqCOYCcHMjOkixsuspIdUBHQ8CVpjR84COV8hVHsQF0ArTzTAyUD613m19t+sk7ykbBnFdjsOIFVm14lDITYukOUBwcKLXUSeLpZZWZu3ELinEfd8jUScB6hP3Dul3LFOwYBwHd03KmNXGj8rKNUAQdC3bmEIYCPHexQV0A6qYSVwcidmDYpka45EgepAmx7T2az4mbQy2QHJhm6WYIDedWLO6AKWyd0mE0mWUxQJlGDAOK3HbsrHaGlqgIMXaIrRePmaUVAXwEsMhJSbkfZ6AE2FbnOVCY7bl3KZsKkeL3EDJ5GJsNmymYCWyPey6V4xDnRNRjDFJAuiSu/kzrDlvSlmKsvWSXhxrQnWazZjOOoAymTSs78nIFiXGgJZY56QLOYm/Ubi0arNNB0FQyqyZMVwxVpJ6JMUWo+K02QSEYGptOJsAasrb6ggotejkBEkaoaceUwvhAywqvqFa/0UHCjEexIK5ppb73JKPmR+egeAABO25ncj6b0fxBcdUt4EMXWhZg3S9g21YvEjL5ZsmUE/IqQIoJyOR3o9pyHGhjwyKlcL376OBhk37jr/hrh5uFW6XyyV29/awmrlmAEzu9RoDo8e56Xke3b/xvKHrgN5vCT608W2g2xYGgFnGtp2mdxFuJ4vNi5htZKtye7a4kcQVuY7o6oBcV0hlCYxLcFmilhXAwojUWqARWQ4cvCw+EYgTUhlBVRiXnBh9HdApOEl1QKqj9Kfh+SSXY+QQizAJajfDorZ7KK5BcpZxfRFRd120EVQ3WLlENjADq4vSqh13nVQ17nJuxgMItZjBZcB0DEHT6uoycAJzEsEVmy2eHF77pcD0trb3jg9SDDBC7yyMhWBzx9jRwwZUULg0HRq+4Uxd6RiUhVlcKaSVxGsmZCt10GXURY86DijqRi1lFBcda7AzMGFQGBQCUHV9YCmZL+0gBKCAKlDULcf6XA2ANl0o49OAwwxgBAu8nS75OgO07SbzeO31bN9+T6lVwdW/7TpLnIk0dbXKxP7IGSn3Xpk3axp3zGBMBwyePq0BiqE2ALoIAG7dWY4ZoaFlb30Z2A+/WdWfJwizJowgfUiQtik1S+OdwYXZ5GiBaQ0IpMnikaxCLbVI+6T55AY2IosBY0EioKC2nbldPNYl0eT3pg8N/AqMiAMTOWg/GTu/jUbS9L/QG8PtIvnbq4bZ381yAJF0k7YfMwPuYYpQJm6PAFIMoMRMmRhcO1EaFahUJ+c4d6tExsSAyRDcO3uaCuy1WQJAseeNbqIARvYDJcdjkaYgRRQqQXG/MiJx7DvY5LZomTKe7OgEIuXXZFxlLsg8ouMBuS7RlyW6IiwKxhW4DKhcUDVLo/0GO0BhSiAapLhgXYFLj1z2kCmjq3sCTngA1UHcRLWoYQOfk4TmMjVmwfz8bqmaX99AZi0oJWEctHw94iLS9kPAxM0scUhlkire4gRYqmdrBkWtVXtaAVAjiWEp81ViTyYB16MbaoAkE9QEQIsNirvZWglkVGS/G8cDICcSc+tOYCg3lWe1TOxXaErRYQ2Z2Hu28Ict1EbVxoNiBCZWwy+RNCwtI+rYo/QjOpvHtWKsjLFWKW+RElr8CYWuwRWljgJwYFqwesHAYtmKmplUbRiRndsUoMQmgDwbHzYGo/EjajjAxwBEEP4Om/n9Yy+JzrKOAB5+IAZ0B+o6BSPdBJxkBSyUO/27a+9ZHaiziUHpuoy+11NwNOna0l/bjfEb6nelocnIoHhdEbUkulJQ6nSxsUf8nfjzFP8PyGXOYMROlxGkxPgOhO0nOwysxxobMmdLHJ23/c3jFKL7h46jbGhypYNqClqA/A2zDJqysHpQEXdZlVnAAoabmyDl+xd491BLBBIpJQcMy+USvabwRKYkpi3uB1CiS8YCWq3DsKUBW8ZNDGwFNgOUeJz7UcDz4zi5k4evKjbNhAVr5+XWXQClAk4Yc0Jl0w+oCkVGRYeCDhUdRmTeQ1cFmOSyh67sIZclUh0AHqcuC5v3esgVxeOpEgagLIGRQFSRakLmAYmXAC/BPEisRy1OBU1coMaD6wyZ1MfRLBm///LjwdpuRtHaIgSrZdKan1brVqxuvZZ5wUh5dGONuWLspFGbxabId0eteyK/X7XzcSkKvtAMPGKJ3xNQoqnFaUfK2ecFauok9sgh6sGFOTAoes4EeAIDtFpyQgAu859yXTzVXpG94VjZ2Gw8BqDu9sjKkLkwcidd6avEOuVapYwDmXtQCrTlAEBL6VC0X1sdE7gWGxWwtggMASoJzS1qIMViQgycWAXiWG3YM7nIsrRaGrkb637+mE0watclrCeWYdqukYYL2MOzMLNUjKX2u5bFk+yzbIkUcY4cbHyc1gAlq691IjpI25C0tyKIMMCin7IhzbmyNuvSAtfmlrAxMeHemzk4ASrwP+ZgITIXEazEDAwHKL7DyGoEQDEHHDNwAn8fQPhdajubAKvJZZ0vVszK+PLkG3Z47bn9phxPoKzTNLvJAVciT7sG4CzZqZTjMQlxG1uQiIRqXy6XUsW479fuS85i4cZYkbivONYMnBjgsdLzln2zKbh103FuchvFvzed7/y9gLftEyjO8CBFsXZZe4mQ32eoFcmw45MvTRwFGy6vLi8gVHSoWGDEAqPEiNQ9BSa7yGUJKitQXSHVEVzHxjpwm+vO+hABKFKhlQEUBdMYkUpCxgirJYvaehCxBtd7JptfB7JLAinQ1uJ7ymj6o11Pc/mQ1SwKKaGW0SZjJnnaJgJo9Qe39hjJjocZXArGvvfMipw7zQKzGKkpSKlalAzGfOrCLRV5O9RktU92MOZDWgNFC7O1kbLBZjv+wmSZTpP7zaIf7OqKi6TpqQDpp4svRVY7rAIGiAF3MTa9A1hrZSIAVTMXswS21qq901hBCjNaIGlSBqiBiFwKRg1ULpRQq4wfSS4rYFiHoubGaxWCdf4DDkha+5NZLZPKPjakKnGIe2y4Y+0O+D1ZW2MUmNl7SVoDkFcatgKbSdmQtk5ZA0kKrh//TkRfB5AzD6AE2XgtXCkGfOl0GBpIQfQH15BfPgsucvYFmEyUfc3BOUiAD5L5a4u8byCiAZLpKc322f6YLlKzkdr2Ndvh/NijdWx/KxPladbhsCbn4j/RwIlNngaqybe1CaIVKgBIN9aHk8TFf76QG5gA4CxK3/fY29ubABSLOYjPcX/zWBGLN5kXUouVXifBuScAKMd770DMSfwe4OBjKuTDjADttRS/Nwe/82/LgLOaQBkFPUYc4gELLNHxHrq6i64cA5UluA4KTEYvptYyLAyM6b4TQdpa6qJHDBpHCZxMhEzWSl4zgCqjFtEJRBpHAIjStkUQshCyulEGB47GujpC0RgS+TtW8GQN5rVFm0iKrUmNE6sK2iq/Wg+ysUqzTmjzvzKOWPQLKRS4WKDrBHx4/ZzgLjIWBX690aLXkQCSxoBTBmVH3DyUxKDYSIWdeFWqLIUnoWPIGTMbVCnqwNlwmY8XBcqi6XliKNnfxsI0sKPAhzLA2jJCs7CkqStLd2EDJwogGNSCrmO67iiF3UZlxctIkAQwrTrMEjckIQJ12pONGjCxqq/Wxyk2nLS6OpZenjMj16TNKs1Yhe9XiqnNr14AIiHYNRkrkps7hzSWyVON7fvOtth3GjhJ1BiUWGn8ZOW0Bih912FxHIACoAGSwA1MkD7gYAQIluVGRuU4D/lywyhx0kxmUFPWdnx2ZA0/2GCdfGMNSzRLdkppri1Ma3+vvZgeazMC/W9vtWKsIMIzTLnHY49MDiaTr4ET1kI/1DYCCeJ2gEKTwkgPF9lvoZ+7cJbLpRbPknOIrIh0mU0e1Gj7sMe8988coBhzYuzJPK7kQY/dWcchAaSgTbukzEIkSXxe6hhaa/w22zXD7FVkVPSo6Klgh0d0KMg0oiOJEquo+r8wE77YQiNF4thM1tVYsiuIoaUHRv9NEHvVX6sbwW7NU+uPFcEJQqwaJxRk1CSB7mz9YxKksajWLamQAmmVimZSVLf0ndGtIySUxtBLi4gzAAewgo4RZQBG2Pm1m+bgt0pD0pGli/PIrfKWBNxnIC/A+RDQ7aDmQ86cjGmBQh0Ews3dsAcbe7W2tiWSOt50imygagFA0BQANgAUHUquL3nz0bSv0USHSnxL03spwYGKVB/XuW73mdWbGQBK8QNvBlwBw2N7qqQ8E9UAjqPOnx6o1EphZZW1for1XNLieJZRIzWrmg4WtjoYvrCqvPKD0T1kGTrOgFgwrMabWPao2ai6TMImO6Ws7p7o3knzMzppOa0BStd3WCwCQNkE3teE2//6nwcS4fggZCO7ohrJaePAoGz2/c1kBi4mf9P0ts4BivzZNqLZl9biSCLw2CRxpvOGKgdkCtsUBK0pEoL90WZbZFMAiUGRiRG+GFFPKEA+OXA0YHCixdfiLja9/2CIAYIIUswtY+/H4NlhGEKEewMotq95NlB07xgwiZVeNwW5nogJ+eyvxdSV2iwpec22IJoLx7Hb9J67lYwpC8NkbsQGTpJS4xmMjhhdAnKClnLXtE+2ap2iHJNW5/L5YIobwc3pY7axGzaHK7FWY26uFwsiTGacBIAtFXYhrELqwB3A3PkZSK8DDZBMdsJV+3BJxWvUolVHpTuWUPhyrklrB1UCwAk1AbUSamIUyfVGTpKdIoUji/SdqQm1yJUsyiiNlTEwYUCHgaQgmVH4Ak46oFuAu0Pgfges3YpLWqBQ9oaMri043t+TFymOJ69j+wFp+yQDiqrcH9KKsX7JZ3p/rjUUlk4HJ9pvueg4Nd1mFSx8rUjwYwBPC6pJMDF54T8wgMxg7hwg2d2vCaAqxeCywrvIRrOyJoCMM05WTTwj65xIBpY5xCd1Hbqcp6CE2iOyNM2FY01GU3sOqcMSV9LAjy1xBsTteP1KB/ePg5Pg3jlojNJpDVD6LgCUiAUChTFlMwKZbErc2BO3UuYAZNMDAZS07/puef7bPDuOqdiU2bhe0IaX4cX0vRk7s89gaFM0HlC7HnawEaCsuWYgLhvrBu3b+C8ECjBOCKcfk/cgiQcWFVz8/lw2MQT7gZL4+QMtm9wj9iAiLJfLiZsmFkWLdQKs/4rtL4KZeTG1GBBrn69Vez3JY78/12QCSo73N7dRZT1QjDlx+jkMfP9e2Jl4DUQlyoLFyFSRSQBKD6lrIUGgGeCMylXLjEvuh6Q9kywUtm1K3gzULUE1MiIrymAvuNZYDAErCQBncT8SWeXYZl1WSuDcoXZqeVIGqJd6TKSlD4jBqNqUbkQdR3AZwNoThopUNs1ZyoTLeWsvH+hCyjkEybbjkblaQSgAJ3BNKAVgJIwKCEYGBk4YAAxEGFKvVVI7UO7AXQ90O2ADKd0C1YJjqUPLpwrz/X5I1Uq7pNdcq0BJ5lBld/EwENJy7ebNAEoYkIGf9n23PUEbASA8ZmaZj1H900rla7CtA2oWcFKrfiFBrmMY64kYhQAeAWIJ8s76iFmktZp7qa0xmeDutsQ2XuWapyRVW83YodAfa9+HBYZruxIDJ0SaCuwJF+21/b7NBQtqNgaFDfCH5A4DKFYi46ByWgOUnJuLZ6KXuQECgySuuNnfCUppHYC0INmmsBA+B6bbTn8jLvTT1/rxDKzsv6iYz3RNKEy+GSBZX3Nmk87/onbh/Jg5fB6e9fe8botSiEaPT8ASoJNXA11toIPQivpIVoEfLM/OlTn89MEG9iagsmkhPl6w6/Fkv88jwCAi73q8X+VWayRoIAWYuoAiCIk1TrznT3ABnUxsycmCkRMBvRMKw63PFpLQ9hdjcRA+S/5qfpwGTmpjUBSgdGopppxAnMGcvWCZuF9s/MleYzZMzq32iMRiRN9/9fldTQcoWPEyBSRtGgyA82QsCyDhlMEZwqbkHpx3wHkh1Ug1DqGiSgDuOIDTAB47IA2gsvJ6GQZQhOCw+kkt3kwWDHtGm0+QGAdwkcWTJBW5TNw6hAEZA2WMicCpB7oe6BbKnizA3Y4+tFtx6lpTQNepayNpcn+PJ5XZY3GsIaK5pMjAJSzdmB1wtN9pbB7rf8FTEzBGC8iefr9t4+AnfBzBi+lCXRIckBp6oEpSw8m/L2G+ktQm1ZYTRs1CU4CidbVqraha5dy7ISkAEn1tBluzGHMK3YO1IJro2jlImYESf04ec+JghAjkxUKn18gNMF8uwv13YNKOc5KCfkCD6LQGKFJ1WEm6iAEcoJjvm7UrrzwTzJfHPninD55YBfGhQ3P2ulGDrKN8spUibhNhCCeYf20O22Idg7tMGU1khkxp9vf+0qyIdZmCE58OBkRM2RtAnoOTOaQgmzC2WBhgSe38CG6D+f1zE+ngyPtUiy3wsfvxhG7dUA9gHn8Sq7vGFGILiN1U7fVBjzs5oJiFaa+B4+gookmRw/32aK0RbFwmW0CCUvTf0/lgC4+wLY1FgYITEKEqfe6z3owPW4S8Pw3cWInHHo7Qf9trayADqQdyD+SFuE+keQXARb5fAU5C6RNXCLIpoJpc3yfS6CxfA+TcKqVgyQOtpL+tomZgWaE6MR4sfqKCtI1GUqanA1IHpB4cH9SASSzgdRIY5Ph3NehvZvaQJLbdc3ue4J7ZWOL4fls1539u1H40GzeufuLeqY3n+W87S0MsvWmIwClJXFCSDu7E2jMJGRnCggnPIiyjr1HeEsX2rT3bSBxFkaEw9tDqkXj83z7sSauzlXwuGFBphmQDQZPVwP9T8EvwtdQAk497//v+gRPgNAcon777/+FzPnmn/DFzq8T4j/Z6yp7IWwpJzEoKbMj639N9TP623wkDOiqweDxrwvOXUfHNvj9B8XGzOezfX6Zzbn5NHBbDAJxbEI7Io4IMfZsrhwAADelJREFU46/t0pWXMyjmWaY5pRiBZTtPDvfzM5/5zOS9dugt7iO+t9+2UU7EEnw2DEL8bgyaNQURg2TN1TN38UTWZe7uOV6114Oc0/29BseDtm2D2fX377F0lTUlaiwk4nkwsDwG/L//CxuPxjYULljRiF0aASoYMGj3WXnmYmXbBxQDhn6+xqC0XjVgK6LW6PVYa6KVqzcdUQ2tIOWK1cjoVwW5GxyYFBaXxbISloUwVKBwQk1LcFqAc+8MigW4Ql08PI5g7aiLkI1EUiEStRCGHGNo5MQm1rwBK2gJfEqgVEBpBKWkDf+AwiSN/xhSKZvlOJE7eawEUHF4IHfTuhifLTjR+jS79/wPPtPvNCCpi58BTw/6RNNBc1vMjTgbhK4S5xysgXn4p5E1mT9j099tSQnXHD5mWsq4pQOrC6/I/UxckUn6/iTUlm3GMXhZq9aqPhX+0BZ+OJhwl2XWXjdkLnVMxkhLBw7drb2oZ3sdTNL56eoSET0Js898wTDDc/p85NN34iByWgIUU2T/+M//jJ1bdk7x0WxlK1t5wGX3duD/3r7xo3se2iPZykMgt7/rhlN9CFt5iGS5XAI4OSOQ+LNyNp8a+djHPoYnPelJp/owtrKVrWxlK1vZyv2QT3ziE/j8z//8425zWjIo559/PgDg4x//OM4777xTfDQPrdx777143OMeh0984hM499xzT/XhPGRytp43cPae+9l63sDZe+5n63kDZ8+5MzPuu+8+XHLJJSfc9rQEKBZgeN55553RN/J4cu65556V5362njdw9p772XrewNl77mfreQNnx7mfLLFwenRh28pWtrKVrWxlK2eVbAHKVrayla1sZStbedjJaQlQdnZ28NrXvhY7O2dfBs/Zeu5n63kDZ++5n63nDZy95362njdwdp/7fnJaZvFsZStb2cpWtrKVM1tOSwZlK1vZyla2spWtnNmyBShb2cpWtrKVrWzlYSdbgLKVrWxlK1vZylYedrIFKFvZyla2spWtbOVhJ1uAspWtbGUrW9nKVh52cloClN/4jd/AE57wBBw6dAiXX3453vWud53qQ/qs5B/+4R/wTd/0TbjkkktARHjTm940+ZyZ8VM/9VO4+OKLcc455+Cqq67CRz7ykck2d999N6699lqce+65OHz4ML73e78XR44ceQjP4uBy3XXX4cu//MvxuZ/7ubjwwgvxrd/6rfjQhz402WZvbw+veMUr8OhHPxqPfOQj8YIXvACf+tSnJtt8/OMfx/Oe9zw84hGPwIUXXogf+qEfwjiOD+WpHFiuv/56POMZz/CqkVdccQXe/OY3++dn6nnP5Rd/8RdBRHj1q1/t752p5/7TP/3Tk7b3RISnPvWp/vmZet4A8N///d/47u/+bjz60Y/GOeecg6c//em45ZZb/PMzVcc94QlPWLvnRIRXvOIVAM7se/6ACJ9m8oY3vIEXiwX/3u/9Hr///e/nl7zkJXz48GH+1Kc+daoP7X7L3/7t3/JP/MRP8J//+Z8zAH7jG984+fwXf/EX+bzzzuM3velN/O///u/8zd/8zfzEJz6Rd3d3fZurr76an/nMZ/K//Mu/8D/+4z/yk5/8ZP6u7/quh/hMDibPfe5z+XWvex2/733v49tuu42/8Ru/kS+99FI+cuSIb/Oyl72MH/e4x/Fb3/pWvuWWW/grv/Ir+au+6qv883Ec+WlPexpfddVV/O53v5v/9m//li+44AL+sR/7sVNxSictf/mXf8l/8zd/wx/+8If5Qx/6EP/4j/84933P73vf+5j5zD3vKO9617v4CU94Aj/jGc/gV73qVf7+mXrur33ta/lLv/RL+ZOf/KQ//ud//sc/P1PP++677+bHP/7x/KIXvYhvvvlm/tjHPsY33HAD33777b7Nmarj7rrrrsn9vvHGGxkAv/3tb2fmM/eeP1By2gGUr/iKr+BXvOIV/ncphS+55BK+7rrrTuFRPXAyByi1Vr7ooov4l37pl/y9e+65h3d2dviP//iPmZn5Ax/4AAPgf/3Xf/Vt3vzmNzMR8X//938/ZMf+2cpdd93FAPid73wnM8t59n3Pf/qnf+rbfPCDH2QAfNNNNzGzgLuUEt95552+zfXXX8/nnnsuL5fLh/YEPkt51KMexb/zO79zVpz3fffdx095ylP4xhtv5K/7uq9zgHImn/trX/tafuYzn7nxszP5vH/kR36Ev+Zrvmbfz88mHfeqV72Kn/SkJ3Gt9Yy+5w+UnFYuntVqhVtvvRVXXXWVv5dSwlVXXYWbbrrpFB7Zgyd33HEH7rzzzsk5n3feebj88sv9nG+66SYcPnwYl112mW9z1VVXIaWEm2+++SE/5vsrn/nMZwC0btW33norhmGYnPtTn/pUXHrppZNzf/rTn47HPvaxvs1zn/tc3HvvvXj/+9//EB79/ZdSCt7whjfg6NGjuOKKK86K837FK16B5z3veZNzBM78e/6Rj3wEl1xyCb7gC74A1157LT7+8Y8DOLPP+y//8i9x2WWX4du//dtx4YUX4lnPehZ++7d/2z8/W3TcarXCH/zBH+DFL34xiOiMvucPlJxWAOV///d/UUqZ3CwAeOxjH4s777zzFB3Vgyt2Xsc75zvvvBMXXnjh5POu63D++eefNtel1opXv/rV+Oqv/mo87WlPAyDntVgscPjw4cm283PfdG3ss4ezvPe978UjH/lI7Ozs4GUvexne+MY34ku+5EvO+PN+wxvegH/7t3/Dddddt/bZmXzul19+OV7/+tfjLW95C66//nrccccd+Nqv/Vrcd999Z/R5f+xjH8P111+PpzzlKbjhhhvw8pe/HD/wAz+A3//93wdw9ui4N73pTbjnnnvwohe9CMCZPdYfKOlO9QFsZSuAWNTve9/78E//9E+n+lAeMvmiL/oi3HbbbfjMZz6DP/uzP8MLX/hCvPOd7zzVh/Wgyic+8Qm86lWvwo033ohDhw6d6sN5SOWaa67x1894xjNw+eWX4/GPfzz+5E/+BOecc84pPLIHV2qtuOyyy/ALv/ALAIBnPetZeN/73off/M3fxAtf+MJTfHQPnfzu7/4urrnmGlxyySWn+lBOGzmtGJQLLrgAOee1KOdPfepTuOiii07RUT24Yud1vHO+6KKLcNddd00+H8cRd99992lxXV75ylfir//6r/H2t78dn//5n+/vX3TRRVitVrjnnnsm28/PfdO1sc8ezrJYLPDkJz8Zz372s3Hdddfhmc98Jn71V3/1jD7vW2+9FXfddRe+7Mu+DF3Xoes6vPOd78Sv/dqvoes6PPaxjz1jz30uhw8fxhd+4Rfi9ttvP6Pv+cUXX4wv+ZIvmbz3xV/8xe7eOht03H/+53/i7//+7/F93/d9/t6ZfM8fKDmtAMpiscCzn/1svPWtb/X3aq1461vfiiuuuOIUHtmDJ0984hNx0UUXTc753nvvxc033+znfMUVV+Cee+7Brbfe6tu87W1vQ60Vl19++UN+zCcrzIxXvvKVeOMb34i3ve1teOITnzj5/NnPfjb6vp+c+4c+9CF8/OMfn5z7e9/73onyuvHGG3HuueeuKcWHu9RasVwuz+jzvvLKK/He974Xt912mz8uu+wyXHvttf76TD33uRw5cgQf/ehHcfHFF5/R9/yrv/qr18oHfPjDH8bjH/94AGe2jjN53etehwsvvBDPe97z/L0z+Z4/YHKqo3QPKm94wxt4Z2eHX//61/MHPvABfulLX8qHDx+eRDmfbnLffffxu9/9bn73u9/NAPhXfuVX+N3vfjf/53/+JzNLCt7hw4f5L/7iL/g973kPf8u3fMvGFLxnPetZfPPNN/M//dM/8VOe8pSHfQrey1/+cj7vvPP4He94xyQV79ixY77Ny172Mr700kv5bW97G99yyy18xRVX8BVXXOGfWxrec57zHL7tttv4LW95Cz/mMY952Kfh/eiP/ii/853v5DvuuIPf85738I/+6I8yEfHf/d3fMfOZe96bJGbxMJ+55/6a17yG3/GOd/Add9zB//zP/8xXXXUVX3DBBXzXXXcx85l73u9617u46zr++Z//ef7IRz7Cf/iHf8iPeMQj+A/+4A98mzNVxzFLpumll17KP/IjP7L22Zl6zx8oOe0ACjPzr//6r/Oll17Ki8WCv+IrvoL/5V/+5VQf0mclb3/72xnA2uOFL3whM0sa3k/+5E/yYx/7WN7Z2eErr7ySP/ShD0328elPf5q/67u+ix/5yEfyueeey9/zPd/D99133yk4m5OXTecMgF/3utf5Nru7u/z93//9/KhHPYof8YhH8POf/3z+5Cc/OdnPf/zHf/A111zD55xzDl9wwQX8mte8hodheIjP5mDy4he/mB//+MfzYrHgxzzmMXzllVc6OGE+c897k8wBypl67t/xHd/BF198MS8WC/68z/s8/o7v+I5JLZAz9byZmf/qr/6Kn/a0p/HOzg4/9alP5d/6rd+afH6m6jhm5htuuIEBrJ0P85l9zx8IIWbmU0LdbGUrW9nKVrayla3sI6dVDMpWtrKVrWxlK1s5O2QLULayla1sZStb2crDTrYAZStb2cpWtrKVrTzsZAtQtrKVrWxlK1vZysNOtgBlK1vZyla2spWtPOxkC1C2spWtbGUrW9nKw062AGUrW9nKVrayla087GQLULayla1sZStb2crDTrYAZStb2cpWtrKVrTzsZAtQtrKVrWxlK1vZysNOtgBlK1vZyla2spWtPOzk/wN0gwDLXp4CJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streetcar orchid leopard porcupine\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Function to show images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize if Normalize was used in transform\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % trainset.classes[labels[j]] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUIbhWRql_oQ",
    "outputId": "17a0dfbd-270a-4696-b95c-dd5804b69f85",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_pil(image_tensor):\n",
    "    return transforms.ToPILImage()(image_tensor).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptMPtw0fYuhS",
    "outputId": "92b65eae-362e-4fea-a093-5881fecc8796",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def save_cifar100_random_replay(dataset, num_images_per_class, save_dir):\n",
    "\tif not os.path.exists(save_dir):\n",
    "\t\tos.makedirs(save_dir)\n",
    "\ttorch.manual_seed(41)\n",
    "\n",
    "\tsaved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "\n",
    "\ttransform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "\tindices = torch.randperm(len(dataset)).tolist()\n",
    "\tfor idx in indices:\n",
    "\t\timage, label = dataset[idx]\n",
    "\n",
    "\t\t# Check if image is already a tensor, transform if not\n",
    "\t\tif not isinstance(image, torch.Tensor):\n",
    "\t\t\timage_tensor = transform_to_tensor(image)\n",
    "\t\telse:\n",
    "\t\t\timage_tensor = image  # Use the image directly if it is already a tensor\n",
    "\n",
    "\t\t# Skip saving if this class already has the desired number of images saved\n",
    "\t\tif saved_counts[label] >= num_images_per_class:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tclass_name = dataset.classes[label]\n",
    "\t\timage_path = os.path.join(save_dir, f'{class_name}{saved_counts[label]}.png')\n",
    "\t\tsave_image(image_tensor, image_path)\n",
    "\t\tsaved_counts[label] += 1\n",
    "\n",
    "\t\t# Check if we have finished saving max_images for all classes\n",
    "\t\tclass_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "\t\twith open(class_file_path, \"a\") as file:\n",
    "\t\t\tfile.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "\t\t# Check if we have finished saving the specified number of images for all classes\n",
    "\t\tif all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "\t\t\tbreak\n",
    "# \t\tif saved_counts[1] < num_images_per_class:\n",
    "# \t\t\tprint('not enough data', saved_counts[1])\n",
    "\tprint(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")\n",
    "\n",
    "\n",
    "integer_to_name = {i: name for i, name in enumerate(name_list)}\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def save_specific_cifar100(dataset, num_images_per_class, save_dir, real_list):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    torch.manual_seed(41)\n",
    "\n",
    "    saved_counts = {label: 0 for label in range(100)}  # Initialize saved image count for each class\n",
    "\n",
    "    transform_to_tensor = transforms.ToTensor()\n",
    "\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        if label in real_list:\n",
    "            # Check if image is already a tensor, transform if not\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image_tensor = transform_to_tensor(image)\n",
    "            else:\n",
    "                image_tensor = image  # Use the image directly if it is already a tensor\n",
    "\n",
    "            # Skip saving if this class already has the desired number of images saved\n",
    "            if saved_counts[label] >= num_images_per_class:\n",
    "                continue\n",
    "\n",
    "            class_name = dataset.classes[label]\n",
    "            image_path = os.path.join(save_dir, f'{class_name}{saved_counts[label]}.png')\n",
    "            save_image(image_tensor, image_path)\n",
    "            saved_counts[label] += 1\n",
    "\n",
    "            # Save path to file\n",
    "            class_file_path = os.path.join(save_dir, f\"class{label}.txt\")\n",
    "            with open(class_file_path, \"a\") as file:\n",
    "                file.write(f\"{image_path} {label}\\n\")\n",
    "\n",
    "            # Check if we have finished saving the specified number of images for all classes\n",
    "            if all(count >= num_images_per_class for count in saved_counts.values()):\n",
    "                break\n",
    "\n",
    "    print(f\"Saved {num_images_per_class} images per class from the CIFAR-100 training dataset.\")\n",
    "\n",
    "\n",
    "def combine_files_with_numbers(folder, file_initial, numbers, output_folder):\n",
    "    \"\"\"use to get the data with label in the training experience\"\"\"\n",
    "    combined_content = \"\"  # Initialize an empty string to store combined content\n",
    "    # Compile a set of filenames to look for, based on the list of numbers\n",
    "    filenames_to_look_for = {file_initial + f\"{number}.txt\" for number in numbers}\n",
    "    print(filenames_to_look_for)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over each file in the specified folder\n",
    "#     files_found = 0\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches exactly any in our set of filenames to look for\n",
    "        if file in filenames_to_look_for:\n",
    "#             print(f'found file {file}')\n",
    "            # Open and read the file, then add its content to the combined_content string\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                combined_content += f.read()  # Add a newline character after each file's content for better separation\n",
    "#                 print(combined_content)\n",
    "\n",
    "    joined_string = '_'.join(str(integer) for integer in numbers)\n",
    "\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_initial}combined_{joined_string}.txt\")\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(combined_content)\n",
    "\n",
    "        \n",
    "def combine_files_with_numbers_toawhole(folder, file_initial, numbers, output_folder):\n",
    "    combined_content = \"\"  # Initialize an empty string to store combined content\n",
    "    # Compile a set of filenames to look for, based on the list of numbers\n",
    "    filenames_to_look_for = {file_initial + f\"{number}.txt\" for number in numbers}\n",
    "    print(filenames_to_look_for)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over each file in the specified folder\n",
    "#     files_found = 0\n",
    "    \n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches exactly any in our set of filenames to look for\n",
    "        if file in filenames_to_look_for:\n",
    "#             print(f'found file {file}')\n",
    "            # Open and read the file, then add its content to the combined_content string\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                combined_content += f.read()  # Add a newline character after each file's content for better separation\n",
    "#                 print(combined_content)\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_initial}_combined_alltogether.txt\")\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[5, 20, 83, 19, 62, 33, 74, 53, 4, 32, 40, 41, 64, 21, 49, 68, 65, 46, 72, 31, 8, 1, 18, 86, 85, 95, 25, 82, 66, 37, 78, 52, 3, 99, 28, 90, 17, 77, 79, 58]\n"
     ]
    }
   ],
   "source": [
    "# sythnthesis classes\n",
    "benchmark = SplitCIFAR100(n_experiences=20,\n",
    "                          seed = 41,             \n",
    "                          )\n",
    "\n",
    "orders = benchmark.classes_order\n",
    "order_list = [orders[x:x+5] for x in range(0, len(orders), 5)]\n",
    "\n",
    "order_sample = [order[3:] for order in order_list]\n",
    "order_sample_real = [order[:3] for order in order_list]\n",
    "classname_list = []\n",
    "label_list = []\n",
    "classname_list_sep = []\n",
    "for order_l in order_sample:\n",
    "    label_list.append(order_l)\n",
    "    cur_classname = [integer_to_name[i] for i in order_l]\n",
    "    classname_list.append(cur_classname)\n",
    "classname_list_sep = [item for lists in classname_list for item in lists]\n",
    "label_list_sep = [item for lists in label_list for item in lists]\n",
    "print(label_list_sep)\n",
    "\n",
    "real_list = set([i for i in range(100)]) - set(label_list_sep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DINOFeatureExtractor_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
    "\n",
    "        \n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def load_and_transform_images(image_paths_file):\n",
    "    # Define the image transformations\n",
    "    transform_to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    transform = Compose([\n",
    "    \n",
    "    Resize(196),\n",
    "    # ToTensor(),\n",
    "    Normalize(mean=[0.5071, 0.4866, 0.4410], std=[0.1941, 0.1917, 0.1957])])\n",
    "    \n",
    "\n",
    "    # Read image paths from the text file\n",
    "    with open(image_paths_file, 'r') as file:\n",
    "        image_paths = [line.split()[0] for line in file.readlines()]\n",
    "\n",
    "    # Load images, apply transformations, and stack into a tensor\n",
    "    # images = [transform(read_image(path).to(torch.float32)) for path in image_paths]\n",
    "    # img = Image.open(path).convert('RGB')\n",
    "    # img_tensor = transform_to_tensor(img)\n",
    "    images = [transform(transform_to_tensor(Image.open(path).convert('RGB'))) for path in image_paths]\n",
    "    \n",
    "    batch_images = torch.stack(images)  # Stack images into a batch\n",
    "\n",
    "    return batch_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(batch_images):\n",
    "    # Assuming the feature_extractor is a PyTorch model loaded and moved to 'cuda' if available\n",
    "    with torch.no_grad():  # No need to track gradients for feature extraction\n",
    "        features = feature_extractor(batch_images.to('cuda')).to('cpu')  # Ensure images are on the same device as the model\n",
    "    return features\n",
    "\n",
    "def process_extractfeatures_images_from_files(file_path1, file_path2):\n",
    "    # Load and transform images from each file\n",
    "    batch_imagesx = load_and_transform_images(file_path1)\n",
    "    batch_imagesy = load_and_transform_images(file_path2)\n",
    "\n",
    "    features_x = extract_features(batch_imagesx)\n",
    "    features_y = extract_features(batch_imagesy)\n",
    "\n",
    "    return features_x, features_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(features_x, features_y, batch_size=32):\n",
    "    # Create a dataset from the extracted features\n",
    "    dataset = TensorDataset(features_x, features_y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_syn = [order[3:] for order in order_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_folder = '/scratch/local/ssd/enbo/saved_data/sdxl_llava_realfromreal_s5g2'\n",
    "x_folder = 'saved_data/sdxl_llava_synfromreal_s8g2'\n",
    "y_folder = 'saved_data/cifar_train_all_fortest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/55/enbo/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/homes/55/enbo/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/homes/55/enbo/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/homes/55/enbo/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "syn = {}\n",
    "real = {}\n",
    "feature_extractor = DINOFeatureExtractor_v2().to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class5.txt', 'class20.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_5_20.txt\n",
      "{'class5.txt', 'class20.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_5_20.txt\n",
      "{'class19.txt', 'class83.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_83_19.txt\n",
      "{'class19.txt', 'class83.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_83_19.txt\n",
      "{'class33.txt', 'class62.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_62_33.txt\n",
      "{'class33.txt', 'class62.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_62_33.txt\n",
      "{'class53.txt', 'class74.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_74_53.txt\n",
      "{'class53.txt', 'class74.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_74_53.txt\n",
      "{'class32.txt', 'class4.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_4_32.txt\n",
      "{'class32.txt', 'class4.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_4_32.txt\n",
      "{'class40.txt', 'class41.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_40_41.txt\n",
      "{'class40.txt', 'class41.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_40_41.txt\n",
      "{'class64.txt', 'class21.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_64_21.txt\n",
      "{'class64.txt', 'class21.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_64_21.txt\n",
      "{'class49.txt', 'class68.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_49_68.txt\n",
      "{'class49.txt', 'class68.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_49_68.txt\n",
      "{'class46.txt', 'class65.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_65_46.txt\n",
      "{'class46.txt', 'class65.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_65_46.txt\n",
      "{'class72.txt', 'class31.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_72_31.txt\n",
      "{'class72.txt', 'class31.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_72_31.txt\n",
      "{'class1.txt', 'class8.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_8_1.txt\n",
      "{'class1.txt', 'class8.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_8_1.txt\n",
      "{'class18.txt', 'class86.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_18_86.txt\n",
      "{'class18.txt', 'class86.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_18_86.txt\n",
      "{'class85.txt', 'class95.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_85_95.txt\n",
      "{'class85.txt', 'class95.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_85_95.txt\n",
      "{'class25.txt', 'class82.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_25_82.txt\n",
      "{'class25.txt', 'class82.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_25_82.txt\n",
      "{'class37.txt', 'class66.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_66_37.txt\n",
      "{'class37.txt', 'class66.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_66_37.txt\n",
      "{'class78.txt', 'class52.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_78_52.txt\n",
      "{'class78.txt', 'class52.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_78_52.txt\n",
      "{'class99.txt', 'class3.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_3_99.txt\n",
      "{'class99.txt', 'class3.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_3_99.txt\n",
      "{'class90.txt', 'class28.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_28_90.txt\n",
      "{'class90.txt', 'class28.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_28_90.txt\n",
      "{'class17.txt', 'class77.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_17_77.txt\n",
      "{'class17.txt', 'class77.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_17_77.txt\n",
      "{'class79.txt', 'class58.txt'}\n",
      "saved_data/sdxl_llava_synfromreal_s8g2_X/classcombined_79_58.txt\n",
      "{'class79.txt', 'class58.txt'}\n",
      "saved_data/cifar_train_all_fortest_Y/classcombined_79_58.txt\n"
     ]
    }
   ],
   "source": [
    "for i, t_class in enumerate(order_syn):\n",
    "    combine_files_with_numbers(x_folder, \n",
    "                               'class', \n",
    "                               t_class, \n",
    "                               x_folder + '_X/') # create txt files for x and y\n",
    "    combine_files_with_numbers(y_folder, \n",
    "                               'class', \n",
    "                               t_class, \n",
    "                               y_folder + '_Y/')\n",
    "    joined_string = '_'.join(str(integer) for integer in t_class)\n",
    "    # output_folder = destination_folder + '_combined/'\n",
    "    file_initial = 'class'\n",
    "    output_x = x_folder + '_X/' +file_initial+ 'combined' + '_' + joined_string + '.txt' \n",
    "    output_y = y_folder + '_Y/' +file_initial+ 'combined' + '_' + joined_string + '.txt' \n",
    "    \n",
    "    all_x, all_y = process_extractfeatures_images_from_files(output_x, output_y)\n",
    "    syn[i] = all_x\n",
    "    real[i] = all_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(syn, 'saved_data/featurex_s8g2_onsyn.pth')\n",
    "torch.save(real, 'saved_data/featurex_real_onsyn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = torch.cat([syn[i] for i in sorted(syn.keys())], dim=0)\n",
    "all_y = torch.cat([real[i] for i in sorted(real.keys())], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dataset = TensorDataset(all_x, all_y)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 256\n",
    "output_size = 1024\n",
    "model = MLP(input_size, hidden_size, output_size).to('cuda')\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.5538\n",
      "Epoch [2/20], Loss: 1.4124\n",
      "Epoch [3/20], Loss: 1.3841\n",
      "Epoch [4/20], Loss: 1.3629\n",
      "Epoch [5/20], Loss: 1.3447\n",
      "Epoch [6/20], Loss: 1.3280\n",
      "Epoch [7/20], Loss: 1.3112\n",
      "Epoch [8/20], Loss: 1.2953\n",
      "Epoch [9/20], Loss: 1.2785\n",
      "Epoch [10/20], Loss: 1.2592\n",
      "Epoch [11/20], Loss: 1.2418\n",
      "Epoch [12/20], Loss: 1.2241\n",
      "Epoch [13/20], Loss: 1.2055\n",
      "Epoch [14/20], Loss: 1.1865\n",
      "Epoch [15/20], Loss: 1.1720\n",
      "Epoch [16/20], Loss: 1.1568\n",
      "Epoch [17/20], Loss: 1.1449\n",
      "Epoch [18/20], Loss: 1.1333\n",
      "Epoch [19/20], Loss: 1.1232\n",
      "Epoch [20/20], Loss: 1.1148\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataset)\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_model/mlp_model1.pth\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Example: Save the model\n",
    "model_path = \"saved_model/mlp_model1.pth\"\n",
    "save_model(model, model_path)\n",
    "\n",
    "\n",
    "def load_model(model_path, input_size, hidden_size, output_size):\n",
    "    model = MLP(input_size, hidden_size, output_size)  # Re-create the model architecture\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the saved state dictionary\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from saved_model/mlp_model1.pth\n"
     ]
    }
   ],
   "source": [
    "# Example: Load the model\n",
    "loaded_model = load_model(model_path, 1024, 256, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Features: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "def map_features(model, features):\n",
    "    model.to('cuda')\n",
    "    with torch.no_grad():  # Ensure no gradients are computed\n",
    "        mapped_features = model(features.to('cuda'))  # Assuming features and model are on the same device\n",
    "    return mapped_features\n",
    "\n",
    "# Example: Map features using the loaded model\n",
    "sample_features = torch.randn(1, 1024).to('cuda')  # Create a dummy feature tensor\n",
    "mapped_features = map_features(loaded_model, sample_features)\n",
    "print(\"Mapped Features:\", mapped_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end mlp estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the saved dictionary of tensors\n",
    "# matrices_A1 = torch.load('saved_data/transform_matrix_s5g2_2.pth')\n",
    "matrices_A = torch.load('saved_data/transform_matrix_s5g2_3.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_eye = {}\n",
    "for i in range(20):\n",
    "    matrix_eye[i] = torch.eye(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(matrices_A[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_specific_cifar100(trainset, 500, 'saved_data/cifar99real_1synthetic', [i for i in range(99)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 .txt files in the folder.\n",
      "There are 0 .png files in the folder.\n"
     ]
    }
   ],
   "source": [
    "# output_folder = 'saved_data/real_sythesis_cifar100_llava'\n",
    "# output_folder= '/scratch/local/ssd/enbo/saved_data/sdxl_llava_synfromreal_text2image'\n",
    "# output_folder = 'saved_data/sdxl_llava_synfromreal_s8g2'\n",
    "# output_folder = 'saved_data/real_sythesis_cifar100_llava_i2i_3real_2'\n",
    "# output_folder = '/scratch/local/ssd/enbo/saved_data/sdxl_llava_realfromreal_s5g2'\n",
    "# output_folder = 'saved_data/sdxl_llava_synfromreal_expsim'\n",
    "output_folder = '/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2'\n",
    "count = count_txt_files(output_folder)\n",
    "print(f\"There are {count} .txt files in the folder.\")\n",
    "count = count_png_files(output_folder)\n",
    "print(f\"There are {count} .png files in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_cifar100_random_replay(testset, 100, 'saved_data/cifar_test100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "TGTjnkg03Dnn",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(root='data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "name_list = trainset.classes\n",
    "\n",
    "integer_to_name = {i: name for i, name in enumerate(name_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def filter_class_txt_files_from1to1(folder_path, output_folder, specific_dict_integer_to_name, folder_path2 = None):\n",
    "    \"\"\"\n",
    "    Filters out images and text files for specified classes and copies them to a new directory.\n",
    "\n",
    "    Args:\n",
    "    image_folder (str): Path to the directory containing images.\n",
    "    txt_folder (str): Path to the directory containing text files.\n",
    "    output_folder (str): Path to the directory where filtered files should be stored.\n",
    "    class_list (list): List of class numbers as strings.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    \n",
    "    class_numbers = list(specific_dict_integer_to_name.keys())\n",
    "    class_names = list(specific_dict_integer_to_name.values())\n",
    "        \n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if not folder_path2:\n",
    "        file_list = os.listdir(folder_path)\n",
    "    else:\n",
    "        file_list = os.listdir(folder_path) + os.listdir(folder_path2)\n",
    "\n",
    "    for file in file_list:\n",
    "        # Check if the file is an image or a text file for the classes in the list\n",
    "        if (file.endswith('.txt') and int(file.split('class')[1].split('.txt')[0]) in class_numbers):\n",
    "            # Copy file to output directory\n",
    "            if file in os.listdir(folder_path):\n",
    "                shutil.copy(os.path.join(folder_path, file), os.path.join(output_folder, file))\n",
    "            else:\n",
    "                shutil.copy(os.path.join(folder_path2, file), os.path.join(output_folder, file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def filter_class_txt_files_from2to1(folder_path1, folder_path2, output_folder, specific_dict1, specific_dict2):\n",
    "    \"\"\"\n",
    "    Filters out text files for specified classes from two directories and copies them to a new directory.\n",
    "\n",
    "    Args:\n",
    "    folder_path1 (str): Path to the first directory containing text files.\n",
    "    folder_path2 (str): Path to the second directory containing text files.\n",
    "    output_folder (str): Path to the directory where filtered files should be stored.\n",
    "    specific_dict1 (dict): Dictionary where keys are class numbers and values are class names for the first directory.\n",
    "    specific_dict2 (dict): Dictionary where keys are class numbers and values are class names for the second directory.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Combine both dictionaries for easier processing\n",
    "    combined_dict = {**specific_dict1, **specific_dict2}\n",
    "\n",
    "    # Get the list of files from both directories\n",
    "    file_list1 = os.listdir(folder_path1)\n",
    "    file_list2 = os.listdir(folder_path2)\n",
    "\n",
    "    # Process files in the first folder\n",
    "    for file in file_list1:\n",
    "        if file.endswith('.txt'):\n",
    "            try:\n",
    "                class_number = int(file.split('class')[1].split('.txt')[0])\n",
    "                if class_number in specific_dict1:\n",
    "                    shutil.copy(os.path.join(folder_path1, file), os.path.join(output_folder, file))\n",
    "            except (IndexError, ValueError):\n",
    "                # Handle cases where file name format doesn't match the expected pattern\n",
    "                continue\n",
    "\n",
    "    # Process files in the second folder\n",
    "    for file in file_list2:\n",
    "        if file.endswith('.txt'):\n",
    "            try:\n",
    "                class_number = int(file.split('class')[1].split('.txt')[0])\n",
    "                if class_number in specific_dict2:\n",
    "                    shutil.copy(os.path.join(folder_path2, file), os.path.join(output_folder, file))\n",
    "            except (IndexError, ValueError):\n",
    "                # Handle cases where file name format doesn't match the expected pattern\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "syn_dict = {class_number: integer_to_name[class_number] for class_number in label_list_sep if class_number in integer_to_name}\n",
    "real_dict = {class_number: integer_to_name[class_number] for class_number in real_list if class_number in integer_to_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classcombined_36_0_54_5_20.txt',\n",
       " 'classcombined_22_45_13_83_19.txt',\n",
       " 'classcombined_26_73_16_62_33.txt',\n",
       " 'classcombined_34_98_24_74_53.txt',\n",
       " 'classcombined_10_94_51_4_32.txt',\n",
       " 'classcombined_38_81_50_40_41.txt',\n",
       " 'classcombined_30_89_69_64_21.txt',\n",
       " 'classcombined_84_14_88_49_68.txt',\n",
       " 'classcombined_6_80_57_65_46.txt',\n",
       " 'classcombined_9_91_48_72_31.txt',\n",
       " 'classcombined_76_7_47_8_1.txt',\n",
       " 'classcombined_61_75_63_18_86.txt',\n",
       " 'classcombined_59_70_43_85_95.txt',\n",
       " 'classcombined_27_93_35_25_82.txt',\n",
       " 'classcombined_44_56_67_66_37.txt',\n",
       " 'classcombined_60_11_2_78_52.txt',\n",
       " 'classcombined_97_39_55_3_99.txt',\n",
       " 'classcombined_29_71_23_28_90.txt',\n",
       " 'classcombined_87_15_92_17_77.txt',\n",
       " 'classcombined_12_42_96_79_58.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list1 = os.listdir('/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined')\n",
    "file_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage'\n",
    "\n",
    "filter_class_txt_files_from2to1('saved_data/sdxl_llava_synfromreal_s8g2', \n",
    "                                '/scratch/local/ssd/enbo/saved_data/sdxl_llava_realfromreal_s5g2', \n",
    "                                '/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2', \n",
    "                                syn_dict, \n",
    "                                real_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter_class_txt_files('saved_data/cifar_train_all_fortest', \n",
    "#                        '/scratch/local/ssd/enbo/saved_data/text1folder', \n",
    "#                        real_dict, folder_path2 = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# folder_path = 'saved_data/sd_turbo_500images_llava_firstthreeclasses'\n",
    "# folder_path2 = 'saved_data/sd_turbo_500images_llava'\n",
    "# output_folder = 'saved_data/sdxl_llava_synfromreal_s8g2'\n",
    "\n",
    "\n",
    "# filter_class_txt_files(folder_path, output_folder, syn_dict, folder_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 .txt files in the folder.\n",
      "There are 0 .png files in the folder.\n"
     ]
    }
   ],
   "source": [
    "count = count_txt_files(output_folder)\n",
    "print(f\"There are {count} .txt files in the folder.\")\n",
    "count = count_png_files(output_folder)\n",
    "print(f\"There are {count} .png files in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_lines_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and prints lines from a text file at the given path.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the text file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                print(line.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file at {file_path} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "# file_path = os.path.join(output_folder, 'class5.txt')\n",
    "file_path = 'saved_data/cifar_test100_combined/classcombined_36_0_54_5_20.txt'\n",
    "read_lines_from_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset concate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_36_0_54_5_20.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_22_45_13_83_19.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_26_73_16_62_33.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_34_98_24_74_53.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_10_94_51_4_32.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_38_81_50_40_41.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_30_89_69_64_21.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_84_14_88_49_68.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_6_80_57_65_46.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_9_91_48_72_31.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_76_7_47_8_1.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_61_75_63_18_86.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_59_70_43_85_95.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_27_93_35_25_82.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_44_56_67_66_37.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_60_11_2_78_52.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_97_39_55_3_99.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_29_71_23_28_90.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_87_15_92_17_77.txt\n",
      "/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2_combined/classcombined_12_42_96_79_58.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# combine the real and sythesis data\n",
    "# folder1 = 'saved_data/cifar99real_1synthetic'\n",
    "# folder2 = 'saved_data/worm99_500'\n",
    "# destination_folder = '/scratch/local/ssd/enbo/saved_data/sdxl_llava_synfromreal_text2image'\n",
    "# destination_folder = 'saved_data/sdxl_llava_synfromreal_s8g2'\n",
    "# destination_folder = 'saved_data/cifar_train_all_fortest'\n",
    "# destination_folder = 'saved_data/sdxl_llava_synfromreal_expsim_guidance6'\n",
    "destination_folder = '/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness_2'\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "def copy_contents(src, dst):\n",
    "    for item in os.listdir(src):\n",
    "        src_path = os.path.join(src, item)\n",
    "        dst_path = os.path.join(dst, item)\n",
    "\n",
    "        if os.path.isdir(src_path):\n",
    "            if not os.path.exists(dst_path):\n",
    "                os.makedirs(dst_path)\n",
    "            copy_contents(src_path, dst_path)  # Recursively copy subdirectories\n",
    "        else:\n",
    "            shutil.copy2(src_path, dst_path)  # Copy files\n",
    "\n",
    "# copy_contents(folder1, destination_folder)\n",
    "# copy_contents(folder2, destination_folder)\n",
    "\n",
    "\n",
    "train_experience_list = []\n",
    "for l in order_list:\n",
    "    combine_files_with_numbers(\n",
    "                destination_folder,\n",
    "                'class',\n",
    "                l,\n",
    "                destination_folder + '_combined/')\n",
    "    \n",
    "    joined_string = '_'.join(str(integer) for integer in l)\n",
    "    output_folder = destination_folder + '_combined/'\n",
    "    file_initial = 'class'\n",
    "    output_file_path = output_folder +file_initial+ 'combined' + '_' + joined_string + '.txt' \n",
    "    train_experience_list.append(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 .txt files in the folder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the folder path here\n",
    "# folder_path = 'saved_data/real_sythesis_cifar100_baseprompt/'\n",
    "folder_path = '/scratch/local/ssd/enbo/saved_data/synreal_realsyn_714_estimatecorrectness'\n",
    "\n",
    "# Call the function and print the result\n",
    "count = count_txt_files(folder_path)\n",
    "print(f\"There are {count} .txt files in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table0.png\n",
      "class84.txt\n",
      "bee0.png\n",
      "class6.txt\n",
      "kangaroo0.png\n",
      "class38.txt\n",
      "clock0.png\n",
      "class22.txt\n",
      "television0.png\n",
      "class87.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_first_ten_files(folder_path):\n",
    "    \"\"\"Prints the names of the first 10 files in the specified folder.\"\"\"\n",
    "    \n",
    "    # Make sure the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The specified folder does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # List files in the directory\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    # Print the first 10 files\n",
    "    for file in files[:10]:\n",
    "        print(file)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"saved_data/real_sythesis_cifar100_llava_i2i_3real\"\n",
    "list_first_ten_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_data/cifar_test100_combined/classcombined_36_0_54_5_20.txt\n",
      "saved_data/cifar_test100_combined/classcombined_22_45_13_83_19.txt\n",
      "saved_data/cifar_test100_combined/classcombined_26_73_16_62_33.txt\n",
      "saved_data/cifar_test100_combined/classcombined_34_98_24_74_53.txt\n",
      "saved_data/cifar_test100_combined/classcombined_10_94_51_4_32.txt\n",
      "saved_data/cifar_test100_combined/classcombined_38_81_50_40_41.txt\n",
      "saved_data/cifar_test100_combined/classcombined_30_89_69_64_21.txt\n",
      "saved_data/cifar_test100_combined/classcombined_84_14_88_49_68.txt\n",
      "saved_data/cifar_test100_combined/classcombined_6_80_57_65_46.txt\n",
      "saved_data/cifar_test100_combined/classcombined_9_91_48_72_31.txt\n",
      "saved_data/cifar_test100_combined/classcombined_76_7_47_8_1.txt\n",
      "saved_data/cifar_test100_combined/classcombined_61_75_63_18_86.txt\n",
      "saved_data/cifar_test100_combined/classcombined_59_70_43_85_95.txt\n",
      "saved_data/cifar_test100_combined/classcombined_27_93_35_25_82.txt\n",
      "saved_data/cifar_test100_combined/classcombined_44_56_67_66_37.txt\n",
      "saved_data/cifar_test100_combined/classcombined_60_11_2_78_52.txt\n",
      "saved_data/cifar_test100_combined/classcombined_97_39_55_3_99.txt\n",
      "saved_data/cifar_test100_combined/classcombined_29_71_23_28_90.txt\n",
      "saved_data/cifar_test100_combined/classcombined_87_15_92_17_77.txt\n",
      "saved_data/cifar_test100_combined/classcombined_12_42_96_79_58.txt\n"
     ]
    }
   ],
   "source": [
    "test_experience_list = []\n",
    "for l in order_list:\n",
    "    combine_files_with_numbers(\n",
    "                'saved_data/cifar_test100',\n",
    "                'class',\n",
    "                l,\n",
    "                'saved_data/cifar_test100' + '_combined/')\n",
    "    \n",
    "    joined_string = '_'.join(str(integer) for integer in l)\n",
    "    output_folder = 'saved_data/cifar_test100' + '_combined/'\n",
    "    file_initial = 'class'\n",
    "    output_file_path = output_folder +file_initial+ 'combined' + '_' + joined_string + '.txt' \n",
    "    test_experience_list.append(output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "DkwtwPTByTNd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop, RandomHorizontalFlip, Resize\n",
    "import os\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "transform_train = Compose([\n",
    "    Resize(196),\n",
    "    # RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5071, 0.4866, 0.4410], std=[0.1941, 0.1917, 0.1957])\n",
    "    \n",
    "])\n",
    "\n",
    "transform_test = transform_train\n",
    "\n",
    "# transform_test = Compose([\n",
    "#     Resize(196),\n",
    "#     ToTensor(),\n",
    "#     Normalize(*stats,inplace=True)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sythesis_cifar_benchmark =  filelist_benchmark(\n",
    "                                None,\n",
    "                                train_file_lists = train_experience_list, # train\n",
    "                                test_file_lists = test_experience_list, # test\n",
    "                                task_labels = [0]*20,\n",
    "                                # complete_test_set_only=True,\n",
    "                                train_transform=transform_train,\n",
    "                            eval_transform=transform_train\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 batch 0 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [0, 36, 5, 20, 54]\n",
      "Task 0 batch 1 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [45, 13, 19, 83, 22]\n",
      "Task 0 batch 2 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [33, 73, 16, 26, 62]\n",
      "Task 0 batch 3 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [34, 98, 74, 53, 24]\n",
      "Task 0 batch 4 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [32, 4, 10, 51, 94]\n",
      "Task 0 batch 5 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [38, 40, 41, 81, 50]\n",
      "Task 0 batch 6 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [64, 69, 21, 89, 30]\n",
      "Task 0 batch 7 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [68, 14, 49, 84, 88]\n",
      "Task 0 batch 8 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [65, 6, 46, 80, 57]\n",
      "Task 0 batch 9 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [72, 9, 48, 91, 31]\n",
      "Task 0 batch 10 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [1, 7, 8, 76, 47]\n",
      "Task 0 batch 11 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [75, 18, 86, 61, 63]\n",
      "Task 0 batch 12 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [70, 43, 85, 59, 95]\n",
      "Task 0 batch 13 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [35, 82, 25, 27, 93]\n",
      "Task 0 batch 14 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [66, 67, 37, 44, 56]\n",
      "Task 0 batch 15 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [2, 11, 78, 52, 60]\n",
      "Task 0 batch 16 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [97, 99, 3, 39, 55]\n",
      "Task 0 batch 17 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [71, 23, 90, 28, 29]\n",
      "Task 0 batch 18 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [77, 15, 17, 87, 92]\n",
      "Task 0 batch 19 -> train\n",
      "This batch contains 2500 patterns\n",
      "This contains labels [96, 42, 12, 79, 58]\n"
     ]
    }
   ],
   "source": [
    "train_stream = sythesis_cifar_benchmark.train_stream\n",
    "for experience in train_stream:\n",
    "    t = experience.task_label\n",
    "    exp_id = experience.current_experience\n",
    "    training_dataset = experience.dataset\n",
    "    print('Task {} batch {} -> train'.format(t, exp_id))\n",
    "    print('This batch contains', len(training_dataset), 'patterns')\n",
    "    print(f'This contains labels {experience.classes_in_this_experience}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VUd1jnlL4vV"
   },
   "source": [
    "# Data transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "ZyQi87koaOjd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch_pretrained_vit\n",
    "\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# ?\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitCIFAR100\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark, \\\n",
    "                                            tensors_benchmark, paths_benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform = transform_train = Compose([\n",
    "#     Resize(196),\n",
    "#     # Resize(384),\n",
    "#     # RandomHorizontalFlip(),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     Mean: tensor([0.5071, 0.4866, 0.4410])\n",
    "# Standard Deviation: tensor([0.1941, 0.1917, 0.1957])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "a5YNVSvRJH6n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger, \\\n",
    "    WandBLogger, TextLogger\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('logs/syntheticreal_synthetic_matrixA.txt', 'w'))\n",
    "# text_logger = TextLogger(open('logs/test.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Qx3UrGFcJH6o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, loss_metrics, class_accuracy_metrics\n",
    "\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    class_accuracy_metrics(minibatch=False, epoch=False, epoch_running=False, experience=False, stream=True),\n",
    "    # forgetting_metrics(experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "f3UKvdtpc9tL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from avalanche.training.plugins.checkpoint import CheckpointPlugin, \\\n",
    "    FileSystemCheckpointStorage\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRjpqXv9dR9V",
    "outputId": "c82c7a66-91d8-4f23-d9bc-ef1bf3a33cfd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "RNGManager.set_random_seeds(1234)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "checkpoint_plugin = CheckpointPlugin(\n",
    "    FileSystemCheckpointStorage(\n",
    "        directory='./checkpoints/task_cifar',\n",
    "    ),\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# Load checkpoint (if exists in the given storage)\n",
    "# If it does not exist, strategy will be None and initial_exp will be 0\n",
    "strategy, initial_exp = checkpoint_plugin.load_checkpoint_if_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "R-twpACd-97s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def combine_files_with_numbers(folder, file_initial, numbers, output_folder):\n",
    "    \"\"\"use to get the data with label in the training experience\"\"\"\n",
    "    combined_content = \"\"  # Initialize an empty string to store combined content\n",
    "    # Compile a set of filenames to look for, based on the list of numbers\n",
    "    filenames_to_look_for = {file_initial + f\"{number}.txt\" for number in numbers}\n",
    "\n",
    "    # Iterate over each file in the specified folder\n",
    "    for file in os.listdir(folder):\n",
    "        # Check if the file name matches exactly any in our set of filenames to look for\n",
    "        if file in filenames_to_look_for:\n",
    "            # Open and read the file, then add its content to the combined_content string\n",
    "            with open(os.path.join(folder, file), 'r') as f:\n",
    "                combined_content += f.read()  # Add a newline character after each file's content for better separation\n",
    "\n",
    "    joined_string = '_'.join(str(integer) for integer in numbers)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    output_file_path = output_folder +file_initial+ 'combined' + '_' + joined_string + '.txt'\n",
    "    print(output_file_path)\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(combined_content)\n",
    "\n",
    "def shuffle_text_file_lines(file_path):\n",
    "    \"\"\"\n",
    "    Shuffles the lines in a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the text file to shuffle.\n",
    "    \"\"\"\n",
    "    # Read the lines from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Shuffle the lines\n",
    "    random.shuffle(lines)\n",
    "\n",
    "    # Write the shuffled lines back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "Ux_kentvVEVO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from typing import Optional\n",
    "\n",
    "class DINOFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)\n",
    "    \n",
    "class DINOFeatureExtractor_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained DINO model\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "        self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "        # self.feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
    "\n",
    "        \n",
    "        # Remove the head or adapt it to return features instead of logits\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the DINO backbone\n",
    "        return self.feature_extractor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN_DINO strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVzdt5GsBMxC"
   },
   "source": [
    "# experiment 20 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "NjP1BTXZBMxC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "lW0jcAVOBMxD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from avalanche.training import Naive, CWRStar, Replay, GDumb, \\\n",
    "    Cumulative, LwF, GEM, AGEM, EWC, AR1\n",
    "\n",
    "# strategies\n",
    "from avalanche.models import SimpleMLP\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dino knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class KNN_storagePlugin_update(SupervisedPlugin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mem_size: int = 200,\n",
    "        batch_size: int = None,\n",
    "        batch_size_mem: int = None,\n",
    "        task_balanced_dataloader: bool = False,\n",
    "        storage_policy: Optional[\"ExemplarsBuffer\"] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_mem = batch_size_mem\n",
    "        self.task_balanced_dataloader = task_balanced_dataloader\n",
    "\n",
    "        if storage_policy is not None:  # Use other storage policy\n",
    "            self.storage_policy = storage_policy\n",
    "            assert storage_policy.max_size == self.mem_size\n",
    "        else:  # Default\n",
    "            self.storage_policy = ExperienceBalancedBuffer(\n",
    "                max_size=self.mem_size, adaptive_size=True\n",
    "            )\n",
    "#         self.accuracy_metric = AccuracyMetric(task='multiclass')\n",
    "\n",
    "    @property\n",
    "    def ext_mem(self):\n",
    "        return self.storage_policy.buffer_groups  # a Dict<task_id, Dataset>\n",
    "\n",
    "    def before_training_exp(\n",
    "        self,\n",
    "        strategy: \"SupervisedTemplate\",\n",
    "        num_workers: int = 0,\n",
    "        shuffle: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataloader to build batches containing examples from both memories and\n",
    "        the training dataset\n",
    "        \"\"\"\n",
    "#         print('before_training_exp in plugin')\n",
    "\n",
    "        if len(self.storage_policy.buffer) == 0:\n",
    "            # first experience. We don't use the buffer, no need to change\n",
    "            # the dataloader.\n",
    "            buffer_size = len(self.storage_policy.buffer)\n",
    "            print(\"buffer size: \" + str(buffer_size))\n",
    "            return\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        if batch_size is None:\n",
    "            batch_size = strategy.train_mb_size\n",
    "\n",
    "        batch_size_mem = self.batch_size_mem\n",
    "        if batch_size_mem is None:\n",
    "            batch_size_mem = strategy.train_mb_size\n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.storage_policy.buffer,\n",
    "            oversample_small_tasks=True,\n",
    "            batch_size=batch_size,\n",
    "            batch_size_mem=batch_size_mem,\n",
    "            task_balanced_dataloader=self.task_balanced_dataloader,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"buffer size: \" + str(buffer_size))\n",
    "\n",
    "    def after_training_exp(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        self.storage_policy.update(strategy, **kwargs)\n",
    "        buffer_size = len(self.storage_policy.buffer)\n",
    "        print(\"after training exp buffer size: \" + str(buffer_size))\n",
    "\n",
    "\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Optional, List, TYPE_CHECKING\n",
    "\n",
    "import torch\n",
    "from numpy import inf\n",
    "from torch import cat, Tensor\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from avalanche.benchmarks.utils import (\n",
    "    make_classification_dataset,\n",
    "    classification_subset,\n",
    "    AvalancheDataset,\n",
    ")\n",
    "from avalanche.models import FeatureExtractorBackbone\n",
    "# from ..benchmarks.utils.utils import concat_datasets\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from .templates import SupervisedTemplate\n",
    "\n",
    "class Custom_ParametricBuffer(BalancedExemplarsBuffer):\n",
    "    \"\"\"Stores samples for replay using a custom selection strategy and\n",
    "    grouping.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        groupby=None,\n",
    "        selection_strategy: Optional[\"ExemplarsSelectionStrategy\"] = None,\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param max_size: The max capacity of the replay memory.\n",
    "        :param groupby: Grouping mechanism. One of {None, 'class', 'task',\n",
    "            'experience'}.\n",
    "        :param selection_strategy: The strategy used to select exemplars to\n",
    "            keep in memory when cutting it off.\n",
    "        \"\"\"\n",
    "        super().__init__(max_size)\n",
    "        assert groupby in {None, \"task\", \"class\", \"experience\"}, (\n",
    "            \"Unknown grouping scheme. Must be one of {None, 'task', \"\n",
    "            \"'class', 'experience'}\"\n",
    "        )\n",
    "        self.groupby = groupby\n",
    "        ss = selection_strategy or RandomExemplarsSelectionStrategy()\n",
    "        self.selection_strategy = ss\n",
    "        self.seen_groups = set()\n",
    "        self._curr_strategy = None\n",
    "\n",
    "\n",
    "    def update(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        new_data = strategy.adapted_dataset\n",
    "        new_groups = self._make_groups(strategy, new_data)\n",
    "        self.seen_groups.update(new_groups.keys())\n",
    "\n",
    "        # associate lengths to classes\n",
    "        lens = self.get_group_lengths(len(self.seen_groups))\n",
    "        group_to_len = {}\n",
    "        for group_id, ll in zip(self.seen_groups, lens):\n",
    "            group_to_len[group_id] = ll\n",
    "\n",
    "        # update buffers with new data\n",
    "        for group_id, new_data_g in new_groups.items():\n",
    "            ll = group_to_len[group_id]\n",
    "            if group_id in self.buffer_groups:\n",
    "                old_buffer_g = self.buffer_groups[group_id]\n",
    "                old_buffer_g.update_from_dataset(strategy, new_data_g)\n",
    "                old_buffer_g.resize(strategy, ll)\n",
    "            else:\n",
    "                new_buffer = _ParametricSingleBuffer(\n",
    "                    ll, self.selection_strategy\n",
    "                )\n",
    "                new_buffer.update_from_dataset(strategy, new_data_g)\n",
    "                self.buffer_groups[group_id] = new_buffer\n",
    "\n",
    "        # resize buffers\n",
    "        for group_id, class_buf in self.buffer_groups.items():\n",
    "            self.buffer_groups[group_id].resize(\n",
    "                strategy, group_to_len[group_id]\n",
    "            )\n",
    "\n",
    "    def _make_groups(self, strategy, data):\n",
    "        \"\"\"Split the data by group according to `self.groupby`.\"\"\"\n",
    "        if self.groupby is None:\n",
    "            return {0: data}\n",
    "        elif self.groupby == \"task\":\n",
    "            return self._split_by_task(data)\n",
    "        elif self.groupby == \"experience\":\n",
    "            return self._split_by_experience(strategy, data)\n",
    "        elif self.groupby == \"class\":\n",
    "            return self._split_by_class(data)\n",
    "        else:\n",
    "            assert False, \"Invalid groupby key. Should never get here.\"\n",
    "\n",
    "    def _split_by_class(self, data):\n",
    "        # Get sample idxs per class\n",
    "        class_idxs = {}\n",
    "        for idx, target in enumerate(data.targets):\n",
    "            if target not in class_idxs:\n",
    "                class_idxs[target] = []\n",
    "            class_idxs[target].append(idx)\n",
    "\n",
    "        # Make AvalancheSubset per class\n",
    "        new_groups = {}\n",
    "        for c, c_idxs in class_idxs.items():\n",
    "            new_groups[c] = classification_subset(data, indices=c_idxs)\n",
    "        return new_groups\n",
    "\n",
    "    def _split_by_experience(self, strategy, data):\n",
    "        exp_id = strategy.clock.train_exp_counter + 1\n",
    "        return {exp_id: data}\n",
    "\n",
    "    def _split_by_task(self, data):\n",
    "        new_groups = {}\n",
    "        for task_id in data.task_set:\n",
    "            new_groups[task_id] = data.task_set[task_id]\n",
    "        return new_groups\n",
    "\n",
    "class _ParametricSingleBuffer(ExemplarsBuffer):\n",
    "    \"\"\"A buffer that stores samples for replay using a custom selection\n",
    "    strategy.\n",
    "\n",
    "    This is a private class. Use `ParametricBalancedBuffer` with\n",
    "    `groupby=None` to get the same behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_size: int,\n",
    "        selection_strategy: Optional[\"ExemplarsSelectionStrategy\"] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param max_size: The max capacity of the replay memory.\n",
    "        :param selection_strategy: The strategy used to select exemplars to\n",
    "                                   keep in memory when cutting it off.\n",
    "        \"\"\"\n",
    "        super().__init__(max_size)\n",
    "        ss = selection_strategy or RandomExemplarsSelectionStrategy()\n",
    "        self.selection_strategy = ss\n",
    "        self._curr_strategy = None\n",
    "\n",
    "    def update(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        new_data = strategy.adapted_dataset\n",
    "        self.update_from_dataset(strategy, new_data)\n",
    "\n",
    "    def update_from_dataset(self, strategy, new_data):\n",
    "        self.buffer = self.buffer.concat(new_data)\n",
    "        self.resize(strategy, self.max_size)\n",
    "\n",
    "    def resize(self, strategy, new_size: int):\n",
    "        self.max_size = new_size\n",
    "        idxs = self.selection_strategy.make_sorted_indices(\n",
    "            strategy=strategy, data=self.buffer\n",
    "        )\n",
    "        self.buffer = self.buffer.subset(idxs[: self.max_size])\n",
    "\n",
    "        \n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "\n",
    "class KNN_DINO_update(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "#         id_all = torch.cat(task_id_list, dim = 0)\n",
    "        # Create a new TensorDataset from these tensors\n",
    "        features_all = l2_normalize(features_all)\n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNN_DINO_transform_original(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        matrix_dict = None,\n",
    "        syn_order = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.matrix_dict = matrix_dict\n",
    "        self.syn_order = syn_order\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            # print(self.mb_x)\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                # feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "                feature_list.append(features.cpu())\n",
    "                # label_list.append(target.to(self.device))\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "        \n",
    "        ###### apply transformation to the synthetic classes #######\n",
    "        cur_exp_number = self.experience.current_experience\n",
    "        # cur_classes = self.experience.classes_in_this_experience\n",
    "        # print(cur_exp_number, cur_classes)\n",
    "        syn_classes = torch.tensor(self.syn_order[cur_exp_number]).cpu()\n",
    "        \n",
    "        indices_to_transform = torch.nonzero(torch.isin(labels_all, syn_classes), as_tuple=True)[0]\n",
    "        # print(indices_to_transform)\n",
    "        \n",
    "        matrix_A = self.matrix_dict[cur_exp_number].to(self.device) # get the matrix specific for this experience\n",
    "        # print(features_all.device, indices_to_transform.device)\n",
    "        features_to_transform = features_all[indices_to_transform]\n",
    "        transformed_features = torch.matmul(features_to_transform.to(self.device), matrix_A)\n",
    "        features_all[indices_to_transform] = transformed_features.cpu()\n",
    "        print(features_all.shape)\n",
    "        features_all = l2_normalize(features_all)\n",
    "        # print(features_all.shape)\n",
    "        \n",
    "#         id_all = torch.cat(task_id_list, dim = 0)\n",
    "        # Create a new TensorDataset from these tensors\n",
    "        \n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    # def data_transform(self, features):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KNN_DINO_transform(BaseTemplate):\n",
    "    \"\"\"Base SGD class for continual learning skeletons.\n",
    "\n",
    "    **Training loop**\n",
    "    The training loop is organized as follows::\n",
    "\n",
    "        train\n",
    "            train_exp  # for each experience\n",
    "\n",
    "    **Evaluation loop**\n",
    "    The evaluation loop is organized as follows::\n",
    "\n",
    "        eval\n",
    "            eval_exp  # for each experience\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    PLUGIN_CLASS = BaseSGDPlugin\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "#         optimizer: Optimizer,\n",
    "#         criterion=CrossEntropyLoss(),\n",
    "        train_mb_size: int = 1,\n",
    "        train_epochs: int = 1,\n",
    "        eval_mb_size: Optional[int] = 1,\n",
    "        device=\"cpu\",\n",
    "        plugins: Optional[List[\"SupervisedPlugin\"]] = None,\n",
    "        matrix_dict = None,\n",
    "        syn_order = None,\n",
    "        evaluator: EvaluationPlugin = default_evaluator(),\n",
    "        eval_every=-1,\n",
    "        peval_mode=\"epoch\",\n",
    "        k: int = 5,\n",
    "        T: float = 0.07\n",
    "    ):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        :param model: PyTorch model.\n",
    "        :param optimizer: PyTorch optimizer.\n",
    "        :param criterion: loss function.\n",
    "        :param train_mb_size: mini-batch size for training.\n",
    "        :param train_epochs: number of training epochs.\n",
    "        :param eval_mb_size: mini-batch size for eval.\n",
    "        :param evaluator: (optional) instance of EvaluationPlugin for logging\n",
    "            and metric computations. None to remove logging.\n",
    "        :param eval_every: the frequency of the calls to `eval` inside the\n",
    "            training loop. -1 disables the evaluation. 0 means `eval` is called\n",
    "            only at the end of the learning experience. Values >0 mean that\n",
    "            `eval` is called every `eval_every` epochs and at the end of the\n",
    "            learning experience.\n",
    "        :param peval_mode: one of {'epoch', 'iteration'}. Decides whether the\n",
    "            periodic evaluation during training should execute every\n",
    "            `eval_every` epochs or iterations (Default='epoch').\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, device=device, plugins=plugins)\n",
    "\n",
    "#         self.optimizer: Optimizer = optimizer\n",
    "#         \"\"\" PyTorch optimizer. \"\"\"\n",
    "\n",
    "#         self._criterion = criterion\n",
    "#         \"\"\" Criterion. \"\"\"\n",
    "\n",
    "        self.train_epochs: int = train_epochs\n",
    "        \"\"\" Number of training epochs. \"\"\"\n",
    "\n",
    "        self.train_mb_size: int = train_mb_size\n",
    "        \"\"\" Training mini-batch size. \"\"\"\n",
    "\n",
    "        self.eval_mb_size: int = (\n",
    "            train_mb_size if eval_mb_size is None else eval_mb_size\n",
    "        )\n",
    "        \"\"\" Eval mini-batch size. \"\"\"\n",
    "\n",
    "        if evaluator is None:\n",
    "            evaluator = EvaluationPlugin()\n",
    "        self.plugins.append(evaluator)\n",
    "        self.matrix_dict = matrix_dict\n",
    "        self.syn_order = syn_order\n",
    "        self.evaluator = evaluator\n",
    "        assert peval_mode in {\"experience\", \"epoch\", \"iteration\"}\n",
    "        self.eval_every = eval_every\n",
    "#         peval = PeriodicEval(eval_every, peval_mode)\n",
    "#         self.plugins.append(peval)\n",
    "\n",
    "        self.clock = Clock()\n",
    "        \"\"\" Incremental counters for strategy events. \"\"\"\n",
    "        self.plugins.append(self.clock)\n",
    "\n",
    "        self.adapted_dataset = None\n",
    "        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n",
    "        append data to it (e.g. for replay). \n",
    "\n",
    "        .. note::\n",
    "\n",
    "            This dataset may contain samples from different experiences. If you \n",
    "            want the original data for the current experience  \n",
    "            use :attr:`.BaseTemplate.experience`.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloader = None\n",
    "        self.mbatch = None\n",
    "        self.mb_output = None\n",
    "        self.loss = None\n",
    "        self._stop_training = False\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.train_features = None\n",
    "        self.train_labels = None\n",
    "        self.replay_plugin = plugins[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self,\n",
    "              experiences: Union[CLExperience,\n",
    "                                 ExpSequence],\n",
    "              eval_streams: Optional[Sequence[Union[CLExperience,\n",
    "                                                    ExpSequence]]] = None,\n",
    "              **kwargs):\n",
    "\n",
    "#         super().train(experiences, eval_streams, **kwargs)\n",
    "#         return self.evaluator.get_last_metrics()\n",
    "        self.is_training = True\n",
    "        self._stop_training = False\n",
    "\n",
    "        self.model.eval()  # Feature extraction mode, so we set the model to eval\n",
    "        self.model.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if not isinstance(experiences, Iterable):\n",
    "                experiences = [experiences]\n",
    "            if eval_streams is None:\n",
    "                eval_streams = [experiences]\n",
    "            self._eval_streams = _group_experiences_by_stream(eval_streams)\n",
    "\n",
    "            self._before_training(**kwargs)\n",
    "            \n",
    "            for self.experience in experiences:\n",
    "                self._before_training_exp(**kwargs)\n",
    "                self._train_exp(experience, **kwargs)\n",
    "                self._after_training_exp(**kwargs)\n",
    "            self._after_training(**kwargs)\n",
    "                \n",
    "                \n",
    "                \n",
    "    def forward(self):\n",
    "        \"\"\"Compute the model's output given the current mini-batch.\"\"\"\n",
    "#         raise NotImplementedError()\n",
    "        if self.mb_x is not None:\n",
    "            # print(self.mb_x)\n",
    "            return self.model(self.mb_x.to(self.device))  # Ensure device compatibility\n",
    "        else:\n",
    "            raise ValueError(\"Input data not loaded: self.mb_x is None\")\n",
    "\n",
    "    def _before_training_exp(self, **kwargs):\n",
    "        \"\"\"Setup to train on a single experience.\"\"\"\n",
    "        print('_before_training_exp in strategy')\n",
    "        # Data Adaptation (e.g. add new samples/data augmentation)\n",
    "        self._before_train_dataset_adaptation(**kwargs)\n",
    "        self.train_dataset_adaptation(**kwargs)\n",
    "        self._after_train_dataset_adaptation(**kwargs)\n",
    "#         trigger_plugins(self, \"before_training_exp\", **kwargs)\n",
    "        self.make_train_dataloader(**kwargs)\n",
    "        print(self.dataloader)\n",
    "\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "#         self.model = self.model_adaptation()\n",
    "        # self.make_optimizer()\n",
    "        self.check_model_and_optimizer()\n",
    "        print('_before_training_exp in strategy super')\n",
    "        super()._before_training_exp(**kwargs)\n",
    "#         if self.dataloader is None:\n",
    "#         # If not set, initialize it here\n",
    "#             self.make_train_dataloader()\n",
    "#             print('train dataloader is made')\n",
    "\n",
    "#         if self.dataloader is None or len(self.dataloader) == 0:\n",
    "#             raise ValueError(\"Dataloader is not initialized or contains no data.\")\n",
    "    def _before_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_train_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_train_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def train_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "#         self.adapted_dataset = self.experience.dataset\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    "        self.model.eval()\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        task_id_list = []\n",
    "#         help(self.experience.dataset)\n",
    "\n",
    "        # Create a DataLoader to handle batches of data\n",
    "        dataloader = DataLoader(self.experience.dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for data, target, mb_task_id in dataloader:\n",
    "                data = data.to(self.device)\n",
    "                # Extract features using the model\n",
    "                features = self.model(data)\n",
    "                # feature_list.append(features.cpu())\n",
    "                label_list.append(target.cpu())\n",
    "                feature_list.append(features.cpu())\n",
    "                # label_list.append(target.to(self.device))\n",
    "#                 task_id_list.append(mb_task_id.cpu())\n",
    "\n",
    "        # Convert lists of batches into a single tensor for features and labels\n",
    "        features_all = torch.cat(feature_list, dim=0)\n",
    "        labels_all = torch.cat(label_list, dim=0)\n",
    "        \n",
    "        ###### apply transformation to the synthetic classes #######\n",
    "        cur_exp_number = self.experience.current_experience\n",
    "        print('cur_exp_number', cur_exp_number)\n",
    "        syn_classes = torch.tensor(self.syn_order[cur_exp_number]).cpu()\n",
    "        print('syn_classes', syn_classes)\n",
    "        \n",
    "        indices_to_transform = torch.nonzero(torch.isin(labels_all, syn_classes), as_tuple=True)[0]\n",
    "        # print(indices_to_transform)\n",
    "        \n",
    "        matrix_A = self.matrix_dict[cur_exp_number].to(self.device) # get the matrix specific for this experience\n",
    "        # print(features_all.device, indices_to_transform.device)\n",
    "        features_to_transform = features_all[indices_to_transform]\n",
    "        transformed_features = torch.matmul(features_to_transform.to(self.device), matrix_A)\n",
    "        features_all[indices_to_transform] = transformed_features.cpu()\n",
    "        print('feature before after transform', features_all[indices_to_transform] == features_to_transform)\n",
    "        features_all = l2_normalize(features_all)\n",
    "        # print(features_all.shape)\n",
    "        \n",
    "#         id_all = torch.cat(task_id_list, dim = 0)\n",
    "        # Create a new TensorDataset from these tensors\n",
    "        \n",
    "        current_dataset = TensorDataset(features_all, labels_all, \n",
    "#                                         id_all\n",
    "                                       )\n",
    "        self.adapted_dataset = make_classification_dataset(current_dataset)\n",
    "        \n",
    "#         self.adapted_dataset = self.adapted_dataset.train()\n",
    " \n",
    "        print('self.adapted_dataset', self.adapted_dataset)\n",
    "    def make_train_dataloader(\n",
    "        self,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Data loader initialization.\n",
    "\n",
    "        Called at the start of each learning experience after the dataset\n",
    "        adaptation.\n",
    "\n",
    "        :param num_workers: number of thread workers for the data loading.\n",
    "        :param shuffle: True if the data should be shuffled, False otherwise.\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        self.dataloader = TaskBalancedDataLoader(\n",
    "            self.adapted_dataset,\n",
    "            oversample_small_groups=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.train_mb_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "#         print('_'*10)\n",
    "#         for mb in self.dataloader:\n",
    "#             print(mb[0].shape)\n",
    "            \n",
    "    def model_adaptation(self, model=None):\n",
    "        \"\"\"Adapts the model to the current experience.\"\"\"\n",
    "        pass\n",
    "    def check_model_and_optimizer(self):\n",
    "        # Should be implemented in observation type\n",
    "        pass\n",
    "    def _train_exp(\n",
    "        self, experience: CLExperience, eval_streams=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Training loop over a single Experience object.\n",
    "\n",
    "        :param experience: CL experience information.\n",
    "        :param eval_streams: list of streams for evaluation.\n",
    "            If None: use the training experience for evaluation.\n",
    "            Use [] if you do not want to evaluate during training.\n",
    "        :param kwargs: custom arguments.\n",
    "        \"\"\"\n",
    "        if eval_streams is None:\n",
    "            eval_streams = [experience]\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, exp in enumerate(eval_streams):\n",
    "                if not isinstance(exp, Iterable):\n",
    "                    eval_streams[i] = [exp]\n",
    "            for _ in range(self.train_epochs):\n",
    "                self._before_training_epoch(**kwargs)\n",
    "\n",
    "                if self._stop_training:  # Early stopping\n",
    "                    self._stop_training = False\n",
    "                    break\n",
    "\n",
    "                self.training_epoch(**kwargs)\n",
    "                self._after_training_epoch(**kwargs)\n",
    "    def _before_training_epoch(self, **kwargs):\n",
    "        print('_before_training_epoch')\n",
    "        trigger_plugins(self, \"before_training_epoch\", **kwargs)\n",
    "    \n",
    "    def training_epoch(self, **kwargs):\n",
    "        # Should be implemented in Update Type\n",
    "#         raise NotADirectoryError()\n",
    "        print('training_epoch')\n",
    "        print(self.dataloader)\n",
    "#         print(self.model) \n",
    "        \n",
    "        for self.mbatch in self.dataloader:\n",
    "            self._unpack_minibatch()\n",
    "            self._before_training_iteration(**kwargs)\n",
    "\n",
    "# #             self._before_forward(**kwargs)\n",
    "# #             self.mb_output = self.forward()\n",
    "#             with torch.no_grad():\n",
    "#                 features = self.forward()\n",
    "#                 all_features.append(features)\n",
    "#                 all_labels.append(self.mb_y)\n",
    "#                 self.mb_output = self.knn_classifier(test_features=features,\n",
    "#                                                  train_features=self.train_features,\n",
    "#                                                  train_labels=self.train_labels,\n",
    "#                                                  k=self.k, T=self.T)\n",
    "#             print('in training_epoch', self.mb_x.shape)\n",
    "            self._after_training_iteration(**kwargs)\n",
    "\n",
    "    def _unpack_minibatch(self):\n",
    "        \"\"\"Move to device\"\"\"\n",
    "#         print('_unpack_minibatch')\n",
    "        # First verify the mini-batch\n",
    "#         self._check_minibatch()\n",
    "\n",
    "        if isinstance(self.mbatch, tuple):\n",
    "            self.mbatch = list(self.mbatch)\n",
    "        for i in range(len(self.mbatch)):\n",
    "#             print(i)\n",
    "            self.mbatch[i] = self.mbatch[i].to(self.device)\n",
    "#         print(self.mbatch)\n",
    "        self.mb_x, self.mb_y, self.mb_task_id = self.mbatch\n",
    "#         print(self.mb_x.shape)\n",
    "    def _before_training_iteration(self, **kwargs):\n",
    "#         print('_before_training_iteration')\n",
    "        trigger_plugins(self, \"before_training_iteration\", **kwargs)\n",
    "        \n",
    "    def _after_training_iteration(self, **kwargs):\n",
    "#         print('_after_training_iteration')\n",
    "#         trigger_plugins(self, \"after_training_iteration\", **kwargs)\n",
    "        pass\n",
    "    def _after_training_epoch(self, **kwargs):\n",
    "#         trigger_plugins(self, \"after_training_epoch\", **kwargs)\n",
    "        print('_after_training_epoch')\n",
    "        pass\n",
    "    \n",
    "#     ---------------------- eval ------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def eval(\n",
    "        self,\n",
    "        exp_list: Union[CLExperience, CLStream],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # eval can be called inside the train method.\n",
    "        # Save the shared state here to restore before returning.\n",
    "        self.model.to(self.device)\n",
    "#         print('eval')\n",
    "#         print(self.model)\n",
    "        prev_train_state = self._save_train_state()\n",
    "        self.is_training = False\n",
    "        self.model.eval()\n",
    "\n",
    "        if not isinstance(exp_list, Iterable):\n",
    "            exp_list = [exp_list]\n",
    "        self.current_eval_stream = exp_list\n",
    "\n",
    "        self._before_eval(**kwargs)\n",
    "        for self.experience in exp_list:\n",
    "            self._before_eval_exp(**kwargs)\n",
    "            self._eval_exp(**kwargs)\n",
    "            self._after_eval_exp(**kwargs)\n",
    "\n",
    "        self._after_eval(**kwargs)\n",
    "\n",
    "        # restore previous shared state.\n",
    "        self._load_train_state(prev_train_state)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Run the backward pass.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Execute the optimizer step (weights update).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def criterion(self):\n",
    "        \"\"\"Compute loss function.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _before_eval_exp(self, **kwargs):\n",
    "\n",
    "        # Data Adaptation\n",
    "#         print(self.model)\n",
    "        self._before_eval_dataset_adaptation(**kwargs)\n",
    "        self.eval_dataset_adaptation(**kwargs)\n",
    "        self._after_eval_dataset_adaptation(**kwargs)\n",
    "\n",
    "        self.make_eval_dataloader(**kwargs)\n",
    "        # Model Adaptation (e.g. freeze/add new units)\n",
    "        print('eval Model Adaptation ')\n",
    "#         self.model = self.model_adaptation(self.model)\n",
    "#         print(self.model)\n",
    "\n",
    "        super()._before_eval_exp(**kwargs)\n",
    "        \n",
    "    def _before_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_dataset_adaptation\", **kwargs)\n",
    "\n",
    "    def _after_eval_dataset_adaptation(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_dataset_adaptation\", **kwargs)\n",
    "    \n",
    "    def eval_dataset_adaptation(self, **kwargs):\n",
    "        \"\"\"Initialize `self.adapted_dataset`.\"\"\"\n",
    "        print('eval_dataset_adaptation')\n",
    "        self.adapted_dataset = self.experience.dataset\n",
    "        self.adapted_dataset = self.adapted_dataset.eval()\n",
    "        print(len(self.adapted_dataset))\n",
    "\n",
    "    def make_eval_dataloader(\n",
    "        self, num_workers=0, pin_memory=True, persistent_workers=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the eval data loader.\n",
    "        :param num_workers: How many subprocesses to use for data loading.\n",
    "            0 means that the data will be loaded in the main process.\n",
    "            (default: 0).\n",
    "        :param pin_memory: If True, the data loader will copy Tensors into CUDA\n",
    "            pinned memory before returning them. Defaults to True.\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        other_dataloader_args = {}\n",
    "\n",
    "        if parse_version(torch.__version__) >= parse_version(\"1.7.0\"):\n",
    "            other_dataloader_args[\"persistent_workers\"] = persistent_workers\n",
    "        for k, v in kwargs.items():\n",
    "            other_dataloader_args[k] = v\n",
    "\n",
    "        collate_from_data_or_kwargs(self.adapted_dataset,\n",
    "                                    other_dataloader_args)\n",
    "        self.dataloader = DataLoader(\n",
    "            self.adapted_dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.eval_mb_size,\n",
    "            pin_memory=pin_memory,\n",
    "            **other_dataloader_args\n",
    "        )\n",
    "        \n",
    "    def _eval_exp(self, **kwargs):\n",
    "        self.eval_epoch(**kwargs)\n",
    "    \n",
    "    def eval_epoch(self, **kwargs):\n",
    "        \"\"\"Evaluation loop over the current `self.dataloader`.\"\"\"\n",
    "#         print('len(self.dataloader)', len(self.dataloader))\n",
    "\n",
    "        for self.mbatch in self.dataloader:\n",
    "            inputs, labels = self.mbatch[0].to(self.device), self.mbatch[1]\n",
    "            self._unpack_minibatch()\n",
    "            self._before_eval_iteration(**kwargs)\n",
    "\n",
    "            self._before_eval_forward(**kwargs)\n",
    "            features = self.forward()\n",
    "#             print(features)\n",
    "#             print(self.buffer)\n",
    "#             features = self.model(self.mb_x)\n",
    "            \n",
    "#             print(self.model)\n",
    "#             self.mb_output = self.forward()\n",
    "            predictions = self.knn_classifier(features)\n",
    "            self.mb_output = predictions  # Set the minibatch output to KNN predictions\n",
    "\n",
    "            self._after_eval_forward(**kwargs)\n",
    "#             self.loss = self.criterion()\n",
    "\n",
    "            self._after_eval_iteration(**kwargs)\n",
    "    def _before_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_iteration\", **kwargs)\n",
    "\n",
    "    def _before_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"before_eval_forward\", **kwargs)\n",
    "\n",
    "    def knn_classifier(self, features):\n",
    "        print('knn classifier')\n",
    "        train_features, train_labels = self.get_buffer_data()\n",
    "        print('number of data in buffer ', len(train_features))\n",
    "        print(self.device)\n",
    "        test_features = features.to(self.device)\n",
    "        \n",
    "        train_features = train_features.to(test_features.device)\n",
    "        train_labels = train_labels.to(test_features.device)\n",
    "    # Assuming train_features are transposed and ready to be used for dot product similarity\n",
    "        distances, indices = torch.cdist(test_features, train_features).topk(self.k, largest=False, sorted=True)\n",
    "        retrieved_neighbors = train_labels[indices]  # Retrieve labels of the k-nearest neighbors\n",
    "\n",
    "        # Voting or averaging can happen here depending on your approach, example with voting:\n",
    "        predictions, _ = torch.mode(retrieved_neighbors, dim=1)\n",
    "#         print('prediction is', predictions)\n",
    "#         print(self.mb_y)\n",
    "        return predictions\n",
    "    # def data_transform(self, features):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_buffer_data(self):\n",
    "#         print(self.replay_plugin.ext_mem.values())\n",
    "#         print(self.replay_plugin.storage_policy.buffer_datasets)\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Iterate over each dataset in the buffer\n",
    "        for dataset in replay_plugin.storage_policy.buffer_datasets:\n",
    "#             print(dataset)\n",
    "            # Assuming the dataset provides a DataLoader to iterate over\n",
    "            loader = DataLoader(dataset, batch_size=self.train_mb_size, shuffle=False)\n",
    "            for features, target, mb_task_id in loader:\n",
    "                # Assuming data is already in the correct format or requires some preprocessing\n",
    "                # You may need to move data to the correct device if using GPU\n",
    "                features = features.to(self.device)\n",
    "#                 print(features.shape)\n",
    "#                 features = self.model(data)  # Extract features using the pre-trained model\n",
    "                all_features.append(features)\n",
    "                all_labels.append(target)\n",
    "\n",
    "        # Concatenate all features and labels from the buffer\n",
    "        train_features = torch.cat(all_features, dim=0)\n",
    "        train_labels = torch.cat(all_labels, dim=0)\n",
    "#         print(train_features.shape)\n",
    "        return train_features, train_labels\n",
    "    \n",
    "    def _after_eval_forward(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_forward\", **kwargs)\n",
    "        \n",
    "    def _after_eval_iteration(self, **kwargs):\n",
    "        trigger_plugins(self, \"after_eval_iteration\", **kwargs)\n",
    "#         strategy.loss = 0\n",
    "#         pass\n",
    "\n",
    "def _group_experiences_by_stream(eval_streams):\n",
    "    if len(eval_streams) == 1:\n",
    "        return eval_streams\n",
    "\n",
    "    exps = []\n",
    "    # First, we unpack the list of experiences.\n",
    "    for exp in eval_streams:\n",
    "        if isinstance(exp, Iterable):\n",
    "            exps.extend(exp)\n",
    "        else:\n",
    "            exps.append(exp)\n",
    "    # Then, we group them by stream.\n",
    "    exps_by_stream = defaultdict(list)\n",
    "    for exp in exps:\n",
    "        sname = exp.origin_stream.name\n",
    "        exps_by_stream[sname].append(exp)\n",
    "    # Finally, we return a list of lists.\n",
    "    return list(exps_by_stream.values())\n",
    "\n",
    "def l2_normalize(features):\n",
    "    # Compute the L2 norm for each row (dim=1)\n",
    "    norms = torch.norm(features, p=2, dim=1, keepdim=True)\n",
    "    # Divide each element by its norm\n",
    "    normalized_features = features / norms\n",
    "    return normalized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1125,  0.1830,  0.2353,  ..., -0.2109, -0.0223,  0.0534],\n",
      "        [-0.0530,  0.1102,  0.0056,  ..., -0.1015, -0.2012,  0.1526],\n",
      "        [-0.2596, -0.0151,  0.0642,  ..., -0.1738,  0.0353,  0.0848],\n",
      "        ...,\n",
      "        [ 0.0571,  0.2684, -0.0377,  ...,  0.1931, -0.0852,  0.1543],\n",
      "        [-0.0283,  0.1948,  0.2074,  ...,  0.1852,  0.0949,  0.0968],\n",
      "        [ 0.0908,  0.1603,  0.0442,  ...,  0.0739,  0.0364,  0.3214]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the saved dictionary of tensors\n",
    "matrices_A = torch.load('saved_data/transform_matrix_s5g2_3.pth')\n",
    "\n",
    "# Use the loaded data\n",
    "print(matrices_A[1])  # Example: Access a matrix if 'matrix1' is a key\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "matrix_eye = {}\n",
    "for i in range(20):\n",
    "    matrix_eye[i] = torch.eye(1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 20],\n",
       " [83, 19],\n",
       " [62, 33],\n",
       " [74, 53],\n",
       " [4, 32],\n",
       " [40, 41],\n",
       " [64, 21],\n",
       " [49, 68],\n",
       " [65, 46],\n",
       " [72, 31],\n",
       " [8, 1],\n",
       " [18, 86],\n",
       " [85, 95],\n",
       " [25, 82],\n",
       " [66, 37],\n",
       " [78, 52],\n",
       " [3, 99],\n",
       " [28, 90],\n",
       " [17, 77],\n",
       " [79, 58]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/55/enbo/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/homes/55/enbo/miniconda3/envs/thesis3.7/lib/python3.7/site-packages/avalanche/training/templates/base.py:219: UserWarning: Plugin <__main__.KNN_storagePlugin_update object at 0x7fc535aefb50> implements incompatible callbacks for template <__main__.KNN_DINO_transform object at 0x7fc52dfcba50>. This may result in errors. Incompatible callbacks: {'after_train_dataset_adaptation', 'after_eval_dataset_adaptation', 'before_train_dataset_adaptation', 'before_eval_dataset_adaptation'}\n",
      "  f\"Plugin {p} implements incompatible callbacks for template\"\n"
     ]
    }
   ],
   "source": [
    "# cl_strategy = Naive(\n",
    "#     resnet_model, torch.optim.SGD(resnet_model.fc.parameters(), lr=0.01, momentum = 0.9),\n",
    "#     CrossEntropyLoss(), train_mb_size=32, train_epochs=1, eval_mb_size=16,\n",
    "#     # eval_every=500,\n",
    "#     device=device,\n",
    "#     evaluator=eval_plugin,\n",
    "#     plugins=[CustomReplay_SD(mem_size=60000, storage_policy = storage_p, \n",
    "#                              image_folder= 'saved_data/sd_turbo_i2i_50all_step20')]\n",
    "#     )\n",
    "\n",
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "storage_p = Custom_ParametricBuffer(\n",
    "    max_size=60000,\n",
    "    groupby='class',\n",
    "    selection_strategy=RandomExemplarsSelectionStrategy()\n",
    ")\n",
    "\n",
    "\n",
    "replay_plugin = KNN_storagePlugin_update(mem_size=60000, storage_policy = storage_p)\n",
    "dino_model = DINOFeatureExtractor_v2()\n",
    "\n",
    "cl_strategy = KNN_DINO_transform(\n",
    "    model=dino_model,\n",
    "    train_mb_size=512, # 32\n",
    "    train_epochs=1,\n",
    "    eval_mb_size=512, # 16\n",
    "    device=device,\n",
    "    matrix_dict = matrices_A,\n",
    "    syn_order = order_sample,\n",
    "    evaluator=eval_plugin,\n",
    "    plugins=[replay_plugin]  # Use the KNN plugin\n",
    ")\n",
    "# cl_strategy = KNN_DINO_update(\n",
    "#     model=dino_model,\n",
    "#     train_mb_size=512, # 32\n",
    "#     train_epochs=1,\n",
    "#     eval_mb_size=512, # 16\n",
    "#     device=device,\n",
    "#     evaluator=eval_plugin,\n",
    "#     plugins=[replay_plugin]  # Use the KNN plugin\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 36, 5, 20, 54]\n",
      "-- >> Start of training phase << --\n",
      "_before_training_exp in strategy\n",
      "cur_exp_number 0\n",
      "syn_classes tensor([ 5, 20])\n",
      "feature before after transform tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "self.adapted_dataset <avalanche.benchmarks.utils.classification_dataset.ClassificationDataset object at 0x7fc5e6efc550>\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7fc5ec7091d0>\n",
      "_before_training_exp in strategy super\n",
      "buffer size: 0\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7fc5ec7091d0>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 2500\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.68s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.6160\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.65s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.60s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.59s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.60s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.59s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.65s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.62s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.56s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.57s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.63s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.56s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.56s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.56s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.59s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.72s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.54s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.65s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  2500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.67s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0308\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.0300\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.0500\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "Start of experience:  1\n",
      "Current Classes:  [45, 13, 19, 83, 22]\n",
      "-- >> Start of training phase << --\n",
      "_before_training_exp in strategy\n",
      "cur_exp_number 1\n",
      "syn_classes tensor([83, 19])\n",
      "feature before after transform tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "self.adapted_dataset <avalanche.benchmarks.utils.classification_dataset.ClassificationDataset object at 0x7fc5ec0d5250>\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7fc5358a3cd0>\n",
      "_before_training_exp in strategy super\n",
      "buffer size: 2500\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.ReplayDataLoader object at 0x7fc5dc0e2550>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 5000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.76s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.5980\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.63s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.5940\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.60s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.60s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.55s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.62s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.62s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.60s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.62s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.63s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.75s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.69s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.67s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.61s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.57s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  5000\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.54s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0596\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 0.9900\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.9700\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "Start of experience:  2\n",
      "Current Classes:  [33, 73, 16, 26, 62]\n",
      "-- >> Start of training phase << --\n",
      "_before_training_exp in strategy\n",
      "cur_exp_number 2\n",
      "syn_classes tensor([62, 33])\n",
      "feature before after transform tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "self.adapted_dataset <avalanche.benchmarks.utils.classification_dataset.ClassificationDataset object at 0x7fc52dbad290>\n",
      "<avalanche.benchmarks.utils.data_loader.TaskBalancedDataLoader object at 0x7fc5dc0dbe90>\n",
      "_before_training_exp in strategy super\n",
      "buffer size: 5000\n",
      "_before_training_epoch\n",
      "0it [00:00, ?it/s]training_epoch\n",
      "<avalanche.benchmarks.utils.data_loader.ReplayDataLoader object at 0x7fc5dc0e2550>\n",
      "_after_training_epoch\n",
      "after training exp buffer size: 7500\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.10s/it]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.6000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.82s/it]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.5600\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.94s/it]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.5720\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.68s/it]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.70s/it]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.74s/it]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.70s/it]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.73s/it]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.79s/it]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.77s/it]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 10 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.82s/it]\n",
      "> Eval on experience 10 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp010 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 11 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.76s/it]\n",
      "> Eval on experience 11 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp011 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 12 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.77s/it]\n",
      "> Eval on experience 12 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp012 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 13 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.70s/it]\n",
      "> Eval on experience 13 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp013 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 14 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.96s/it]\n",
      "> Eval on experience 14 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp014 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 15 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.88s/it]\n",
      "> Eval on experience 15 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp015 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 16 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.76s/it]\n",
      "> Eval on experience 16 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp016 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 17 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.71s/it]\n",
      "> Eval on experience 17 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp017 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 18 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:04<00:00,  4.73s/it]\n",
      "> Eval on experience 18 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp018 = 0.0000\n",
      "eval_dataset_adaptation\n",
      "500\n",
      "eval Model Adaptation \n",
      "-- Starting eval on experience 19 (Task 0) from test stream --\n",
      "0it [00:00, ?it/s]knn classifier\n",
      "number of data in buffer  7500\n",
      "cuda\n",
      "100%|| 1/1 [00:05<00:00,  5.46s/it]\n",
      "> Eval on experience 19 (Task 0) from test stream ended.\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp019 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0866\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/0 = 0.9900\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/1 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/10 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/11 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/12 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/13 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/14 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/15 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/16 = 0.9900\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/17 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/18 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/19 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/2 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/20 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/21 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/22 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/23 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/24 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/25 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/26 = 0.9000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/27 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/28 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/29 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/3 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/30 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/31 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/32 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/33 = 0.0100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/34 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/35 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/36 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/37 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/38 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/39 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/4 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/40 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/41 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/42 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/43 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/44 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/45 = 0.8000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/46 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/47 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/48 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/49 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/5 = 0.0100\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/50 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/51 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/52 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/53 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/54 = 1.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/55 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/56 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/57 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/58 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/59 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/6 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/60 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/61 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/62 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/63 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/64 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/65 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/66 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/67 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/68 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/69 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/7 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/70 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/71 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/72 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/73 = 0.9600\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/74 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/75 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/76 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/77 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/78 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/79 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/8 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/80 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/81 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/82 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/83 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/84 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/85 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/86 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/87 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/88 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/89 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/9 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/90 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/91 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/92 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/93 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/94 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/95 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/96 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/97 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/98 = 0.0000\n",
      "\tTop1_ClassAcc_Stream/eval_phase/test_stream/Task000/99 = 0.0000\n",
      "Start of experience:  3\n",
      "Current Classes:  [34, 98, 74, 53, 24]\n",
      "-- >> Start of training phase << --\n",
      "_before_training_exp in strategy\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in sythesis_cifar_benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    test_re = cl_strategy.eval(sythesis_cifar_benchmark.test_stream)\n",
    "    results.append(test_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ruhDj4_LBy1o"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
